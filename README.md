# AICE-rlaif

Execu0ve summary Technical descrip0on
We aim to research, design, and implement a conversa:onal AI that can rapidly become an “expert” on a user-defined topic. Our team at NCSA has been developing an AI chatbot plaHorm to create “teaching assistants” trained on course materials [1]. Our prototype is capable of rapidly inges:ng instructor- supplied course materials (textbook, instructor notes, lecture slides, lecture videos) and using this informa:on to ground GPT-4 model in order to respond to student ques:ons related to the course. In the process of implemen:ng this framework, we iden:fied opportuni:es for such chatbots to become very effec:ve in answering ques:ons withing a narrowly defined area as well as challenges in adap:ng large language models (LLMs) for such tasks. With this proposal, we are looking to address some of these challenges and develop and demonstrate a framework capable of quickly becoming an “expert” on a user-defined topic. The proposed system will operate as follows:
1. A user specifies the source or subject, e.g., biology textbook for the class they are taking, a series of historical documents for the research paper they are wri:ng, or a literature piece they are reading for the book club, to name a few.
2. LLM(s) is “trained” on these sources, relevant addi:onal materials, and their deriva:ves. To include deeper levels of primary materials for each topic, we will include in the training corpora all references in the cita:on tree of each source. Addi:onally, separate LLMs will use the primary material to generate task-specific versions of the data, e.g., Q&A pairs, “explain like I’m 5” answers, summaries and more.
3. The user interacts with the newly tuned LLM via a chatbot interface, either text-based or via voice.
Key innova:ons will include:
1. Scalable Oversight: We aim to refine generated responses using a factual consistency model. This model will evaluate whether the answer is backed by a corpus of verified informa:on sources like textbooks and scien:fic databases [2]. Non-factual responses are self-corrected using prompt engineering to force models to s:ck to the facts. Appendix A details prompts for (1) ques:on genera:on, (2) answer genera:on, (3) answer scoring with and without ground truth answers. Appendix B shows the Evaluator applica:on that will enable our work on large scale corpuses.
2. Novel factuality loss in RL with AI Feedback. We will introduce a novel training penalty, beyond cross entropy, termed factuality loss, a method of retrieval-augmented RL with AI feedback. Given a document, textbook or conference paper, we preprocess it by genera:ng Q&A pairs (as in contribu:on 1 above). During training, retrieved documents are included in the prompt and the generated answers are evaluated by our factual consistency model that directly produces a final reward value. As we’ve done before, we will use TRLX for distributed RLHF/RLAIF training.
This addi:onal factuality loss builds on the fundamental successes of Reinforcement Learning from
 
Human Feedback [8] (RLHF) and Anthropic’s Cons:tu:onal AI [9], in a deep research effort to teach
the model that hallucina:on is never acceptable.
3. Supervise the reasoning process in addi0on to outcomes. In addi:on to (1) we propose
decomposing the ques:on into PageRank style considera:ons such as “Is the author an expert in this field? Do they have prior work on this topic? “, etc. We assert that by forcing the model to think through the process of research, in an extended ‘chain of thought’ loop, it will be capable of both world class ques:on answering and even the crea:on of new knowledge. The models will have access to tools such as relevant scien:fic databases, WolframAlpha and more [3-6], enabling to verify hypothesis by direct examina:on of exis:ng data and even proposing new experiments to fill gaps in the exis:ng literature.
