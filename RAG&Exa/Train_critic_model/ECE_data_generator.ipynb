{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3fBuScalhnc"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjEC8jEs7K-T"
      },
      "outputs": [],
      "source": [
        "!pip install openai>=1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJ0BC2n1lwhU",
        "outputId": "afb23d66-ab05-4cdc-9ff5-ed897d30d7eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token is valid (permission: write).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "login(\n",
        "token=\"xxxxxx\", # ADD YOUR TOKEN HERE\n",
        "add_to_git_credential=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151,
          "referenced_widgets": [
            "6fdec7ba61b443faab785c31847c8e2a",
            "3720e92b4e58458d9c7513ad019a553a",
            "8543aef8a11e4e2b8462293ac4c94050",
            "779c0dddbec541a797ce21340d8ee73c",
            "fecd041b849642cea28999820c5de64a",
            "9145b882f306494991f51f082c2a40df",
            "bff1bfa9aaaa4374833d39c076c48a89",
            "3b152e2573874204ab02d341c088857d",
            "faf50398f0c74656ae8b32b2be6c3303",
            "9bc250482fe44ccd8ba6b6811d1cbbd0",
            "635698185a364db0a50e6fad692abe34",
            "18ad3c635e944fb8b588050837808dc7",
            "a92acd3e322c4a168b73f0dcea2cf578",
            "4e660636610d4cf3a166e55eb61f7a33",
            "b179cd1a6a6d41c098a18366c615eedb",
            "e6bbdb2988f34fbe81398a0f14026d93",
            "57369b947c2b48fa8b6041a00eb84375",
            "6bf5b18bdc6948fca69f728bba98c361",
            "a71f1b911a0346db95216680cf99b2ac",
            "a77535dabc8b400ba6edca10beaa0ef2",
            "9db9bd7e2a944221bfcc41c2c91489ce",
            "e38c2b96b1b547bf81d8acb07c3db858",
            "b35c153b6c4d458bbbbc7ac79a408177",
            "8e05e15e0a6540d8949a5d9bbe7c624c",
            "27962ff14c2c40e592ea4894ea605d44",
            "61fcf7a204df43819c1b96431b81bdca",
            "fdf08ebec2414fe2845611c8d19416dd",
            "f27bd3f50328414c8485452911666183",
            "7e1ddbec3e5743b9917cc07f76d85585",
            "0eb1b19e25c44782b839d11617affdfc",
            "16cdce3d28d54a588c094b88a0848561",
            "f6c5a2b3de6e4b0dba31b812a4af10c5",
            "576616bf75224bbbb6b2e7297a6f2e0c"
          ]
        },
        "id": "FLRRO5-dlRt4",
        "outputId": "4a036767-5fb9-4c4e-86fe-f78ce64c8540"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6fdec7ba61b443faab785c31847c8e2a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/180k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18ad3c635e944fb8b588050837808dc7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/216M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b35c153b6c4d458bbbbc7ac79a408177",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/1072 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['first_query', 'first_contexts', 'contexts', 'full_prompt', 'first_response', 'references', 'response_without_references', '__index_level_0__'])\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"CAII-NCSA/ECE408_FA23_first_interactions\")\n",
        "print(dataset['train'][0].keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xr8nOA-tmO1L",
        "outputId": "4801ef03-9332-4dae-eaee-c109c95a26ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "________________this is first query_________________\n",
            "Explain how tiling helps with global memory bandwidth.\n",
            "\n",
            "\n",
            "________________this is first contexts_________________\n",
            "[{'base_url': '', 'course_name ': 'ECE408SP24', 'pagenumber': 57, 'readable_filename': 'CUDA_C_Best_Practices_Guide.pdf', 's3_path': 'courses/ECE408SP24/f1ffcfe0-43a3-42bd-a60b-94fdbab7914f-CUDA_C_Best_Practices_Guide.pdf', 'text': 'CUDA C++ Best Practices Guide, Release 12.3\\nFig. 10: The performance of the sliding-window benchmark with tuned hit-ratio\\n13.2.3. Shared Memory\\nBecause it is on-chip, shared memory has much higher bandwidth and lower latency than local and\\nglobal memory - provided there are no bank conflicts between the threads, as detailed in the following\\nsection.\\n13.2.3.1 Shared Memory and Memory Banks\\nTo achieve high memory bandwidth for concurrent accesses, shared memory is divided into equally\\nsized memory modules (banks) that can be accessed simultaneously. Therefore, any memory load or\\nstore of n addresses that spans n distinct memory banks can be serviced simultaneously, yielding an\\neffective bandwidth that is n times as high as the bandwidth of a single bank.\\nHowever, if multiple addresses of a memory request map to the same memory bank, the accesses\\nare serialized. The hardware splits a memory request that has bank conflicts into as many separate\\nconflict-free requests as necessary, decreasing the effective bandwidth by a factor equal to the num-\\nber of separate memory requests. The one exception here is when multiple threads in a warp address\\nthe same shared memory location, resulting in a broadcast. In this case, multiple broadcasts from\\ndifferent banks are coalesced into a single multicast from the requested shared memory locations to\\nthe threads.\\nTo minimize bank conflicts, it is important to understand how memory addresses map to memory\\nbanks and how to optimally schedule memory requests.\\nOn devices of compute capability 5.x or newer, each bank has a bandwidth of 32 bits every clock cycle,\\nand successive 32-bit words are assigned to successive banks. The warp size is 32 threads and the\\nnumber of banks is also 32, so bank conflicts can occur between any threads in the warp. See Compute\\nCapability 5.x in the CUDA C++ Programming Guide for further details.\\n13.2. Device Memory Spaces\\n51', 'url': ''}, {'base_url': '', 'course_name ': 'ECE408SP24', 'pagenumber': 164, 'readable_filename': 'CUDA_C_Programming_Guide.pdf', 's3_path': 'courses/ECE408SP24/f69627cb-3930-47cb-a4b3-18ab0b0c96b7-CUDA_C_Programming_Guide.pdf', 'text': 'CUDA C++ Programming Guide, Release 12.3\\nTo achieve high bandwidth, shared memory is divided into equally-sized memory modules, called banks,\\nwhich can be accessed simultaneously. Any memory read or write request made of n addresses that\\nfall in n distinct memory banks can therefore be serviced simultaneously, yielding an overall bandwidth\\nthat is n times as high as the bandwidth of a single module.\\nHowever, if two addresses of a memory request fall in the same memory bank, there is a bank conflict\\nand the access has to be serialized. The hardware splits a memory request with bank conflicts into\\nas many separate conflict-free requests as necessary, decreasing throughput by a factor equal to the\\nnumber of separate memory requests. If the number of separate memory requests is n, the initial\\nmemory request is said to cause n-way bank conflicts.\\nTo get maximum performance, it is therefore important to understand how memory addresses map\\nto memory banks in order to schedule the memory requests so as to minimize bank conflicts. This\\nis described in Compute Capability 5.x, Compute Capability 6.x, Compute Capability 7.x, Compute Ca-\\npability 8.x, and Compute Capability 9.0 for devices of compute capability 5.x, 6.x, 7.x, 8.x, and 9.0\\nrespectively.\\nConstant Memory\\nThe constant memory space resides in device memory and is cached in the constant cache.\\nA request is then split into as many separate requests as there are different memory addresses in the\\ninitial request, decreasing throughput by a factor equal to the number of separate requests.\\nThe resulting requests are then serviced at the throughput of the constant cache in case of a cache\\nhit, or at the throughput of device memory otherwise.\\nTexture and Surface Memory\\nThe texture and surface memory spaces reside in device memory and are cached in texture cache, so\\na texture fetch or surface read costs one memory read from device memory only on a cache miss,\\notherwise it just costs one read from texture cache. The texture cache is optimized for 2D spatial\\nlocality, so threads of the same warp that read texture or surface addresses that are close together in\\n2D will achieve best performance. Also, it is designed for streaming fetches with a constant latency;\\na cache hit reduces DRAM bandwidth demand but not fetch latency.\\nReading device memory through texture or surface fetching present some benefits that can make it\\nan advantageous alternative to reading device memory from global or constant memory:\\n▶ If the memory reads do not follow the access patterns that global or constant memory reads\\nmust follow to get good performance, higher bandwidth can be achieved providing that there is\\nlocality in the texture fetches or surface reads;\\n▶ Addressing calculations are performed outside the kernel by dedicated units;\\n▶ Packed data may be broadcast to separate variables in a single operation;\\n▶ 8-bit and 16-bit integer input data may be optionally converted to 32 bit floating-point values in\\nthe range [0.0, 1.0] or [-1.0, 1.0] (see Texture Memory).\\n148\\nChapter 8. Performance Guidelines', 'url': ''}, {'base_url': '', 'course_name ': 'ECE408SP24', 'pagenumber': 43, 'readable_filename': 'CUDA_C_Best_Practices_Guide.pdf', 's3_path': 'courses/ECE408SP24/f1ffcfe0-43a3-42bd-a60b-94fdbab7914f-CUDA_C_Best_Practices_Guide.pdf', 'text': 'Chapter 13. Memory Optimizations\\nMemory optimizations are the most important area for performance. The goal is to maximize the use\\nof the hardware by maximizing bandwidth. Bandwidth is best served by using as much fast memory\\nand as little slow-access memory as possible. This chapter discusses the various kinds of memory on\\nthe host and device and how best to set up data items to use the memory effectively.\\n13.1. Data Transfer Between Host and Device\\nThe peak theoretical bandwidth between the device memory and the GPU is much higher (898 GB/s\\non the NVIDIA Tesla V100, for example) than the peak theoretical bandwidth between host memory\\nand device memory (16 GB/s on the PCIe x16 Gen3). Hence, for best overall application performance,\\nit is important to minimize data transfer between the host and the device, even if that means running\\nkernels on the GPU that do not demonstrate any speedup compared with running them on the host\\nCPU.\\nNote: High Priority: Minimize data transfer between the host and the device, even if it means running\\nsome kernels on the device that do not show performance gains when compared with running them\\non the host CPU.\\nIntermediate data structures should be created in device memory, operated on by the device, and\\ndestroyed without ever being mapped by the host or copied to host memory.\\nAlso, because of the overhead associated with each transfer, batching many small transfers into one\\nlarger transfer performs significantly better than making each transfer separately, even if doing so\\nrequires packing non-contiguous regions of memory into a contiguous buffer and then unpacking\\nafter the transfer.\\nFinally, higher bandwidth between the host and the device is achieved when using page-locked (or\\npinned) memory, as discussed in the CUDA C++ Programming Guide and the Pinned Memory section\\nof this document.\\n37', 'url': ''}, {'base_url': '', 'course_name ': 'ECE408SP24', 'pagenumber': 163, 'readable_filename': 'CUDA_C_Programming_Guide.pdf', 's3_path': 'courses/ECE408SP24/f69627cb-3930-47cb-a4b3-18ab0b0c96b7-CUDA_C_Programming_Guide.pdf', 'text': 'CUDA C++ Programming Guide, Release 12.3\\nReading non-naturally aligned 8-byte or 16-byte words produces incorrect results (off by a few words),\\nso special care must be taken to maintain alignment of the starting address of any value or array of\\nvalues of these types. A typical case where this might be easily overlooked is when using some cus-\\ntom global memory allocation scheme, whereby the allocations of multiple arrays (with multiple calls\\nto cudaMalloc() or cuMemAlloc()) is replaced by the allocation of a single large block of memory\\npartitioned into multiple arrays, in which case the starting address of each array is offset from the\\nblock’s starting address.\\nTwo-Dimensional Arrays\\nA common global memory access pattern is when each thread of index (tx,ty) uses the following\\naddress to access one element of a 2D array of width width, located at address BaseAddress of type\\ntype* (where type meets the requirement described in Maximize Utilization):\\nBaseAddress + width * ty + tx\\nFor these accesses to be fully coalesced, both the width of the thread block and the width of the array\\nmust be a multiple of the warp size.\\nIn particular, this means that an array whose width is not a multiple of this size will be accessed much\\nmore efficiently if it is actually allocated with a width rounded up to the closest multiple of this size\\nand its rows padded accordingly. The cudaMallocPitch() and cuMemAllocPitch() functions and\\nassociated memory copy functions described in the reference manual enable programmers to write\\nnon-hardware-dependent code to allocate arrays that conform to these constraints.\\nLocal Memory\\nLocal memory accesses only occur for some automatic variables as mentioned in Variable Memory\\nSpace Specifiers. Automatic variables that the compiler is likely to place in local memory are:\\n▶ Arrays for which it cannot determine that they are indexed with constant quantities,\\n▶ Large structures or arrays that would consume too much register space,\\n▶ Any variable if the kernel uses more registers than available (this is also known as register spilling).\\nInspection of the PTX assembly code (obtained by compiling with the -ptx or-keep option) will tell if a\\nvariable has been placed in local memory during the first compilation phases as it will be declared using\\nthe .local mnemonic and accessed using the ld.local and st.local mnemonics. Even if it has not,\\nsubsequent compilation phases might still decide otherwise though if they find it consumes too much\\nregister space for the targeted architecture: Inspection of the cubin object using cuobjdump will tell if\\nthis is the case. Also, the compiler reports total local memory usage per kernel (lmem) when compiling\\nwith the --ptxas-options=-v option. Note that some mathematical functions have implementation\\npaths that might access local memory.\\nThe local memory space resides in device memory, so local memory accesses have the same high\\nlatency and low bandwidth as global memory accesses and are subject to the same requirements for\\nmemory coalescing as described in Device Memory Accesses. Local memory is however organized\\nsuch that consecutive 32-bit words are accessed by consecutive thread IDs. Accesses are therefore\\nfully coalesced as long as all threads in a warp access the same relative address (for example, same\\nindex in an array variable, same member in a structure variable).\\nOn devices of compute capability 5.x onwards, local memory accesses are always cached in L2 in the\\nsame way as global memory accesses (see Compute Capability 5.x and Compute Capability 6.x).\\nShared Memory\\nBecause it is on-chip, shared memory has much higher bandwidth and much lower latency than local\\nor global memory.\\n8.3. Maximize Memory Throughput\\n147', 'url': ''}, {'base_url': '', 'course_name ': 'ECE408SP24', 'pagenumber': 63, 'readable_filename': 'CUDA_C_Best_Practices_Guide.pdf', 's3_path': 'courses/ECE408SP24/f1ffcfe0-43a3-42bd-a60b-94fdbab7914f-CUDA_C_Best_Practices_Guide.pdf', 'text': 'CUDA C++ Best Practices Guide, Release 12.3\\nThe examples in this section have illustrated three reasons to use shared memory:\\n▶ To enable coalesced accesses to global memory, especially to avoid large strides (for general\\nmatrices, strides are much larger than 32)\\n▶ To eliminate (or reduce) redundant loads from global memory\\n▶ To avoid wasted bandwidth\\n13.2.3.4 Asynchronous Copy from Global Memory to Shared Memory\\nCUDA 11.0 introduces an async-copy feature that can be used within device code to explicitly manage\\nthe asynchronous copying of data from global memory to shared memory. This feature enables CUDA\\nkernels to overlap copying data from global to shared memory with computation. It also avoids an\\nintermediary register file access traditionally present between the global memory read and the shared\\nmemory write.\\nFor more details refer to the memcpy_async section in the CUDA C++ Programming Guide.\\nTo understand the performance difference between synchronous copy and asynchronous copy of data\\nfrom global memory to shared memory, consider the following micro benchmark CUDA kernels for\\ndemonstrating the synchronous and asynchronous approaches. Asynchronous copies are hardware\\naccelerated for NVIDIA A100 GPU.\\ntemplate <typename T>\\n__global__ void pipeline_kernel_sync(T *global, uint64_t *clock, size_t copy_count) {\\nextern __shared__ char s[];\\nT *shared = reinterpret_cast<T *>(s);\\nuint64_t clock_start = clock64();\\nfor (size_t i = 0; i < copy_count; ++i) {\\nshared[blockDim.x * i + threadIdx.x] = global[blockDim.x * i + threadIdx.x];\\n}\\nuint64_t clock_end = clock64();\\natomicAdd(reinterpret_cast<unsigned long long *>(clock),\\nclock_end - clock_start);\\n}\\ntemplate <typename T>\\n__global__ void pipeline_kernel_async(T *global, uint64_t *clock, size_t copy_count) {\\nextern __shared__ char s[];\\nT *shared = reinterpret_cast<T *>(s);\\nuint64_t clock_start = clock64();\\n∕∕pipeline pipe;\\nfor (size_t i = 0; i < copy_count; ++i) {\\n__pipeline_memcpy_async(&shared[blockDim.x * i + threadIdx.x],\\n&global[blockDim.x * i + threadIdx.x], sizeof(T));\\n}\\n__pipeline_commit();\\n__pipeline_wait_prior(0);\\nuint64_t clock_end = clock64();\\n(continues on next page)\\n13.2. Device Memory Spaces\\n57', 'url': ''}, {'base_url': '', 'course_name ': 'ECE408SP24', 'pagenumber': 161, 'readable_filename': 'CUDA_C_Programming_Guide.pdf', 's3_path': 'courses/ECE408SP24/f69627cb-3930-47cb-a4b3-18ab0b0c96b7-CUDA_C_Programming_Guide.pdf', 'text': 'CUDA C++ Programming Guide, Release 12.3\\n▶ Write the results back to device memory.\\nFor some applications (for example, for which global memory access patterns are data-dependent),\\na traditional hardware-managed cache is more appropriate to exploit data locality. As mentioned in\\nCompute Capability 7.x, Compute Capability 8.x and Compute Capability 9.0, for devices of compute\\ncapability 7.x, 8.x and 9.0, the same on-chip memory is used for both L1 and shared memory, and how\\nmuch of it is dedicated to L1 versus shared memory is configurable for each kernel call.\\nThe throughput of memory accesses by a kernel can vary by an order of magnitude depending on ac-\\ncess pattern for each type of memory. The next step in maximizing memory throughput is therefore\\nto organize memory accesses as optimally as possible based on the optimal memory access patterns\\ndescribed in Device Memory Accesses. This optimization is especially important for global memory\\naccesses as global memory bandwidth is low compared to available on-chip bandwidths and arith-\\nmetic instruction throughput, so non-optimal global memory accesses generally have a high impact\\non performance.\\n8.3.1. Data Transfer between Host and Device\\nApplications should strive to minimize data transfer between the host and the device. One way to\\naccomplish this is to move more code from the host to the device, even if that means running kernels\\nthat do not expose enough parallelism to execute on the device with full efficiency. Intermediate data\\nstructures may be created in device memory, operated on by the device, and destroyed without ever\\nbeing mapped by the host or copied to host memory.\\nAlso, because of the overhead associated with each transfer, batching many small transfers into a\\nsingle large transfer always performs better than making each transfer separately.\\nOn systems with a front-side bus, higher performance for data transfers between host and device is\\nachieved by using page-locked host memory as described in Page-Locked Host Memory.\\nIn addition, when using mapped page-locked memory (Mapped Memory), there is no need to allocate\\nany device memory and explicitly copy data between device and host memory. Data transfers are\\nimplicitly performed each time the kernel accesses the mapped memory. For maximum performance,\\nthese memory accesses must be coalesced as with accesses to global memory (see Device Memory\\nAccesses). Assuming that they are and that the mapped memory is read or written only once, using\\nmapped page-locked memory instead of explicit copies between device and host memory can be a\\nwin for performance.\\nOn integrated systems where device memory and host memory are physically the same, any copy\\nbetween host and device memory is superfluous and mapped page-locked memory should be used\\ninstead. Applications may query a device is integrated by checking that the integrated device prop-\\nerty (see Device Enumeration) is equal to 1.\\n8.3.2. Device Memory Accesses\\nAn instruction that accesses addressable memory (i.e., global, local, shared, constant, or texture mem-\\nory) might need to be re-issued multiple times depending on the distribution of the memory addresses\\nacross the threads within the warp. How the distribution affects the instruction throughput this way\\nis specific to each type of memory and described in the following sections. For example, for global\\nmemory, as a general rule, the more scattered the addresses are, the more reduced the throughput is.\\nGlobal Memory\\n8.3. Maximize Memory Throughput\\n145', 'url': ''}, {'base_url': '', 'course_name ': 'ECE408SP24', 'pagenumber': 453, 'readable_filename': 'CUDA_C_Programming_Guide.pdf', 's3_path': 'courses/ECE408SP24/f69627cb-3930-47cb-a4b3-18ab0b0c96b7-CUDA_C_Programming_Guide.pdf', 'text': 'CUDA C++ Programming Guide, Release 12.3\\n19.4.2. Global Memory\\nGlobal memory accesses are always cached in L2.\\nData that is read-only for the entire lifetime of the kernel can also be cached in the unified L1/texture\\ncache described in the previous section by reading it using the __ldg() function (see Read-Only Data\\nCache Load Function). When the compiler detects that the read-only condition is satisfied for some\\ndata, it will use __ldg() to read it. The compiler might not always be able to detect that the read-\\nonly condition is satisfied for some data. Marking pointers used for loading such data with both the\\nconst and __restrict__ qualifiers increases the likelihood that the compiler will detect the read-\\nonly condition.\\nData that is not read-only for the entire lifetime of the kernel cannot be cached in the unified\\nL1/texture cache for devices of compute capability 5.0. For devices of compute capability 5.2, it is,\\nby default, not cached in the unified L1/texture cache, but caching may be enabled using the following\\nmechanisms:\\n▶ Perform the read using inline assembly with the appropriate modifier as described in the PTX\\nreference manual;\\n▶ Compile with the -Xptxas -dlcm=ca compilation flag, in which case all reads are cached, except\\nreads that are performed using inline assembly with a modifier that disables caching;\\n▶ Compile with the -Xptxas -fscm=ca compilation flag, in which case all reads are cached, in-\\ncluding reads that are performed using inline assembly regardless of the modifier used.\\nWhen caching is enabled using one of the three mechanisms listed above, devices of compute capa-\\nbility 5.2 will cache global memory reads in the unified L1/texture cache for all kernel launches except\\nfor the kernel launches for which thread blocks consume too much of the SM’s register file. These\\nexceptions are reported by the profiler.\\n19.4.3. Shared Memory\\nShared memory has 32 banks that are organized such that successive 32-bit words map to successive\\nbanks. Each bank has a bandwidth of 32 bits per clock cycle.\\nA shared memory request for a warp does not generate a bank conflict between two threads that\\naccess any address within the same 32-bit word (even though the two addresses fall in the same bank).\\nIn that case, for read accesses, the word is broadcast to the requesting threads and for write accesses,\\neach address is written by only one of the threads (which thread performs the write is undefined).\\nFigure 22 shows some examples of strided access.\\nFigure 23 shows some examples of memory read accesses that involve the broadcast mechanism.\\n19.4. Compute Capability 5.x\\n437', 'url': ''}, {'base_url': '', 'course_name ': 'ECE408SP24', 'pagenumber': 162, 'readable_filename': 'CUDA_C_Programming_Guide.pdf', 's3_path': 'courses/ECE408SP24/f69627cb-3930-47cb-a4b3-18ab0b0c96b7-CUDA_C_Programming_Guide.pdf', 'text': 'CUDA C++ Programming Guide, Release 12.3\\nGlobal memory resides in device memory and device memory is accessed via 32-, 64-, or 128-byte\\nmemory transactions. These memory transactions must be naturally aligned: Only the 32-, 64-, or 128-\\nbyte segments of device memory that are aligned to their size (i.e., whose first address is a multiple\\nof their size) can be read or written by memory transactions.\\nWhen a warp executes an instruction that accesses global memory, it coalesces the memory accesses\\nof the threads within the warp into one or more of these memory transactions depending on the size\\nof the word accessed by each thread and the distribution of the memory addresses across the threads.\\nIn general, the more transactions are necessary, the more unused words are transferred in addition to\\nthe words accessed by the threads, reducing the instruction throughput accordingly. For example, if a\\n32-byte memory transaction is generated for each thread’s 4-byte access, throughput is divided by 8.\\nHow many transactions are necessary and how much throughput is ultimately affected varies with the\\ncompute capability of the device. Compute Capability 5.x, Compute Capability 6.x, Compute Capabil-\\nity 7.x, Compute Capability 8.x and Compute Capability 9.0 give more details on how global memory\\naccesses are handled for various compute capabilities.\\nTo maximize global memory throughput, it is therefore important to maximize coalescing by:\\n▶ Following the most optimal access patterns based on Compute Capability 5.x, Compute Capabil-\\nity 6.x, Compute Capability 7.x, Compute Capability 8.x and Compute Capability 9.0\\n▶ Using data types that meet the size and alignment requirement detailed in the section Size and\\nAlignment Requirement below,\\n▶ Padding data in some cases, for example, when accessing a two-dimensional array as described\\nin the section Two-Dimensional Arrays below.\\nSize and Alignment Requirement\\nGlobal memory instructions support reading or writing words of size equal to 1, 2, 4, 8, or 16 bytes.\\nAny access (via a variable or a pointer) to data residing in global memory compiles to a single global\\nmemory instruction if and only if the size of the data type is 1, 2, 4, 8, or 16 bytes and the data is\\nnaturally aligned (i.e., its address is a multiple of that size).\\nIf this size and alignment requirement is not fulfilled, the access compiles to multiple instructions\\nwith interleaved access patterns that prevent these instructions from fully coalescing. It is therefore\\nrecommended to use types that meet this requirement for data that resides in global memory.\\nThe alignment requirement is automatically fulfilled for the Built-in Vector Types.\\nFor structures, the size and alignment requirements can be enforced by the compiler using the align-\\nment specifiers__align__(8) or __align__(16), such as\\nstruct __align__(8) {\\nfloat x;\\nfloat y;\\n};\\nor\\nstruct __align__(16) {\\nfloat x;\\nfloat y;\\nfloat z;\\n};\\nAny address of a variable residing in global memory or returned by one of the memory allocation rou-\\ntines from the driver or runtime API is always aligned to at least 256 bytes.\\n146\\nChapter 8. Performance Guidelines', 'url': ''}, {'base_url': '', 'course_name ': 'ECE408SP24', 'pagenumber': 32, 'readable_filename': 'CUDA_C_Programming_Guide.pdf', 's3_path': 'courses/ECE408SP24/f69627cb-3930-47cb-a4b3-18ab0b0c96b7-CUDA_C_Programming_Guide.pdf', 'text': 'CUDA C++ Programming Guide, Release 12.3\\nin a cluster have the ability to read, write, and perform atomics to any address in the distributed shared\\nmemory. Distributed Shared Memory gives an example of performing histograms in distributed shared\\nmemory.\\n5.3. Memory Hierarchy\\nCUDA threads may access data from multiple memory spaces during their execution as illustrated by\\nFigure 6. Each thread has private local memory. Each thread block has shared memory visible to all\\nthreads of the block and with the same lifetime as the block. Thread blocks in a thread block cluster\\ncan perform read, write, and atomics operations on each other’s shared memory. All threads have\\naccess to the same global memory.\\nThere are also two additional read-only memory spaces accessible by all threads: the constant and\\ntexture memory spaces. The global, constant, and texture memory spaces are optimized for differ-\\nent memory usages (see Device Memory Accesses). Texture memory also offers different addressing\\nmodes, as well as data filtering, for some specific data formats (see Texture and Surface Memory).\\nThe global, constant, and texture memory spaces are persistent across kernel launches by the same\\napplication.\\nFigure 6: Memory Hierarchy\\n16\\nChapter 5. Programming Model', 'url': ''}, {'base_url': '', 'course_name ': 'ECE408SP24', 'pagenumber': 50, 'readable_filename': 'CUDA_C_Best_Practices_Guide.pdf', 's3_path': 'courses/ECE408SP24/f1ffcfe0-43a3-42bd-a60b-94fdbab7914f-CUDA_C_Best_Practices_Guide.pdf', 'text': 'CUDA C++ Best Practices Guide, Release 12.3\\n13.2.1. Coalesced Access to Global Memory\\nA very important performance consideration in programming for CUDA-capable GPU architectures is\\nthe coalescing of global memory accesses. Global memory loads and stores by threads of a warp are\\ncoalesced by the device into as few as possible transactions.\\nNote: High Priority: Ensure global memory accesses are coalesced whenever possible.\\nThe access requirements for coalescing depend on the compute capability of the device and are doc-\\numented in the CUDA C++ Programming Guide.\\nFor devices of compute capability 6.0 or higher, the requirements can be summarized quite easily: the\\nconcurrent accesses of the threads of a warp will coalesce into a number of transactions equal to the\\nnumber of 32-byte transactions necessary to service all of the threads of the warp.\\nFor certain devices of compute capability 5.2, L1-caching of accesses to global memory can be option-\\nally enabled. If L1-caching is enabled on these devices, the number of required transactions is equal\\nto the number of required 128-byte aligned segments.\\nNote:\\nOn devices of compute capability 6.0 or higher, L1-caching is the default, however the data\\naccess unit is 32-byte regardless of whether global loads are cached in L1 or not.\\nOn devices with GDDR memory, accessing memory in a coalesced way is even more important when\\nECC is turned on. Scattered accesses increase ECC memory transfer overhead, especially when writing\\ndata to global memory.\\nCoalescing concepts are illustrated in the following simple examples. These examples assume com-\\npute capability 6.0 or higher and that accesses are for 4-byte words, unless otherwise noted.\\n13.2.1.1 A Simple Access Pattern\\nThe first and simplest case of coalescing can be achieved by any CUDA-enabled device of compute\\ncapability 6.0 or higher: the k-th thread accesses the k-th word in a 32-byte aligned array. Not all\\nthreads need to participate.\\nFor example, if the threads of a warp access adjacent 4-byte words (e.g., adjacent float values), four\\ncoalesced 32-byte transactions will service that memory access. Such a pattern is shown in Figure 3.\\nFig. 3: Coalesced access\\nThis access pattern results in four 32-byte transactions, indicated by the red rectangles.\\nIf from any of the four 32-byte segments only a subset of the words are requested (e.g. if several\\nthreads had accessed the same word or if some threads did not participate in the access), the full\\nsegment is fetched anyway. Furthermore, if accesses by the threads of the warp had been permuted\\nwithin or accross the four segments, still only four 32-byte transactions would have been performed\\nby a device with compute capability 6.0 or higher.\\n44\\nChapter 13. Memory Optimizations', 'url': ''}, {'base_url': '', 'course_name ': 'ECE408SP24', 'pagenumber': 58, 'readable_filename': 'CUDA_C_Best_Practices_Guide.pdf', 's3_path': 'courses/ECE408SP24/f1ffcfe0-43a3-42bd-a60b-94fdbab7914f-CUDA_C_Best_Practices_Guide.pdf', 'text': 'CUDA C++ Best Practices Guide, Release 12.3\\n13.2.3.2 Shared Memory in Matrix Multiplication (C=AB)\\nShared memory enables cooperation between threads in a block. When multiple threads in a block\\nuse the same data from global memory, shared memory can be used to access the data from global\\nmemory only once. Shared memory can also be used to avoid uncoalesced memory accesses by loading\\nand storing data in a coalesced pattern from global memory and then reordering it in shared memory.\\nAside from memory bank conflicts, there is no penalty for non-sequential or unaligned accesses by a\\nwarp in shared memory.\\nThe use of shared memory is illustrated via the simple example of a matrix multiplication C = AB for\\nthe case with A of dimension Mxw, B of dimension wxN, and C of dimension MxN. To keep the kernels\\nsimple, M and N are multiples of 32, since the warp size (w) is 32 for current devices.\\nA natural decomposition of the problem is to use a block and tile size of wxw threads. Therefore, in\\nterms of wxw tiles, A is a column matrix, B is a row matrix, and C is their outer product; see Figure 11. A\\ngrid of N/w by M/w blocks is launched, where each thread block calculates the elements of a different\\ntile in C from a single tile of A and a single tile of B.\\nFig. 11: Block-column matrix multiplied by block-row matrix. Block-column matrix (A) multiplied by\\nblock-row matrix (B) with resulting product matrix (C).\\nTo do this, the simpleMultiply kernel (Unoptimized matrix multiplication) calculates the output el-\\nements of a tile of matrix C.\\nUnoptimized matrix multiplication\\n__global__ void simpleMultiply(float *a, float* b, float *c,\\nint N)\\n{\\nint row = blockIdx.y * blockDim.y + threadIdx.y;\\nint col = blockIdx.x * blockDim.x + threadIdx.x;\\nfloat sum = 0.0f;\\nfor (int i = 0; i < TILE_DIM; i++) {\\nsum += a[row*TILE_DIM+i] * b[i*N+col];\\n}\\nc[row*N+col] = sum;\\n}\\nIn Unoptimized matrix multiplication, a, b, and c are pointers to global memory for the matrices A, B,\\nand C, respectively; blockDim.x, blockDim.y, and TILE_DIM are all equal to w. Each thread in the\\n52\\nChapter 13. Memory Optimizations', 'url': ''}, {'base_url': '', 'course_name ': 'ECE408SP24', 'pagenumber': 20, 'readable_filename': 'CUDA_C_Programming_Guide.pdf', 's3_path': 'courses/ECE408SP24/f69627cb-3930-47cb-a4b3-18ab0b0c96b7-CUDA_C_Programming_Guide.pdf', 'text': 'CUDA C++ Programming Guide, Release 12.3\\nof relying on large data caches and complex flow control to avoid long memory access latencies, both\\nof which are expensive in terms of transistors.\\nIn general, an application has a mix of parallel parts and sequential parts, so systems are designed with\\na mix of GPUs and CPUs in order to maximize overall performance. Applications with a high degree of\\nparallelism can exploit this massively parallel nature of the GPU to achieve higher performance than\\non the CPU.\\n4\\nChapter 1. The Benefits of Using GPUs', 'url': ''}, {'base_url': '', 'course_name ': 'ECE408SP24', 'pagenumber': 65, 'readable_filename': 'CUDA_C_Best_Practices_Guide.pdf', 's3_path': 'courses/ECE408SP24/f1ffcfe0-43a3-42bd-a60b-94fdbab7914f-CUDA_C_Best_Practices_Guide.pdf', 'text': 'CUDA C++ Best Practices Guide, Release 12.3\\n▶ Best performance with synchronous copy is achieved when the copy_count parameter is a mul-\\ntiple of 4 for all three element sizes. The compiler can optimize groups of 4 load and store in-\\nstructions. This is evident from the saw tooth curves.\\n▶ Asynchronous copy achieves better performance in nearly all cases.\\n▶ The async-copy does not require the copy_count parameter to be a multiple of 4, to maximize\\nperformance through compiler optimizations.\\n▶ Overall, best performance is achieved when using asynchronous copies with an element of size\\n8 or 16 bytes.\\n13.2.4. Local Memory\\nLocal memory is so named because its scope is local to the thread, not because of its physical location.\\nIn fact, local memory is off-chip. Hence, access to local memory is as expensive as access to global\\nmemory. In other words, the term local in the name does not imply faster access.\\nLocal memory is used only to hold automatic variables. This is done by the nvcc compiler when it\\ndetermines that there is insufficient register space to hold the variable. Automatic variables that are\\nlikely to be placed in local memory are large structures or arrays that would consume too much register\\nspace and arrays that the compiler determines may be indexed dynamically.\\nInspection of the PTX assembly code (obtained by compiling with -ptx or -keep command-line op-\\ntions to nvcc) reveals whether a variable has been placed in local memory during the first compilation\\nphases. If it has, it will be declared using the .local mnemonic and accessed using the ld.local\\nand st.local mnemonics. If it has not, subsequent compilation phases might still decide otherwise,\\nif they find the variable consumes too much register space for the targeted architecture. There is no\\nway to check this for a specific variable, but the compiler reports total local memory usage per kernel\\n(lmem) when run with the--ptxas-options=-v option.\\n13.2.5. Texture Memory\\nThe read-only texture memory space is cached. Therefore, a texture fetch costs one device memory\\nread only on a cache miss; otherwise, it just costs one read from the texture cache. The texture cache\\nis optimized for 2D spatial locality, so threads of the same warp that read texture addresses that are\\nclose together will achieve best performance. Texture memory is also designed for streaming fetches\\nwith a constant latency; that is, a cache hit reduces DRAM bandwidth demand, but not fetch latency.\\nIn certain addressing situations, reading device memory through texture fetching can be an advanta-\\ngeous alternative to reading device memory from global or constant memory.\\n13.2.5.1 Additional Texture Capabilities\\nIf textures are fetched using tex1D(),tex2D(), or tex3D() rather than tex1Dfetch(), the hardware\\nprovides other capabilities that might be useful for some applications such as image processing, as\\nshown in Table 4.\\n13.2. Device Memory Spaces\\n59', 'url': ''}, {'base_url': '', 'course_name ': 'ECE408SP24', 'pagenumber': 23, 'readable_filename': 'CUDA_C_Best_Practices_Guide.pdf', 's3_path': 'courses/ECE408SP24/f1ffcfe0-43a3-42bd-a60b-94fdbab7914f-CUDA_C_Best_Practices_Guide.pdf', 'text': 'CUDA C++ Best Practices Guide, Release 12.3\\n▶ For best performance, there should be some coherence in memory access by adjacent threads\\nrunning on the device. Certain memory access patterns enable the hardware to coalesce groups\\nof reads or writes of multiple data items into one operation. Data that cannot be laid out so as to\\nenable coalescing, or that doesn’t have enough locality to use the L1 or texture caches effectively,\\nwill tend to see lesser speedups when used in computations on GPUs. A noteworthy exception\\nto this are completely random memory access patterns. In general, they should be avoided, be-\\ncause compared to peak capabilities any architecture processes these memory access patterns\\nat a low efficiency. However, compared to cache based architectures, like CPUs, latency hiding\\narchitectures, like GPUs, tend to cope better with completely random memory access patterns.\\n6.2. What Runs on a CUDA-Enabled Device?\\n17', 'url': ''}, {'base_url': '', 'course_name ': 'ECE408SP24', 'pagenumber': 42, 'readable_filename': 'CUDA_C_Best_Practices_Guide.pdf', 's3_path': 'courses/ECE408SP24/f1ffcfe0-43a3-42bd-a60b-94fdbab7914f-CUDA_C_Best_Practices_Guide.pdf', 'text': 'CUDA C++ Best Practices Guide, Release 12.3\\n12.2.3. Throughput Reported by Visual Profiler\\nFor devices with compute capability of 2.0 or greater, the Visual Profiler can be used to collect several\\ndifferent memory throughput measures. The following throughput metrics can be displayed in the\\nDetails or Detail Graphs view:\\n▶ Requested Global Load Throughput\\n▶ Requested Global Store Throughput\\n▶ Global Load Throughput\\n▶ Global Store Throughput\\n▶ DRAM Read Throughput\\n▶ DRAM Write Throughput\\nThe Requested Global Load Throughput and Requested Global Store Throughput values indicate the\\nglobal memory throughput requested by the kernel and therefore correspond to the effective band-\\nwidth obtained by the calculation shown under Effective Bandwidth Calculation.\\nBecause the minimum memory transaction size is larger than most word sizes, the actual memory\\nthroughput required for a kernel can include the transfer of data not used by the kernel. For global\\nmemory accesses, this actual throughput is reported by the Global Load Throughput and Global Store\\nThroughput values.\\nIt’s important to note that both numbers are useful. The actual memory throughput shows how close\\nthe code is to the hardware limit, and a comparison of the effective or requested bandwidth to the\\nactual bandwidth presents a good estimate of how much bandwidth is wasted by suboptimal coalesc-\\ning of memory accesses (see Coalesced Access to Global Memory). For global memory accesses, this\\ncomparison of requested memory bandwidth to actual memory bandwidth is reported by the Global\\nMemory Load Efficiency and Global Memory Store Efficiency metrics.\\n36\\nChapter 12. Performance Metrics', 'url': ''}]\n",
            "\n",
            "\n",
            "________________this is first response_________________\n",
            "Tiling is a technique used in CUDA programming to improve the efficiency of memory access and thereby optimize the performance of CUDA programs. This technique involves dividing the data into smaller chunks or 'tiles' that fit into the faster shared memory on the GPU, which is much faster than global memory.\n",
            "\n",
            "The main advantage of tiling is that it reduces the number of global memory accesses. When a tile is loaded into the shared memory, all threads in a block can access the data within the tile from the shared memory, which has much higher bandwidth and lower latency than global memory[1](https://uiuc-chatbot.s3.us-east-1.amazonaws.com/courses/ECE408SP24/f1ffcfe0-43a3-42bd-a60b-94fdbab7914f-CUDA_C_Best_Practices_Guide.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAUCD3K6ETL7TF7F5Q%2F20240116%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240116T180310Z&X-Amz-Expires=3600&X-Amz-Signature=f3d5c6e14b0f3b13394b62a34e79614aed40a8691edd07ca20068139675f41e9&X-Amz-SignedHeaders=host&response-content-disposition=inline&response-content-type=application%2Fpdf&x-id=GetObject#page=57)[2](https://uiuc-chatbot.s3.us-east-1.amazonaws.com/courses/ECE408SP24/f69627cb-3930-47cb-a4b3-18ab0b0c96b7-CUDA_C_Programming_Guide.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAUCD3K6ETL7TF7F5Q%2F20240116%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240116T180310Z&X-Amz-Expires=3600&X-Amz-Signature=adf9e09d38c3b51a59abb649a6916619ca18b5b231955ae9becb590b0616d591&X-Amz-SignedHeaders=host&response-content-disposition=inline&response-content-type=application%2Fpdf&x-id=GetObject#page=164). This can significantly reduce the time taken for memory access, thereby increasing the overall performance of the program.\n",
            "\n",
            "Another advantage of tiling is that it can help to ensure coalesced access to global memory. Coalescing is a mechanism that allows multiple threads in a warp to access memory in a way that minimizes the number of memory transactions, thus maximizing memory throughput[10](https://uiuc-chatbot.s3.us-east-1.amazonaws.com/courses/ECE408SP24/f1ffcfe0-43a3-42bd-a60b-94fdbab7914f-CUDA_C_Best_Practices_Guide.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAUCD3K6ETL7TF7F5Q%2F20240116%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240116T180316Z&X-Amz-Expires=3600&X-Amz-Signature=67626b5dff91fa65f9f990c524ae607223d1bc1da69be15029e945dc0ca6a3b4&X-Amz-SignedHeaders=host&response-content-disposition=inline&response-content-type=application%2Fpdf&x-id=GetObject#page=63). By organizing data into tiles, it's easier to ensure that threads in a warp access adjacent memory locations, which is a requirement for coalescing[1](https://uiuc-chatbot.s3.us-east-1.amazonaws.com/courses/ECE408SP24/f1ffcfe0-43a3-42bd-a60b-94fdbab7914f-CUDA_C_Best_Practices_Guide.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAUCD3K6ETL7TF7F5Q%2F20240116%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240116T180310Z&X-Amz-Expires=3600&X-Amz-Signature=f3d5c6e14b0f3b13394b62a34e79614aed40a8691edd07ca20068139675f41e9&X-Amz-SignedHeaders=host&response-content-disposition=inline&response-content-type=application%2Fpdf&x-id=GetObject#page=57)[2](https://uiuc-chatbot.s3.us-east-1.amazonaws.com/courses/ECE408SP24/f69627cb-3930-47cb-a4b3-18ab0b0c96b7-CUDA_C_Programming_Guide.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAUCD3K6ETL7TF7F5Q%2F20240116%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240116T180310Z&X-Amz-Expires=3600&X-Amz-Signature=adf9e09d38c3b51a59abb649a6916619ca18b5b231955ae9becb590b0616d591&X-Amz-SignedHeaders=host&response-content-disposition=inline&response-content-type=application%2Fpdf&x-id=GetObject#page=164).\n",
            "\n",
            "Lastly, tiling can help to eliminate or reduce redundant loads from global memory. If multiple threads need to access the same data, loading the data into a tile in shared memory allows all the threads to access the data without having to make multiple loads from global memory[5](https://uiuc-chatbot.s3.us-east-1.amazonaws.com/courses/ECE408SP24/f1ffcfe0-43a3-42bd-a60b-94fdbab7914f-CUDA_C_Best_Practices_Guide.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAUCD3K6ETL7TF7F5Q%2F20240116%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240116T180323Z&X-Amz-Expires=3600&X-Amz-Signature=f0fc89d052163d643d0df133fee5bc6c4f900441f2be95058e142713c832c597&X-Amz-SignedHeaders=host&response-content-disposition=inline&response-content-type=application%2Fpdf&x-id=GetObject#page=63).\n",
            "\n",
            "Relevant Sources:\n",
            "\n",
            "1. [1. CUDA_C_Best_Practices_Guide.pdf, page: 57](https://uiuc-chatbot.s3.us-east-1.amazonaws.com/courses/ECE408SP24/f1ffcfe0-43a3-42bd-a60b-94fdbab7914f-CUDA_C_Best_Practices_Guide.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAUCD3K6ETL7TF7F5Q%2F20240116%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240116T180310Z&X-Amz-Expires=3600&X-Amz-Signature=f3d5c6e14b0f3b13394b62a34e79614aed40a8691edd07ca20068139675f41e9&X-Amz-SignedHeaders=host&response-content-disposition=inline&response-content-type=application%2Fpdf&x-id=GetObject#page=57)\n",
            "2. [2. CUDA_C_Programming_Guide.pdf, page: 164](https://uiuc-chatbot.s3.us-east-1.amazonaws.com/courses/ECE408SP24/f69627cb-3930-47cb-a4b3-18ab0b0c96b7-CUDA_C_Programming_Guide.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAUCD3K6ETL7TF7F5Q%2F20240116%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240116T180310Z&X-Amz-Expires=3600&X-Amz-Signature=adf9e09d38c3b51a59abb649a6916619ca18b5b231955ae9becb590b0616d591&X-Amz-SignedHeaders=host&response-content-disposition=inline&response-content-type=application%2Fpdf&x-id=GetObject#page=164)\n",
            "5. [5. CUDA_C_Best_Practices_Guide.pdf, page: 63](https://uiuc-chatbot.s3.us-east-1.amazonaws.com/courses/ECE408SP24/f1ffcfe0-43a3-42bd-a60b-94fdbab7914f-CUDA_C_Best_Practices_Guide.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAUCD3K6ETL7TF7F5Q%2F20240116%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240116T180323Z&X-Amz-Expires=3600&X-Amz-Signature=f0fc89d052163d643d0df133fee5bc6c4f900441f2be95058e142713c832c597&X-Amz-SignedHeaders=host&response-content-disposition=inline&response-content-type=application%2Fpdf&x-id=GetObject#page=63)\n",
            "10. [10. CUDA_C_Best_Practices_Guide.pdf, page: 50](https://uiuc-chatbot.s3.us-east-1.amazonaws.com/courses/ECE408SP24/f1ffcfe0-43a3-42bd-a60b-94fdbab7914f-CUDA_C_Best_Practices_Guide.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAUCD3K6ETL7TF7F5Q%2F20240116%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240116T180316Z&X-Amz-Expires=3600&X-Amz-Signature=67626b5dff91fa65f9f990c524ae607223d1bc1da69be15029e945dc0ca6a3b4&X-Amz-SignedHeaders=host&response-content-disposition=inline&response-content-type=application%2Fpdf&x-id=GetObject#page=63)\n",
            "\n",
            "\n",
            "________________this is references_________________\n",
            "{'': None, ' L = r \\\\times 0.21 + g \\\\times 0.72 + b \\\\times 0.07 \\\\] [4': None, ' P[1': None, ' P[2': None, ' P[3': None, ' P[4': None, ' \\\\text{grayImage[grayOffset]} = 0.21f \\\\times r + 0.71f \\\\times g + 0.07f \\\\times b; \\\\] [5': None, '(blockIdx.y*blockDim.y+threadIdx.y)*Width+k]`, represents an un-coalesced access pattern. This is because the only part of Row*Width+k that can vary across a thread block is threadIdx.y[1': None, '0], N[1': None, '0], we only have N[0], N[1': None, '0]` after this kernel execution are either 0 or 1[1': None, '0]` and `data[1': None, '0]` is 0 and `row_ptr[1': None, '0]` is not used after the write, or if the buffer is not correctly marked as volatile[37': None, '1': 'https://uiuc-chatbot.s3.us-east-1.amazonaws.com/courses/ECE408SP24/f1ffcfe0-43a3-42bd-a60b-94fdbab7914f-CUDA_C_Best_Practices_Guide.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAUCD3K6ETL7TF7F5Q%2F20240116%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240116T180310Z&X-Amz-Expires=3600&X-Amz-Signature=f3d5c6e14b0f3b13394b62a34e79614aed40a8691edd07ca20068139675f41e9&X-Amz-SignedHeaders=host&response-content-disposition=inline&response-content-type=application%2Fpdf&x-id=GetObject#page=57', '1, page: 20][3, page: 25][4, page: 81][9, page: 26][13': None, '1, page: 22][4, page: 151][12, page: 19][77]. Since threads within a block can synchronize using barriers such as `__syncthreads()`, it is essential that the entire block is executed on the same SM to ensure that all threads can reach the synchronization point and proceed together[76': None, '1, page: 22][6': None, '1, page: 27][7, page: 26][8, page: 27][9, page: 153][28, page: 151][29, page: 7][34, page: 27][35, page: 317][36, page: 163][37': None, '1, page: 2]. The example provided in Document 1, page 5, shows the calculation of a specific output element P2,2 using a 5x5 mask and a sub-array of the input array N. The calculation involves pairwise multiplication between elements of the mask array and those of the image array, followed by summing all elements of the resulting product array to obtain the value of the output element[1': None, '1, page: 2][2, page: 10][3, page: 2][4, page: 2][5, page: 2][6, page: 2][7, page: 2][9': None, '1, page: 31][3': None, '1, page: 4][37': None, '1, page: 9]. However, the Kogge-Stone scan algorithm is not very work-efficient, as it does much more work than the sequential algorithm, which can be problematic in power-constrained environments such as mobile applications[1': None, '1, page: 9][2, page: 7]. Additionally, having an identity value for the operator is important, such as 0 for addition or 1 for multiplication, as it allows for the proper initialization of the reduction result[1': None, '1. 3rd-Edition-Chapter01-introduction-Final-corrected.pdf, page: 5': None, '1. 3rd-Edition-Chapter01-introduction-Final-corrected.pdf, page: 6': None, '1. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 10': None, '1. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 11': None, '1. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 12': None, '1. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 13': None, '1. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 24': None, '1. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 29': None, '1. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 6': None, '1. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 15': None, '1. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 20': None, '1. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 21': None, '1. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 27': None, '1. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 28': None, '1. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 3': None, '1. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 5': None, '1. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 6': None, '1. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 9': None, '1. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf': None, '1. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 12': None, '1. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 16': None, '1. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 20': None, '1. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 22': None, '1. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 30': None, '1. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 34': None, '1. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 5': None, '1. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 7': None, '1. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 9': None, '1. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, pages: 2, 3, 15': None, '1. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 15': None, '1. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 16': None, '1. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 17': None, '1. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 18': None, '1. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 20': None, '1. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 21': None, '1. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 22': None, '1. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 24': None, '1. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 27': None, '1. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 3': None, '1. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 4': None, '1. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 5': None, '1. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 6': None, '1. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 7': None, '1. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf': None, '1. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 1': None, '1. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 11': None, '1. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 12': None, '1. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 15': None, '1. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 16': None, '1. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 18': None, '1. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 19': None, '1. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 2': None, '1. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 21': None, '1. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 24': None, '1. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 26': None, '1. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 4': None, '1. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 5': None, '1. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 9': None, '1. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf': None, '1. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 10': None, '1. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 13': None, '1. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 17': None, '1. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 22': None, '1. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 4': None, '1. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 6': None, '1. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 9': None, '1. 3rd-Edition-Chapter09-histogram-FINAL-corrected, page: 19': None, '1. 3rd-Edition-Chapter09-histogram-FINAL-corrected.pdf, page: 16': None, '1. 3rd-Edition-Chapter09-histogram-FINAL-corrected.pdf, page: 19': None, '1. 3rd-Edition-Chapter11-merge-FINAL-corrected.pdf, page: 27': None, '1. 3rd-Edition-Chapter16-case-study-DNN-FINAL-corrected.pdf, page: 4': None, '1. CUDA C Best Practices Guide, page: 92': None, '1. CUDA C Programming Guide': None, '1. CUDA C Programming Guide, page: 152': None, '1. CUDA C Programming Guide, page: 175': None, '1. CUDA C Programming Guide, page: 189': None, '1. CUDA C++ Best Practices Guide, page: 50': None, '1. CUDA C++ Programming Guide, page: 402': None, '1. CUDA C++ Programming Guide, page: 78': None, '1. CUDA_C_Best_Practices_Guide.pdf, page: 33': None, '1. CUDA_C_Best_Practices_Guide.pdf, page: 50': None, '1. CUDA_C_Best_Practices_Guide.pdf, page: 57': 'https://uiuc-chatbot.s3.us-east-1.amazonaws.com/courses/ECE408SP24/f1ffcfe0-43a3-42bd-a60b-94fdbab7914f-CUDA_C_Best_Practices_Guide.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAUCD3K6ETL7TF7F5Q%2F20240116%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240116T180310Z&X-Amz-Expires=3600&X-Amz-Signature=f3d5c6e14b0f3b13394b62a34e79614aed40a8691edd07ca20068139675f41e9&X-Amz-SignedHeaders=host&response-content-disposition=inline&response-content-type=application%2Fpdf&x-id=GetObject#page=57', '1. CUDA_C_Best_Practices_Guide.pdf, page: 64': None, '1. CUDA_C_Best_Practices_Guide.pdf, page: 81': None, '1. CUDA_C_Programming_Guide, page: 161': None, '1. CUDA_C_Programming_Guide.pdf, page: 109': None, '1. CUDA_C_Programming_Guide.pdf, page: 13': None, '1. CUDA_C_Programming_Guide.pdf, page: 152': None, '1. CUDA_C_Programming_Guide.pdf, page: 16': None, '1. CUDA_C_Programming_Guide.pdf, page: 173': None, '1. CUDA_C_Programming_Guide.pdf, page: 203': None, '1. CUDA_C_Programming_Guide.pdf, page: 22': None, '1. CUDA_C_Programming_Guide.pdf, page: 465': None, '1. Spring 2024-ECE 408-overview.pdf, page: 10': None, '1. Spring 2024-ECE 408-overview.pdf, page: 11': None, '1. Spring 2024-ECE 408-overview.pdf, page: 5': None, '1. Spring 2024-ECE 408-overview.pdf, page: 9': None, '1. ece408-lecture1-introduction-vk-SP24.pdf, page: 10': None, '1. ece408-lecture1-introduction-vk-SP24.pdf, page: 11': None, '1. ece408-lecture1-introduction-vk-SP24.pdf, page: 29': None, '1. ece408-lecture11-ML-DL-vk-SP24.pdf, page: 2': None, '1. ece408-lecture12-CNN-vk-SP24.pdf, page: 2': None, '1. ece408-lecture12-CNN-vk-SP24.pdf, page: 45': None, '1. ece408-lecture12-CNN-vk-SP24.pdf, page: 48': None, '1. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 14': None, '1. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 3': None, '1. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 38': None, '1. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 4': None, '1. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 6': None, '1. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 8': None, '1. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 9': None, '1. ece408-lecture14-scan-vk-SP24.pdf, page: 2': None, '1. ece408-lecture14-scan-vk-SP24.pdf, page: 28': None, '1. ece408-lecture15-scan-part2-vk-SP24.pdf, page: 11': None, '1. ece408-lecture15-scan-part2-vk-SP24.pdf, page: 15': None, '1. ece408-lecture15-scan-part2-vk-SP24.pdf, page: 17': None, '1. ece408-lecture15-scan-part2-vk-SP24.pdf, page: 26': None, '1. ece408-lecture16-histogram-vk-SP24.pdf, page: 13': None, '1. ece408-lecture16-histogram-vk-SP24.pdf, page: 18': None, '1. ece408-lecture16-histogram-vk-SP24.pdf, page: 43': None, '1. ece408-lecture17-sparse-matrix-vk-SP24.pdf, page: 10': None, '1. ece408-lecture2-CUDA-introduction-vk-SP24.pdf, page: 32': None, '1. ece408-lecture2-CUDA-introduction-vk-SP24.pdf, page: 4': None, '1. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 22': None, '1. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 23': None, '1. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 25': None, '1. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 3': None, '1. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 30': None, '1. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 31': None, '1. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 9': None, '1. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 17': None, '1. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 2': None, '1. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 28': None, '1. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 32': None, '1. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 33': None, '1. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 10': None, '1. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 11': None, '1. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 14': None, '1. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 3': None, '1. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 31': None, '1. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 35': None, '1. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 36': None, '1. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 6': None, '1. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 9': None, '1. ece408-lecture6-dram-tiling-vk-SP24.pdf, page: 33': None, '1. ece408-lecture6-dram-tiling-vk-SP24.pdf, page: 35': None, '1. ece408-lecture6-dram-tiling-vk-SP24.pdf, page: 45': None, '1. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 1': None, '1. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 13': None, '1. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 18': None, '1. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 2': None, '1. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 23': None, '1. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 24': None, '1. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 7': None, '1. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 14': None, '1. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 3': None, '1. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 30': None, '1. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 32': None, '1. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 33': None, '1. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 35': None, '1. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 36': None, '1. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 38': None, '1. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 4': None, '1. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 40': None, '1. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 9': None, '1. ece408-lecture9-convolution-analysis-vk-SP24.pdf, page: 11': None, '1. ece408-lecture9-convolution-analysis-vk-SP24.pdf, page: 26': None, '1. ece408-lecture9-convolution-analysis-vk-SP24.pdf, page: 32': None, '1. ece408-lecture9-convolution-analysis-vk-SP24.pdf, page: 33': None, '1. ece408-lecture9-convolution-analysis-vk-SP24.pdf, page: 34': None, '1. ece408-lecture9-convolution-analysis-vk-SP24.pdf, page: 4': None, '10': 'https://uiuc-chatbot.s3.us-east-1.amazonaws.com/courses/ECE408SP24/f1ffcfe0-43a3-42bd-a60b-94fdbab7914f-CUDA_C_Best_Practices_Guide.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAUCD3K6ETL7TF7F5Q%2F20240116%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240116T180316Z&X-Amz-Expires=3600&X-Amz-Signature=67626b5dff91fa65f9f990c524ae607223d1bc1da69be15029e945dc0ca6a3b4&X-Amz-SignedHeaders=host&response-content-disposition=inline&response-content-type=application%2Fpdf&x-id=GetObject#page=63', '10. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 25': None, '10. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 23': None, '10. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 26': None, '10. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 16': None, '10. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 17': None, '10. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 26': None, '10. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 33': None, '10. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 12': None, '10. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 19': None, '10. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 2': None, '10. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 24': None, '10. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 27': None, '10. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 4': None, '10. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 5': None, '10. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 7': None, '10. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 10': None, '10. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 15': None, '10. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 8': None, '10. 3rd-Edition-Chapter09-histogram-FINAL-corrected.pdf, page: 12': None, '10. CUDA_C_Best_Practices_Guide.pdf, page: 50': 'https://uiuc-chatbot.s3.us-east-1.amazonaws.com/courses/ECE408SP24/f1ffcfe0-43a3-42bd-a60b-94fdbab7914f-CUDA_C_Best_Practices_Guide.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAUCD3K6ETL7TF7F5Q%2F20240116%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240116T180316Z&X-Amz-Expires=3600&X-Amz-Signature=67626b5dff91fa65f9f990c524ae607223d1bc1da69be15029e945dc0ca6a3b4&X-Amz-SignedHeaders=host&response-content-disposition=inline&response-content-type=application%2Fpdf&x-id=GetObject#page=63', '10. CUDA_C_Best_Practices_Guide.pdf, page: 67': None, '10. CUDA_C_Programming_Guide.pdf, page: 169': None, '10. CUDA_C_Programming_Guide.pdf, page: 31': None, '10. CUDA_C_Programming_Guide.pdf, page: 339': None, '10. Preparing for Deployment, page: 21': None, '10. Review – Vector Addition Kernel': None, '10. Spring 2024-ECE 408-overview.pdf, page: 4': None, '10. Spring 2024-ECE 408-overview.pdf, page: 6': None, '10. Spring 2024-ECE 408-overview.pdf, page: 8': None, '10. ece408-lecture1-introduction-vk-SP24.pdf, page: 14': None, '10. ece408-lecture1-introduction-vk-SP24.pdf, page: 27': None, '10. ece408-lecture10-ML-DL-vk-SP24.pdf, page: 63': None, '10. ece408-lecture12-CNN-vk-SP24.pdf, page: 17': None, '10. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 35': None, '10. ece408-lecture16-histogram-vk-SP24.pdf, page: 40': None, '10. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 22': None, '10. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 26': None, '10. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 3': None, '10. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 4': None, '10. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 8': None, '10. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf': None, '10. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 3': None, '10. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 26': None, '10. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 3': None, '10. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 40': None, '10. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 7': None, '10. ece408-lecture9-convolution-analysis-vk-SP24.pdf, page: 2': None, \"10]. The lecture's reading material was Chapter 3 of the textbook by Kirk & Hwu[1\": None, '11': None, '11. 3rd-Edition-Chapter01-introduction-Final-corrected.pdf, page: 5': None, '11. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 17': None, '11. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 24': None, '11. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 15': None, '11. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 19': None, '11. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 28': None, '11. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 32': None, '11. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 13': None, '11. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 7': None, '11. 3rd-Edition-Chapter09-histogram-FINAL-corrected.pdf, page: 18': None, '11. 3rd-Edition-Chapter11-merge-FINAL-corrected.pdf, page: 24': None, '11. C++ Language Support, page: 391': None, '11. CUDA_C_Best_Practices_Guide.pdf, page: 23': None, '11. CUDA_C_Programming_Guide.pdf, page: 170': None, '11. Spring 2024-ECE 408-overview.pdf, page: 9': None, '11. ece408-lecture1-introduction-vk-SP24.pdf, page: 14': None, '11. ece408-lecture1-introduction-vk-SP24.pdf, page: 21': None, '11. ece408-lecture11-ML-DL-vk-SP24.pdf, page: 2': None, '11. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 25': None, '11. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 8': None, '11. ece408-lecture14-scan-vk-SP24.pdf, page: 25': None, '11. ece408-lecture15-scan-part2-vk-SP24.pdf, page: 5': None, '11. ece408-lecture2-CUDA-introduction-vk-SP24.pdf, page: 8': None, '11. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 26': None, '11. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 9': None, '11. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 23': None, '11. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 9': None, '11. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 5': None, '11. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 36': None, '11: CUDA_C_Programming_Guide.pdf, page: 492': None, '12': None, '12, page: 43][16': None, '12. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 20': None, '12. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 23': None, '12. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 26': None, '12. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 18': None, '12. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 20': None, '12. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 29': None, '12. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 14': None, '12. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 3': None, '12. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 12': None, '12. CUDA C Programming Guide, page: 163': None, '12. CUDA C++ Programming Guide, Release 12.3': None, '12. CUDA C++ Programming Guide, page: 153': None, '12. CUDA_C_Best_Practices_Guide.pdf, page: 81': None, '12. CUDA_C_Programming_Guide.pdf, page: 175': None, '12. CUDA_C_Programming_Guide.pdf, page: 189': None, '12. CUDA_C_Programming_Guide.pdf, page: 27': None, '12. CUDA_C_Programming_Guide.pdf, page: 303': None, '12. Execution of a CUDA program, page: 6': None, '12. ece408-lecture1-introduction-vk-SP24.pdf, page: 2': None, '12. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 11': None, '12. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 5': None, '12. ece408-lecture15-scan-part2-vk-SP24, page: 2': None, '12. ece408-lecture15-scan-part2-vk-SP24.pdf, page: 18': None, '12. ece408-lecture16-histogram-vk-SP24, page: 43': None, '12. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 19': None, '12. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 27': None, '12. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 15': None, '12. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 18': None, '12. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 14': None, '12. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 8': None, '12. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 3': None, '12. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 12': None, '12. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 18': None, '12. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 25': None, '12. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 26': None, '13': None, '13. 3rd-Edition-Chapter01-introduction-Final-corrected.pdf, page: 3': None, '13. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 21': None, '13. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 5': None, '13. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 6': None, '13. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 16': None, '13. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 21': None, '13. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 23': None, '13. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 5': None, '13. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 19': None, '13. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 3': None, '13. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 17': None, '13. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 19': None, '13. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 23': None, '13. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 8': None, '13. 3rd-Edition-Chapter16-case-study-DNN-FINAL-corrected.pdf, page: 5': None, '13. CUDA C Programming Guide, page: 486': None, '13. CUDA C++ Best Practices Guide, Release 12.3': None, '13. CUDA_C_Programming_Guide.pdf, page: 151': None, '13. CUDA_C_Programming_Guide.pdf, page: 229': None, '13. CUDA_C_Programming_Guide.pdf, page: 23': None, '13. CUDA_C_Programming_Guide.pdf, page: 32': None, '13. CUDA_C_Programming_Guide.pdf, page: 448': None, '13. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 27': None, '13. ece408-lecture14-scan-vk-SP24.pdf, page: 4': None, '13. ece408-lecture15-scan-part2-vk-SP24.pdf, page: 26': None, '13. ece408-lecture16-histogram-vk-SP24.pdf, page: 43': None, '13. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 23': None, '13. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 25': None, '13. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 2': None, '13. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 16': None, '13. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 27': None, '13. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 38': None, '14': None, '14. 3rd-Edition-Chapter01-introduction-Final-corrected.pdf, page: 16': None, '14. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 6': None, '14. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 21': None, '14. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 22': None, '14. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 1': None, '14. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 14': None, '14. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 15': None, '14. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 23': None, '14. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 24': None, '14. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 25': None, '14. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 8': None, '14. 3rd-Edition-Chapter09-histogram-FINAL-corrected.pdf, page: 15': None, '14. CUDA C Programming Guide, page: 485': None, '14. CUDA_C_Best_Practices_Guide.pdf, page: 114': None, '14. CUDA_C_Best_Practices_Guide.pdf, page: 58': None, '14. CUDA_C_Programming_Guide.pdf, page: 202': None, '14. CUDA_C_Programming_Guide.pdf, page: 299': None, '14. CUDA_C_Programming_Guide.pdf, page: 449': None, '14. ece408-lecture1-introduction-vk-SP24.pdf, page: 11': None, '14. ece408-lecture10-ML-DL-vk-SP24.pdf, page: 2': None, '14. ece408-lecture14-scan-vk-SP24.pdf, page: 5': None, '14. ece408-lecture2-CUDA-introduction-vk-SP24.pdf, page: 8': None, '14. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 25': None, '14. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 3': None, '14. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 15': None, '14. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 3': None, '14. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 36': None, '14. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 19': None, '14. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 13': None, '14. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 3': None, '14. ece408-lecture9-convolution-analysis-vk-SP24.pdf, page: 5': None, '15': None, '15. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 21': None, '15. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 29': None, '15. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 22': None, '15. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 7': None, '15. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 22': None, '15. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 23': None, '15. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 7': None, '15. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 8': None, '15. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 19': None, '15. 3rd-Edition-Chapter09-histogram-FINAL-corrected.pdf, page: 15': None, '15. 3rd-Edition-Chapter10-sparse-FINAL-corrected.pdf, page: 7': None, '15. CUDA C++ Programming Guide, page: 460': None, '15. CUDA_C_Programming_Guide.pdf, page: 450': None, '15. CUDA_C_Programming_Guide.pdf, page: 492': None, '15. Spring 2024-ECE 408-overview.pdf, page: 6': None, '15. ece408-lecture15-scan-part2-vk-SP24.pdf, page: 28': None, '15. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 23': None, '15. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 14': None, '15. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 26': None, '15. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 29': None, '15. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 7': None, '15: 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 5': None, '16': None, '16. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 4': None, '16. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 16': None, '16. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 1': None, '16. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 31': None, '16. 3rd-Edition-Chapter06-numerical-consideration-FINAL-corrected.pdf, page: 1': None, '16. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 17': None, '16. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 7': None, '16. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 13': None, '16. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 19': None, '16. 3rd-Edition-Chapter09-histogram-FINAL-corrected, page: 15': None, '16. CUDA_C_Programming_Guide.pdf, page: 152': None, '16. CUDA_C_Programming_Guide.pdf, page: 336': None, '16. CUDA_C_Programming_Guide.pdf, page: 452': None, '16. Spring 2024-ECE 408-overview.pdf, page: 4': None, '16. Spring 2024-ECE 408-overview.pdf, page: 8': None, '16. ece408-lecture1-introduction-vk-SP24.pdf, page: 11': None, '16. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 9': None, '16. ece408-lecture14-scan-vk-SP24.pdf, page: 13': None, '16. ece408-lecture14-scan-vk-SP24.pdf, page: 18': None, '16. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 3': None, '16. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 28': None, '16. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 3': None, '16. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 27': None, '16. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 26': None, '16. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 38': None, '17': None, '17. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 16': None, '17. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 33': None, '17. 3rd-Edition-Chapter06-numerical-consideration-FINAL-corrected.pdf, page: 12': None, '17. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 14': None, '17. 3rd-Edition-Chapter16-case-study-DNN-FINAL-corrected.pdf, page: 17': None, '17. 3rd-Edition-Chapter16-case-study-DNN-FINAL-corrected.pdf, page: 8': None, '17. CUDA_C_Best_Practices_Guide.pdf, page: 91': None, '17. CUDA_C_Programming_Guide.pdf, page: 17': None, '17. CUDA_C_Programming_Guide.pdf, page: 455': None, '17. Spring 2024-ECE 408-overview.pdf, page: 8': None, '17. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 157': None, '17. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 24': None, '17. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 38': None, '17. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 8': None, '17. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 23': None, '17. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 3': None, '17. ece408-lecture8-tiled-convolution-vk-SP24.pdf': None, '17. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 19': None, '17. ece408-lecture9-convolution-analysis-vk-SP24.pdf, page: 19': None, '18': None, '18, 9]. You should also be familiar with the CUDA memory model, as discussed in Lecture 4 [2': None, '18. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 26': None, '18. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 26': None, '18. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 31': None, '18. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 10': None, '18. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 22': None, '18. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 31': None, '18. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 22': None, '18. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 5': None, '18. CUDA C++ Programming Guide, page: 3': None, '18. CUDA_C_Programming_Guide.pdf, page: 151': None, '18. CUDA_C_Programming_Guide.pdf, page: 456': None, '18. Chapter 1': None, '18. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 26': None, '18. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 3': None, '18. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 14': None, '18. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 25': None, '18. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 38': None, '19': None, '19. 3rd-Edition-Chapter01-introduction-Final-corrected.pdf, page: 19': None, '19. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 3': None, '19. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 14': None, '19. 3rd-Edition-Chapter06-numerical-consideration-FINAL-corrected.pdf, page: 12': None, '19. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 13': None, '19. CUDA C Best Practices Guide, page: 110': None, '19. CUDA_C_Best_Practices_Guide.pdf, page: 58': None, '19. CUDA_C_Programming_Guide.pdf, page: 457': None, '19. Spring 2024-ECE 408-overview.pdf, page: 4': None, '19. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 9': None, '19. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 3': None, '19. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 6': None, '1][3][4][5][10][11][12][13][21][22][23][25][26][27][31][32][33][34][36][37][39][40][42][43][44][45][46': None, '2': 'https://uiuc-chatbot.s3.us-east-1.amazonaws.com/courses/ECE408SP24/f69627cb-3930-47cb-a4b3-18ab0b0c96b7-CUDA_C_Programming_Guide.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAUCD3K6ETL7TF7F5Q%2F20240116%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240116T180310Z&X-Amz-Expires=3600&X-Amz-Signature=adf9e09d38c3b51a59abb649a6916619ca18b5b231955ae9becb590b0616d591&X-Amz-SignedHeaders=host&response-content-disposition=inline&response-content-type=application%2Fpdf&x-id=GetObject#page=164', '2, page: 453]. Additionally, for devices of compute capability 6.0 or higher, global memory loads and stores by threads of a warp are coalesced by the device into as few transactions as possible, and these accesses are cached in L1 and L2 by default [21': None, '2, page: 8]. It enables parallelization by allowing each thread to work on a private copy of the output and then combining these private copies to form the final result [2': None, '2. 20200416_nsight.pdf, page: 10': None, '2. 20200416_nsight.pdf, page: 5': None, '2. 20200416_nsight.pdf, page: 58': None, '2. 3rd-Edition-Chapter01-introduction-Final-corrected.pdf, page: 3': None, '2. 3rd-Edition-Chapter01-introduction-Final-corrected.pdf, page: 5': None, '2. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 12': None, '2. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 4': None, '2. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 5': None, '2. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 16': None, '2. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 22': None, '2. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 28': None, '2. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 29': None, '2. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 32': None, '2. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 12': None, '2. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 16': None, '2. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 21': None, '2. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 3': None, '2. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 30': None, '2. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 34': None, '2. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 11': None, '2. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 13': None, '2. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 17': None, '2. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 18': None, '2. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 2': None, '2. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 20': None, '2. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 21': None, '2. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 27': None, '2. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 3': None, '2. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 5': None, '2. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 6': None, '2. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 7': None, '2. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, pages: 12-13': None, '2. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 1': None, '2. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 13': None, '2. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 17': None, '2. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 19': None, '2. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 24': None, '2. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 4': None, '2. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 5': None, '2. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 13': None, '2. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 8': None, '2. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 9': None, '2. 3rd-Edition-Chapter09-histogram-FINAL-corrected.pdf, page: 13': None, '2. 3rd-Edition-Chapter09-histogram-FINAL-corrected.pdf, page: 17': None, '2. 3rd-Edition-Chapter10-sparse-FINAL-corrected.pdf, page: 3': None, '2. 3rd-Edition-Chapter16-case-study-DNN-FINAL-corrected.pdf': None, '2. CUDA C Best Practices Guide, page: 114': None, '2. CUDA C Programming Guide, page: 153': None, '2. CUDA C Programming Guide, page: 30': None, '2. CUDA C++ Best Practices Guide, page: 50': None, '2. CUDA C++ Programming Guide, page: 55': None, '2. CUDA_C_Best_Practices_Guide.pdf, page: 81': None, '2. CUDA_C_Programming_Guide.pdf, page: 109': None, '2. CUDA_C_Programming_Guide.pdf, page: 164': 'https://uiuc-chatbot.s3.us-east-1.amazonaws.com/courses/ECE408SP24/f69627cb-3930-47cb-a4b3-18ab0b0c96b7-CUDA_C_Programming_Guide.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAUCD3K6ETL7TF7F5Q%2F20240116%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240116T180310Z&X-Amz-Expires=3600&X-Amz-Signature=adf9e09d38c3b51a59abb649a6916619ca18b5b231955ae9becb590b0616d591&X-Amz-SignedHeaders=host&response-content-disposition=inline&response-content-type=application%2Fpdf&x-id=GetObject#page=164', '2. CUDA_C_Programming_Guide.pdf, page: 189': None, '2. CUDA_C_Programming_Guide.pdf, page: 19': None, '2. CUDA_C_Programming_Guide.pdf, page: 21': None, '2. CUDA_C_Programming_Guide.pdf, page: 331': None, '2. CUDA_C_Programming_Guide.pdf, page: 373': None, '2. CUDA_C_Programming_Guide.pdf, page: 42': None, '2. CUDA_C_Programming_Guide.pdf, page: 44': None, '2. CUDA_C_Programming_Guide.pdf, page: 456': None, '2. CUDA_C_Programming_Guide.pdf, page: 65': None, '2. Spring 2024-ECE 408-overview.pdf, page: 10': None, '2. Spring 2024-ECE 408-overview.pdf, page: 6': None, '2. ece408-lecture1-introduction-vk-SP24.pdf, page: 14': None, '2. ece408-lecture10-ML-DL-vk-SP24.pdf, page: 53': None, '2. ece408-lecture12-CNN-vk-SP24.pdf, page: 44': None, '2. ece408-lecture12-CNN-vk-SP24.pdf, page: 45': None, '2. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 3': None, '2. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 36': None, '2. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 4': None, '2. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 42': None, '2. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 7': None, '2. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 8': None, '2. ece408-lecture14-scan-vk-SP24.pdf, page: 28': None, '2. ece408-lecture15-scan-part2-vk-SP24.pdf, page: 13': None, '2. ece408-lecture15-scan-part2-vk-SP24.pdf, page: 15': None, '2. ece408-lecture15-scan-part2-vk-SP24.pdf, page: 5': None, '2. ece408-lecture2-CUDA-introduction-vk-SP24.pdf, page: 22': None, '2. ece408-lecture2-CUDA-introduction-vk-SP24.pdf, page: 31': None, '2. ece408-lecture2-CUDA-introduction-vk-SP24.pdf, page: 8': None, '2. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf': None, '2. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 23': None, '2. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 25': None, '2. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 30': None, '2. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 2': None, '2. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 27': None, '2. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24, page: 31': None, '2. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 31': None, '2. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 34': None, '2. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 8': None, '2. ece408-lecture6-dram-tiling-vk-SP24.pdf, page: 33': None, '2. ece408-lecture6-dram-tiling-vk-SP24.pdf, page: 39': None, '2. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 2': None, '2. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 22': None, '2. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 23': None, '2. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 24': None, '2. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 6': None, '2. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 1': None, '2. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 10': None, '2. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 11': None, '2. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 14': None, '2. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 27': None, '2. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 3': None, '2. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 33': None, '2. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 35': None, '2. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 36': None, '2. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 38': None, '2. ece408-lecture9-convolution-analysis-vk-SP24.pdf, page: 25': None, '2. ece408-lecture9-convolution-analysis-vk-SP24.pdf, page: 5': None, '2. ece408-lecture9-convolution-analysis-vk-SP24.pdf, page: 6': None, '20': None, '20. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 19': None, '20. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 23': None, '20. 3rd-Edition-Chapter12-graph-FINAL-corrected.pdf, page: 17': None, '20. 3rd-Edition-Chapter16-case-study-DNN-FINAL-corrected.pdf, page: 5': None, '20. CUDA C Best Practices Guide, page: 109': None, '20. CUDA_C_Programming_Guide.pdf, page: 189': None, '20. CUDA_C_Programming_Guide.pdf, page: 302': None, '20. CUDA_C_Programming_Guide.pdf, page: 460': None, '20. Spring 2024-ECE 408-overview.pdf': None, '20. Spring 2024-ECE 408-overview.pdf, page: 4': None, '20. Technical Specifications per Compute Capability, page: 449': None, '20. ece408-lecture1-introduction-vk-SP24.pdf, page: 2': None, '20. ece408-lecture1-introduction-vk-SP24.pdf, page: 27': None, '20. ece408-lecture14-scan-vk-SP24.pdf, page: 5': None, '20. ece408-lecture15-scan-part2-vk-SP24.pdf, page: 4': None, '20. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 14': None, '21': None, '21,22, page: 8][47': None, '21. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 1': None, '21. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 32': None, '21. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 17': None, '21. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 11': None, '21. 3rd-Edition-Chapter09-histogram-FINAL-corrected.pdf, page: 14': None, '21. CUDA C++ Programming Guide, page: 453': None, '21. CUDA_C_Programming_Guide.pdf, page: 461': None, '21. Spring 2024-ECE 408-overview.pdf, page: 10': None, '21. ece408-lecture15-scan-part2-vk-SP24.pdf, page: 5': None, '21. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 3': None, '21. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 26': None, '22': None, '22. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 22': None, '22. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 24': None, '22. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 32': None, '22. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 10': None, '22. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 2': None, '22. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 7': None, '22. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 22': None, '22. CUDA_C_Best_Practices_Guide, page: 43': None, '22. CUDA_C_Programming_Guide, page: 156': None, '22. CUDA_C_Programming_Guide.pdf, page: 162': None, '22. CUDA_C_Programming_Guide.pdf, page: 23': None, '22. CUDA_C_Programming_Guide.pdf, page: 462': None, '22. Chapter 5: Performance Considerations, page: 2': None, '22. Double Buffering': None, '22. Spring 2024-ECE 408-overview.pdf, page: 4': None, '22. ece408-lecture2-CUDA-introduction-vk-SP24.pdf, page: 8': None, '22. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 38': None, '22. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 1': None, '23': None, '23. 3rd-Edition-Chapter01-introduction-Final-corrected, page: 5': None, '23. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 20': None, '23. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 15': None, '23. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 25': None, '23. CUDA_C_Best_Practices_Guide.pdf, page: 57': None, '23. CUDA_C_Programming_Guide, page: 163': None, '23. CUDA_C_Programming_Guide.pdf, page: 308': None, '23. CUDA_C_Programming_Guide.pdf, page: 463': None, '23. ece408-lecture1-introduction-vk-SP24, page: 9': None, '23. ece408-lecture1-introduction-vk-SP24.pdf, page: 17': None, '23. ece408-lecture1-introduction-vk-SP24.pdf, page: 19': None, '23. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 3': None, '23. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 24': None, '23. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 7': None, '23. ece408-lecture9-convolution-analysis-vk-SP24.pdf, page: 5': None, '24': None, '24. 20200416_nsight.pdf, page: 5': None, '24. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected, page: 17': None, '24. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 2': None, '24. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 23': None, '24. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 27': None, '24. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 23': None, '24. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 22': None, '24. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 4': None, '24. CUDA C++ Programming Guide, page: 153': None, '24. CUDA_C_Programming_Guide.pdf, page: 307': None, '24. CUDA_C_Programming_Guide.pdf, page: 463': None, '24. CUDA_C_Programming_Guide.pdf, page: 464': None, '24. Spring 2024-ECE 408-overview.pdf, page: 8': None, '24. ece408-lecture1-introduction-vk-SP24.pdf, page: 9': None, '24. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 17': None, '24. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 25': None, '24. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 3': None, '24. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 5': None, '24. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 14': None, '24. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 32': None, '24. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 13': None, '25': None, '25. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 5': None, '25. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 9': None, '25. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 6': None, '25. CUDA C++ Best Practices Guide, Release 12.3': None, '25. CUDA C++ Programming Guide, page: 31': None, '25. CUDA_C_Best_Practices_Guide.pdf, page: 19': None, '25. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf': None, '25. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 302': None, '25. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 35': None, '25. ece408-lecture6-dram-tiling-vk-SP24.pdf, page: 4': None, '25: CUDA C Programming Guide, page: 43': None, '25]. When threads within a warp encounter a divergent branch, the warp must serialize the execution of the different paths, which can lead to performance degradation[2': None, '26': None, '26. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 23': None, '26. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 23': None, '26. CUDA C++ Best Practices Guide, page: 57': None, '26. CUDA C++ Programming Guide, page: 28': None, '26. CUDA_C_Best_Practices_Guide.pdf, page: 21': None, '26. CUDA_C_Programming_Guide.pdf, page: 162': None, '26. ece408-lecture1-introduction-vk-SP24.pdf, page: 13': None, '26. ece408-lecture8-tiled-convolution-vk-SP24.pdf': None, '26. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 14': None, '26. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 15': None, '26. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 3': None, '27': None, '27. 3rd-Edition-Chapter01-introduction-Final-corrected.pdf, page: 3': None, '27. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 10': None, '27. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 23': None, '27. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 18-19': None, '27. 3rd-Edition-Chapter09-histogram-FINAL-corrected.pdf, page: 14': None, '27. CUDA_C_Best_Practices_Guide.pdf, page: 33': None, '27. Spring 2024-ECE 408-overview.pdf, page: 4': None, '27. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 17': None, '27. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 13': None, '27. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 30': None, '27. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 39': None, '27. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 7': None, '28': None, '28. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 19': None, '28. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 23': None, '28. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 27': None, '28. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 3': None, '28. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 18': None, '28. CUDA C++ Programming Guide, page: 453': None, '28. CUDA_C_Best_Practices_Guide.pdf, page: 69': None, '28. CUDA_C_Programming_Guide.pdf': None, '28. Chapter 5: Performance Considerations, page: 3': None, '28. Spring 2024-ECE 408-overview.pdf, page: 5': None, '28. Spring 2024-ECE 408-overview.pdf, page: 8': None, '28. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 38': None, '28. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 8': None, '28. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 25': None, '28. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 26': None, '29': None, '29. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 12': None, '29. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 2': None, '29. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 21': None, '29. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 4': None, '29. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 19': None, '29. CUDA C++ Best Practices Guide, page: 67': None, '29. CUDA C++ Programming Guide, page: 450': None, '29. CUDA_C_Best_Practices_Guide.pdf, page: 21': None, '29. CUDA_C_Best_Practices_Guide.pdf, page: 86': None, '29. ece408-lecture14-scan-vk-SP24.pdf, page: 14': None, '29. ece408-lecture6-dram-tiling-vk-SP24.pdf, page: 43': None, '29. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 26': None, '2: CUDA C Programming Guide, page: 43': None, '3': None, '3, 1, 7, 0, 4, 1, 6, 3] would return [3, 4, 11, 11, 15, 16, 22, 25], where each element is the sum of all preceding elements in the input array and itself[3': None, '3, page: 151]. When a CUDA program on the host CPU invokes a kernel grid, the blocks of the grid are enumerated and distributed to multiprocessors with available execution capacity[3': None, '3, page: 27]. This strategy involves some threads not participating in calculating the output, which is a trade-off to avoid branch divergence during the loading phase [12': None, '3, page: 31][12, page: 189]. When a thread reaches this point in the code, it will wait until all other threads in the block have also reached this point before any of them can proceed[22': None, '3. 3rd-Edition-Chapter01-introduction-Final-corrected.pdf, page: 15': None, '3. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 12': None, '3. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 13': None, '3. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 6': None, '3. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 17': None, '3. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 21': None, '3. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 27': None, '3. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 29': None, '3. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 30': None, '3. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 5': None, '3. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 15': None, '3. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 20': None, '3. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 33': None, '3. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 4': None, '3. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 9': None, '3. 3rd-Edition-Chapter05-performance-FINAL-corrected, page: 23': None, '3. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 17': None, '3. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 18': None, '3. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 19': None, '3. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 2': None, '3. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 20': None, '3. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 21': None, '3. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 28': None, '3. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 31': None, '3. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 5': None, '3. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 1': None, '3. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 10': None, '3. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 11': None, '3. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 2': None, '3. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 22': None, '3. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 23': None, '3. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 3': None, '3. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 17': None, '3. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 2': None, '3. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 22': None, '3. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 9': None, '3. 3rd-Edition-Chapter09-histogram-FINAL-corrected.pdf, page: 15': None, '3. 3rd-Edition-Chapter10-sparse-FINAL-corrected.pdf, page: 2': None, '3. 3rd-Edition-Chapter16-case-study-DNN-FINAL-corrected.pdf, page: 8': None, '3. 4th-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 3': None, '3. CUDA C Programming Guide, page: 151': None, '3. CUDA C Programming Guide, page: 152': None, '3. CUDA C++ Best Practices Guide, page: 50': None, '3. CUDA_C_Best_Practices_Guide.pdf, page: 42': None, '3. CUDA_C_Best_Practices_Guide.pdf, page: 50': None, '3. CUDA_C_Best_Practices_Guide.pdf, page: 81': None, '3. CUDA_C_Best_Practices_Guide.pdf, page: 90': None, '3. CUDA_C_Programming_Guide.pdf, page: 151': None, '3. CUDA_C_Programming_Guide.pdf, page: 157': None, '3. CUDA_C_Programming_Guide.pdf, page: 172': None, '3. CUDA_C_Programming_Guide.pdf, page: 189': None, '3. CUDA_C_Programming_Guide.pdf, page: 19': None, '3. CUDA_C_Programming_Guide.pdf, page: 254': None, '3. CUDA_C_Programming_Guide.pdf, page: 337': None, '3. CUDA_C_Programming_Guide.pdf, page: 35': None, '3. CUDA_C_Programming_Guide.pdf, page: 44': None, '3. CUDA_C_Programming_Guide.pdf, page: 456': None, '3. CUDA_C_Programming_Guide.pdf, page: 507': None, '3. ece408-lecture1-introduction-vk-SP24.pdf, page: 13': None, '3. ece408-lecture1-introduction-vk-SP24.pdf, page: 26': None, '3. ece408-lecture12-CNN-vk-SP24.pdf, page: 46': None, '3. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 15': None, '3. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 2': None, '3. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 3': None, '3. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 34': None, '3. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 9': None, '3. ece408-lecture14-scan-vk-SP24.pdf, page: 28': None, '3. ece408-lecture15-scan-part2-vk-SP24.pdf, page: 15': None, '3. ece408-lecture15-scan-part2-vk-SP24.pdf, page: 18': None, '3. ece408-lecture15-scan-part2-vk-SP24.pdf, page: 2': None, '3. ece408-lecture15-scan-part2-vk-SP24.pdf, page: 5': None, '3. ece408-lecture16-histogram-vk-SP24.pdf, page: 15': None, '3. ece408-lecture2-CUDA-introduction-vk-SP24.pdf, page: 22': None, '3. ece408-lecture2-CUDA-introduction-vk-SP24.pdf, page: 5': None, '3. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 23': None, '3. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 25': None, '3. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 26': None, '3. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 27': None, '3. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 29': None, '3. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 27': None, '3. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 3': None, '3. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 31': None, '3. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 34': None, '3. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 36': None, '3. ece408-lecture6-dram-tiling-vk-SP24.pdf, page: 13': None, '3. ece408-lecture6-dram-tiling-vk-SP24.pdf, page: 31': None, '3. ece408-lecture6-dram-tiling-vk-SP24.pdf, page: 33': None, '3. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 15': None, '3. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 21': None, '3. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 23': None, '3. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 24': None, '3. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 3': None, '3. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 4': None, '3. ece408-lecture8-tiled-convolution-vk-SP24.pdf': None, '3. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 14': None, '3. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 17': None, '3. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 27': None, '3. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 3': None, '3. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 38': None, '3. ece408-lecture9-convolution-analysis-vk-SP24.pdf, page: 13': None, '3. ece408-lecture9-convolution-analysis-vk-SP24.pdf, page: 2': None, '3. nvcc Compiler Switches, page: 115': None, '30': None, '30. CUDA C++ Programming Guide, page: 151': None, '30. CUDA_C_Best_Practices_Guide.pdf, page: 58': None, '30. CUDA_C_Programming_Guide.pdf, page: 202': None, '30. CUDA_C_Programming_Guide.pdf, page: 320': None, '30. ece408-lecture1-introduction-vk-SP24.pdf, page: 13': None, '30. ece408-lecture1-introduction-vk-SP24.pdf, page: 30': None, '30. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 17': None, '30. ece408-lecture14-scan-vk-SP24.pdf, page: 16': None, '30. ece408-lecture6-dram-tiling-vk-SP24.pdf, page: 4': None, '30. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 14': None, '30. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 38': None, '31': None, '31. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 10': None, '31. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 5': None, '31. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 13': None, '31. 3rd-Edition-Chapter09-histogram-FINAL-corrected.pdf, page: 5': None, '31. CUDA_C_Programming_Guide.pdf, page: 151': None, '31. Chapter 5: Performance Considerations, page: 5': None, '31. ece408-lecture1-introduction-vk-SP24.pdf, page: 9': None, '31. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 7': None, '32': None, '32. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 12': None, '32. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 22': None, '32. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 29': None, '32. CUDA_C_Best_Practices_Guide.pdf, page: 69': None, '32. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 21': None, '32. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 32': None, '32. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 6': None, '32. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 3': None, '33': None, '33. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 28': None, '33. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 31': None, '33. 3rd-Edition-Chapter06-numerical-consideration-FINAL-corrected.pdf, page: 12': None, '33. CUDA_C_Programming_Guide.pdf, page: 177': None, '33. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 28': None, '33. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 26': None, '33. ece408-lecture9-convolution-analysis-vk-SP24.pdf, page: 5': None, '34': None, '34. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 28': None, '34. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 21': None, '34. 3rd-Edition-Chapter09-histogram-FINAL-corrected.pdf, page: 1': None, '34. CUDA_C_Best_Practices_Guide.pdf, page: 69': None, '34. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 19': None, '34. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf': None, '34. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 3': None, '35': None, '35. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 6': None, '35. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 22': None, '35. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 5': None, '35. CUDA_C_Best_Practices_Guide.pdf, page: 72': None, '35. ece408-lecture1-introduction-vk-SP24.pdf, page: 15': None, '35. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 40': None, '35. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf': None, '35. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 13': None, '35. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 35': None, '36': None, '36. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 9': None, '36. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 24': None, '36. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 3': None, '36. CUDA_C_Programming_Guide.pdf, page: 403': None, '36. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 3': None, '36. ece408-lecture6-dram-tiling-vk-SP24.pdf, page: 44': None, '36. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 38': None, '36. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 39': None, '37': None, '37, 77]. This tiling technique is a common optimization in CUDA to enhance data locality and reduce global memory bandwidth requirements[12': None, '37. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 9': None, '37. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 9': None, '37. CUDA_C_Programming_Guide.pdf, page: 152': None, '37. CUDA_C_Programming_Guide.pdf, page: 63': None, '37. ece408-lecture12-CNN-vk-SP24.pdf, page: 9': None, '37. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 23': None, '37. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 29': None, '37. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 9': None, '37. ece408-lecture6-dram-tiling-vk-SP24.pdf, page: 3': None, '37. ece408-lecture8-tiled-convolution-vk-SP24, page: 6': None, '37: CUDA_C_Programming_Guide.pdf, page: 177': None, '38': None, \"38, page: 1][44][77]. This is part of a technique known as mini-batch training, where instead of updating the network's weights after computing the gradient for each individual sample, the gradients are accumulated over a batch of samples and the weights are updated based on this accumulated gradient. This approach helps to improve computational efficiency and can also lead to more stable convergence during training [74\": None, '38. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 9': None, '38. 3rd-Edition-Chapter09-histogram-FINAL-corrected.pdf, page: 10': None, '38. A Common Programming Strategy': None, '38. CUDA_C_Programming_Guide.pdf, page: 19': None, '38. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 15': None, '38. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 8': None, '39': None, '39. 3rd-Edition-Chapter09-histogram-FINAL-corrected.pdf, page: 7': None, '39. CUDA C++ Best Practices Guide, Release 12.3': None, '39. CUDA_C_Best_Practices_Guide.pdf, page: 81': None, '39. CUDA_C_Programming_Guide.pdf, page: 16': None, '39. CUDA_C_Programming_Guide.pdf, page: 260': None, '39. Spring 2024-ECE 408-overview.pdf, page: 2': None, '39. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 26': None, '39. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 6': None, '3: 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 2': None, '3] mentions that many heterogeneous computing applications manage overlapped CPU and GPU execution to take advantage of both CPUs and GPUs. This is further supported by Document [6': None, '3rd-Edition-Chapter01-introduction-Final-corrected.pdf, page: 10': None, '3rd-Edition-Chapter01-introduction-Final-corrected.pdf, page: 12': None, '3rd-Edition-Chapter01-introduction-Final-corrected.pdf, page: 14': None, '3rd-Edition-Chapter01-introduction-Final-corrected.pdf, page: 15': None, '3rd-Edition-Chapter01-introduction-Final-corrected.pdf, page: 16': None, '3rd-Edition-Chapter01-introduction-Final-corrected.pdf, page: 20': None, '3rd-Edition-Chapter01-introduction-Final-corrected.pdf, page: 3': None, '3rd-Edition-Chapter01-introduction-Final-corrected.pdf, page: 4': None, '3rd-Edition-Chapter01-introduction-Final-corrected.pdf, page: 5': None, '3rd-Edition-Chapter01-introduction-Final-corrected.pdf, page: 6': None, '3rd-Edition-Chapter01-introduction-Final-corrected.pdf, page: 8': None, '3rd-Edition-Chapter02-data-parallel-execution-FINAL-corrected.pdf, page: 6': None, '3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 10': None, '3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 11': None, '3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 12': None, '3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 13': None, '3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 14': None, '3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 16': None, '3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 17': None, '3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 18': None, '3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 19': None, '3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 20': None, '3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 22': None, '3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 24': None, '3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 4': None, '3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 5': None, '3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 6': None, '3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 1': None, '3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 18': None, '3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 2': None, '3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 23': None, '3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 26': None, '3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 27': None, '3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 28': None, '3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 29': None, '3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 3': None, '3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 30': None, '3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 31': None, '3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 32': None, '3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 5': None, '3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 6': None, '3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 1': None, '3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 11': None, '3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 12': None, '3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 14': None, '3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 16': None, '3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 17': None, '3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 20': None, '3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 21': None, '3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 22': None, '3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 23': None, '3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 26': None, '3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 27': None, '3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 30': None, '3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 31': None, '3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 32': None, '3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 33': None, '3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 8': None, '3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 9': None, '3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 12': None, '3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 13': None, '3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 17': None, '3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 18': None, '3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 19': None, '3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 2': None, '3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 20': None, '3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 26': None, '3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 28': None, '3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 3': None, '3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 12': None, '3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 14': None, '3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 17': None, '3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 18': None, '3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 21': None, '3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 23': None, '3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 24': None, '3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 7': None, '3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 11': None, '3rd-Edition-Chapter10-sparse-FINAL-corrected.pdf, page: 7': None, '4': None, '4*i] and A[4*i+1], the threads are accessing memory locations that are not consecutive but strided, with a stride of 4 floating-point numbers (assuming each floating-point number is 4 bytes). This means that in a burst of 512 bytes, only every other 8 bytes are used, as the threads skip over elements to access the next one in the pattern[1': None, '4. 3rd-Edition-Chapter01-introduction-Final-corrected.pdf, page: 7': None, '4. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 12': None, '4. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 4': None, '4. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 6': None, '4. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected, page: 21': None, '4. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf': None, '4. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 11': None, '4. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 20': None, '4. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 27': None, '4. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 31': None, '4. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 11': None, '4. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 15': None, '4. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 16': None, '4. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 31': None, '4. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 32': None, '4. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 33': None, '4. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 11': None, '4. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 15': None, '4. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 17': None, '4. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 19': None, '4. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 2': None, '4. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 20': None, '4. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 21': None, '4. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 24': None, '4. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 8': None, '4. 3rd-Edition-Chapter06-numerical-consideration-FINAL-corrected.pdf, page: 10': None, '4. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 14': None, '4. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 23': None, '4. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 24': None, '4. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 4': None, '4. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 5': None, '4. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 6': None, '4. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 9': None, '4. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 13': None, '4. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 15': None, '4. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 21': None, '4. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 4': None, '4. 3rd-Edition-Chapter11-merge-FINAL-corrected.pdf, page: 21': None, '4. CUDA C Programming Guide, page: 151': None, '4. CUDA_C_Best_Practices_Guide.pdf, page: 81': None, '4. CUDA_C_Best_Practices_Guide.pdf, page: 87': None, '4. CUDA_C_Programming_Guide, page: 151': None, '4. CUDA_C_Programming_Guide.pdf, page: 151': None, '4. CUDA_C_Programming_Guide.pdf, page: 153': None, '4. CUDA_C_Programming_Guide.pdf, page: 163': None, '4. CUDA_C_Programming_Guide.pdf, page: 172': None, '4. CUDA_C_Programming_Guide.pdf, page: 254': None, '4. CUDA_C_Programming_Guide.pdf, page: 27': None, '4. CUDA_C_Programming_Guide.pdf, page: 36': None, '4. CUDA_C_Programming_Guide.pdf, page: 453': None, '4. CUDA_C_Programming_Guide.pdf, page: 72': None, '4. Compilation with NVCC, page: 38': None, '4. Hardware Implementation, page: 151': None, '4. Spring 2024-ECE 408-overview.pdf, page: 8': None, '4. ece408-lecture10-ML-DL-vk-SP24.pdf, page: 53': None, '4. ece408-lecture12-CNN-vk-SP24.pdf, page: 2': None, '4. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 13': None, '4. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 22': None, '4. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 3': None, '4. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 36': None, '4. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 7': None, '4. ece408-lecture14-scan-vk-SP24.pdf, page: 15': None, '4. ece408-lecture14-scan-vk-SP24.pdf, page: 8': None, '4. ece408-lecture17-sparse-matrix-vk-SP24.pdf, page: 11': None, '4. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 23': None, '4. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 26': None, '4. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 32': None, '4. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 9': None, '4. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 13': None, '4. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 18': None, '4. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 29': None, '4. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 30': None, '4. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 31': None, '4. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 8': None, '4. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 1': None, '4. ece408-lecture6-dram-tiling-vk-SP24.pdf, page: 43': None, '4. ece408-lecture6-dram-tiling-vk-SP24.pdf, page: 45': None, '4. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 13': None, '4. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 2': None, '4. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 25': None, '4. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 6': None, '4. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 7': None, '4. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 11': None, '4. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 14': None, '4. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 26': None, '4. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 33': None, '4. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 38': None, '4. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 40': None, '4. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 73': None, '4. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 9': None, '4. ece408-lecture9-convolution-analysis-vk-SP24.pdf, page: 5': None, '40': None, '40. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 24': None, '40. CUDA_C_Programming_Guide.pdf, page: 172': None, '40. CUDA_C_Programming_Guide.pdf, page: 499': None, '40. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 19': None, '40. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 11': None, '41': None, '41. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 19': None, '41. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 8': None, '41. 3rd-Edition-Chapter11-merge-FINAL-corrected.pdf, page: 20': None, '41. CUDA C++ Best Practices Guide, page: 61': None, '41. CUDA_C_Best_Practices_Guide.pdf, page: 73': None, '41. CUDA_C_Programming_Guide.pdf, page: 151': None, '41. CUDA_C_Programming_Guide.pdf, page: 232': None, '41. CUDA_C_Programming_Guide.pdf, page: 449': None, '41. Problem Solving': None, '41. Spring 2024-ECE 408-overview, page: 4': None, '41. ece408-lecture15-scan-part2-vk-SP24.pdf, page: 14': None, '41. ece408-lecture15-scan-part2-vk-SP24.pdf, page: 7': None, '41. ece408-lecture2-CUDA-introduction-vk-SP24.pdf, page: 21': None, '41. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 18': None, '41. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 14': None, '41. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 26': None, '41. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 34': None, '42': None, '42. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 1': None, '42. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 20': None, '42. CUDA C++ Best Practices Guide, page: 54': None, '42. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 28': None, '43': None, '43. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 1': None, '43. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 3': None, '43. CUDA_C_Programming_Guide.pdf, page: 43': None, '43. ece408-lecture1-introduction-vk-SP24, page: 14': None, '43. ece408-lecture6-dram-tiling-vk-SP24.pdf, page: 4': None, '44': None, '44. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 17': None, '44. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 9': None, '44. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 2': None, '44. CUDA C++ Best Practices Guide, page: 58': None, '44. CUDA_C_Programming_Guide.pdf, page: 454': None, '44. ece408-lecture1-introduction-vk-SP24.pdf, page: 16': None, '44. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 13': None, '44. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 8': None, '44. ece408-lecture8-tiled-convolution-vk-SP24, page: 10': None, '45': None, '45. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 4': None, '45. CUDA C++ Best Practices Guide, page: 45': None, '45. CUDA_C_Best_Practices_Guide.pdf, page: 50': None, '45. CUDA_C_Programming_Guide.pdf, page: 151': None, '45. ece408-lecture14-scan-vk-SP24.pdf, page: 15': None, '45. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 34': None, '45. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 6': None, '45. ece408-lecture9-convolution-analysis-vk-SP24.pdf, page: 5': None, '46': None, '46. 20200416_nsight.pdf, page: 10': None, '46. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 1': None, '46. CUDA C++ Best Practices Guide, Release 12.3': None, '46. Spring 2024-ECE 408-overview.pdf, page: 10': None, '46. ece408-lecture1-introduction-vk-SP24.pdf, page: 27': None, '46. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 24': None, '46. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf': None, '46. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 13': None, '46. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 38': None, '47': None, '47. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 12': None, '47. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 21': None, '47. CUDA_C_Programming_Guide.pdf, page: 254': None, '47. ece408-lecture1-introduction-vk-SP24.pdf, page: 6': None, '47. ece408-lecture2-CUDA-introduction-vk-SP24.pdf, page: 7': None, '47. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 12': None, '47. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 6': None, '48': None, '48. CUDA_C_Programming_Guide.pdf, page: 450': None, '48. ece408-lecture15-scan-part2-vk-SP24.pdf, page: 2': None, '48. ece408-lecture8-tiled-convolution-vk-SP24.pdf': None, '49': None, '49. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 3': None, '49. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 13': None, '49. 3rd-Edition-Chapter16-case-study-DNN-FINAL-corrected.pdf, page: 23': None, '49. CUDA_C_Programming_Guide.pdf, page: 326': None, '49. ece408-lecture2-CUDA-introduction-vk-SP24.pdf, page: 9': None, '49. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 27': None, '4]. This overlapping is possible because while the CPU and the GPU are separate devices with different architectural designs and memory spaces, they can be programmed to operate concurrently [79]. The CPU is optimized for sequential tasks with a focus on minimizing latency, while the GPU is designed for parallel tasks to maximize throughput [1], [15': None, '5': 'https://uiuc-chatbot.s3.us-east-1.amazonaws.com/courses/ECE408SP24/f1ffcfe0-43a3-42bd-a60b-94fdbab7914f-CUDA_C_Best_Practices_Guide.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAUCD3K6ETL7TF7F5Q%2F20240116%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240116T180323Z&X-Amz-Expires=3600&X-Amz-Signature=f0fc89d052163d643d0df133fee5bc6c4f900441f2be95058e142713c832c597&X-Amz-SignedHeaders=host&response-content-disposition=inline&response-content-type=application%2Fpdf&x-id=GetObject#page=63', '5. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 3': None, '5. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 4': None, '5. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 12': None, '5. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 19': None, '5. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 23': None, '5. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 13': None, '5. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 3': None, '5. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 32': None, '5. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 19': None, '5. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 27': None, '5. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 3': None, '5. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 5': None, '5. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 8': None, '5. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 13': None, '5. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 16': None, '5. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 2': None, '5. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 4': None, '5. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 12': None, '5. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 15': None, '5. 3rd-Edition-Chapter09-histogram-FINAL-corrected.pdf, page: 8': None, '5. 3rd-Edition-Chapter10-sparse-FINAL-corrected.pdf, page: 5': None, '5. 3rd-Edition-Chapter12-graph-FINAL-corrected.pdf, page: 15': None, '5. 3rd-Edition-Chapter16-case-study-DNN-FINAL-corrected.pdf, page: 16': None, '5. 3rd-Edition-Chapter16-case-study-DNN-FINAL-corrected.pdf, page: 24': None, '5. CUDA C Programming Guide, page: 156': None, '5. CUDA_C_Best_Practices_Guide.pdf, page: 3': None, '5. CUDA_C_Best_Practices_Guide.pdf, page: 4': None, '5. CUDA_C_Best_Practices_Guide.pdf, page: 57': None, '5. CUDA_C_Best_Practices_Guide.pdf, page: 63': 'https://uiuc-chatbot.s3.us-east-1.amazonaws.com/courses/ECE408SP24/f1ffcfe0-43a3-42bd-a60b-94fdbab7914f-CUDA_C_Best_Practices_Guide.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAUCD3K6ETL7TF7F5Q%2F20240116%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240116T180323Z&X-Amz-Expires=3600&X-Amz-Signature=f0fc89d052163d643d0df133fee5bc6c4f900441f2be95058e142713c832c597&X-Amz-SignedHeaders=host&response-content-disposition=inline&response-content-type=application%2Fpdf&x-id=GetObject#page=63', '5. CUDA_C_Best_Practices_Guide.pdf, page: 81': None, '5. CUDA_C_Programming_Guide.pdf, page: 153': None, '5. CUDA_C_Programming_Guide.pdf, page: 17': None, '5. CUDA_C_Programming_Guide.pdf, page: 172': None, '5. CUDA_C_Programming_Guide.pdf, page: 37': None, '5. CUDA_C_Programming_Guide.pdf, page: 504': None, '5. CUDA_C_Programming_Guide.pdf, page: 63': None, '5. CUDA_C_Programming_Guide.pdf, page: 66': None, '5. Spring 2024-ECE 408-overview.pdf, page: 8': None, '5. Understanding the Programming Environment, page: 90': None, '5. ece408-lecture1-introduction-vk-SP24.pdf, page: 14': None, '5. ece408-lecture11-ML-DL-vk-SP24.pdf, page: 2': None, '5. ece408-lecture11-ML-DL-vk-SP24.pdf, page: 4': None, '5. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 15': None, '5. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 2': None, '5. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 9': None, '5. ece408-lecture14-scan-vk-SP24.pdf, page: 28': None, '5. ece408-lecture15-scan-part2-vk-SP24.pdf, page: 5': None, '5. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 13': None, '5. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 24': None, '5. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 25': None, '5. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 26': None, '5. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 27': None, '5. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 32': None, '5. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 13': None, '5. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 18': None, '5. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 12': None, '5. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 34': None, '5. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 38': None, '5. ece408-lecture6-dram-tiling-vk-SP24, page: 2': None, '5. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 16': None, '5. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 21': None, '5. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 27': None, '5. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 4': None, '5. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 19': None, '5. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 2': None, '5. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 25': None, '5. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 26': None, '5. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 3': None, '5.5.1][70': None, '50': None, '50. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 20': None, '50. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 28': None, '50. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 23': None, '50. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 24': None, '50. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 6': None, '50. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 11': None, '50. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 4': None, '50. CUDA C++ Best Practices Guide, page: 72': None, '50. Chapter 5: Performance Considerations, page: 13': None, '50. Spring 2024-ECE 408-overview.pdf, page: 8': None, '50. ece408-lecture1-introduction-vk-SP24, page: 16': None, '50. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 11': None, '50. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf': None, '50. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 26': None, '50. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 13': None, '51': None, '51. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 31': None, '51. CUDA C++ Programming Guide': None, '51. ece408-lecture2-CUDA-introduction-vk-SP24.pdf, page: 11': None, '51. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 13': None, '51. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 20': None, '52': None, '52. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 21': None, '52. ece408-lecture13-reduction-tree-vk-SP24, page: 2': None, '52. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 16': None, '52. ece408-lecture15-scan-part2-vk-SP24.pdf, page: 18': None, '52. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 5': None, '52. ece408-lecture9-convolution-analysis-vk-SP24, page: 5': None, '53': None, '53. 3rd-Edition-Chapter04-memory-model-FINAL-corrected, page: 12': None, '53. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 22': None, '53. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 14': None, '53. 3rd-Edition-Chapter09-histogram-FINAL-corrected.pdf, page: 15': None, '53. 3rd-Edition-Chapter16-case-study-DNN-FINAL-corrected.pdf, page: 19': None, '53. ece408-lecture1-introduction-vk-SP24, page: 6': None, '53. ece408-lecture14-scan-vk-SP24.pdf, page: 13': None, '53. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 1': None, '54': None, '54. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 29': None, '54. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 8': None, '54. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 8': None, '54. CUDA_C_Best_Practices_Guide.pdf, page: 69': None, '54. CUDA_C_Programming_Guide.pdf, page: 162': None, '54. Chapter 5: Performance Considerations, page: 32': None, '54. Spring 2024-ECE 408-overview.pdf, page: 3': None, '54. Tiled Multiply': None, '54. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 13': None, '54. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 27': None, '54. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf': None, '54. ece408-lecture6-dram-tiling-vk-SP24.pdf, page: 42': None, '55': None, '55. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 3': None, '55. CUDA C++ Best Practices Guide, page: 63': None, '55. CUDA_C_Best_Practices_Guide.pdf, page: 65': None, '55. CUDA_C_Programming_Guide.pdf, page: 499': None, '55. Chapter 4: Memory and Data Locality, page: 5': None, '55. Spring 2024-ECE 408-overview.pdf, page: 3': None, '55. ece408-lecture2-CUDA-introduction-vk-SP24.pdf': None, '55. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 1': None, '55. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24': None, '55. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf': None, '55. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 12': None, '55. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 35': None, '56': None, '56, 57]. If this convolution is part of machine learning tasks, such as training a 3D ConvNet, you will also need to handle aspects like weight initialization, forward and backward passes, and gradient updates [37': None, '56. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 24': None, '56. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf': None, '56. 3rd-Edition-Chapter09-histogram-FINAL-corrected.pdf, page: 15': None, '56. CUDA_C_Programming_Guide.pdf, page: 152': None, '56. Chapter 4: Memory and Data Locality, page: 6': None, '56. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 19': None, '56. ece408-lecture14-scan-vk-SP24.pdf, page: 3': None, '56. ece408-lecture6-dram-tiling-vk-SP24.pdf, page: 13': None, '56. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 26': None, '57': None, '57. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 27': None, '57. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 17': None, '57. 3rd-Edition-Chapter16-case-study-DNN-FINAL-corrected.pdf': None, '57. CUDA C++ Programming Guide, page: 456': None, '57. CUDA_C_Best_Practices_Guide.pdf, page: 45': None, '57. CUDA_C_Programming_Guide.pdf, page: 298': None, '58': None, '58. CUDA C++ Programming Guide, Release 12.3': None, '58. Shared Memory Tiling Basic Idea': None, '58. ece408-lecture9-convolution-analysis-vk-SP24.pdf, page: 32': None, '59': None, '59. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 26': None, '59. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 33': None, '59. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 3': None, '59. CUDA C++ Programming Guide, page: 66': None, '59. ece408-lecture1-introduction-vk-SP24.pdf, page: 15': None, '59. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 15': None, '59. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 9': None, '59. ece408-lecture14-scan-vk-SP24.pdf, page: 21': None, '59. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 10': None, '59. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 28': None, '59. ece408-lecture9-convolution-analysis-vk-SP24, page: 18': None, '5: 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 6': None, '6': None, '6. 3rd-Edition-Chapter01-introduction-Final-corrected.pdf, page: 4': None, '6. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 13': None, '6. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 3': None, '6. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 18': None, '6. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 23': None, '6. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 32': None, '6. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 13': None, '6. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 18': None, '6. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 20': None, '6. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 21': None, '6. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 22': None, '6. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 27': None, '6. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 33': None, '6. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 13': None, '6. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 16': None, '6. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 17': None, '6. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 2': None, '6. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 22': None, '6. 3rd-Edition-Chapter06-numerical-consideration-FINAL-corrected.pdf, page: 10': None, '6. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 13': None, '6. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 16': None, '6. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 22': None, '6. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 3': None, '6. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 8': None, '6. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 23': None, '6. C++ Language Support, page: 395': None, '6. CUDA C Programming Guide, page: 172': None, '6. CUDA_C_Best_Practices_Guide.pdf, page: 69': None, '6. CUDA_C_Programming_Guide.pdf, page: 151': None, '6. CUDA_C_Programming_Guide.pdf, page: 153': None, '6. CUDA_C_Programming_Guide.pdf, page: 161': None, '6. CUDA_C_Programming_Guide.pdf, page: 172': None, '6. CUDA_C_Programming_Guide.pdf, page: 20': None, '6. CUDA_C_Programming_Guide.pdf, page: 21': None, '6. CUDA_C_Programming_Guide.pdf, page: 259': None, '6. CUDA_C_Programming_Guide.pdf, page: 398': None, '6. CUDA_C_Programming_Guide.pdf, page: 45': None, '6. CUDA_C_Programming_Guide.pdf, page: 59': None, '6. ece408-lecture1-introduction-vk-SP24.pdf, page: 11': None, '6. ece408-lecture1-introduction-vk-SP24.pdf, page: 13': None, '6. ece408-lecture12-CNN-vk-SP24.pdf, page: 34': None, '6. ece408-lecture12-CNN-vk-SP24.pdf, page: 41': None, '6. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 22': None, '6. ece408-lecture15-scan-part2-vk-SP24.pdf, page: 2': None, '6. ece408-lecture15-scan-part2-vk-SP24.pdf, page: 5': None, '6. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 24': None, '6. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 25': None, '6. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 26': None, '6. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 27': None, '6. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 32': None, '6. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 8': None, '6. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 11': None, '6. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 27': None, '6. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 31': None, '6. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 35': None, '6. ece408-lecture6-dram-tiling-vk-SP24.pdf, page: 2': None, '6. ece408-lecture6-dram-tiling-vk-SP24.pdf, page: 43': None, '6. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 3': None, '6. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 2': None, '6. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 27': None, '6. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 63': None, '60': None, '60. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 15': None, '60. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 2': None, '60. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 3': None, '60. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 4': None, '60. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 1': None, '60. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 5': None, '60. 3rd-Edition-Chapter09-histogram-FINAL-corrected.pdf, page: 7': None, '60. CUDA_C_Best_Practices_Guide.pdf, page: 70': None, '60. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 19': None, '61': None, '61. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 19': None, '61. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 28': None, '61. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 26': None, '61. Partial Overview of CUDA Memories': None, '61. Spring 2024-ECE 408-overview.pdf, page: 11': None, '61. ece408-lecture1-introduction-vk-SP24.pdf, page: 17': None, '61. ece408-lecture14-scan-vk-SP24.pdf, page: 23': None, '61. ece408-lecture6-dram-tiling-vk-SP24, page: 4': None, '61. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 30': None, '62': None, '62. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 25': None, '62. A Common Programming Strategy': None, '62. CUDA C Programming Guide, page: 163': None, '62. CUDA C++ Programming Guide, page: 163': None, '62. CUDA_C_Programming_Guide.pdf, page: 163': None, '62. CUDA_C_Programming_Guide.pdf, page: 21': None, '62. ece408-lecture14-scan-vk-SP24.pdf, page: 3': None, '62. ece408-lecture9-convolution-analysis-vk-SP24.pdf, page: 17': None, '63': None, '63. CUDA C++ Programming Guide, Release 12.3': None, '63. CUDA_C_Best_Practices_Guide.pdf, page: 22': None, '63. Lecture 5: CUDA Tiled Matrix Multiplication, page: 8': None, '64': None, '64. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 20': None, '64. 3rd-Edition-Chapter09-histogram-FINAL-corrected.pdf, page: 7': None, '64. CUDA C++ Programming Guide, Release 12.3': None, '64. CUDA-Enabled GPUs, page: 175': None, '64. CUDA_C_Best_Practices_Guide.pdf, page: 50': None, '64. CUDA_C_Programming_Guide.pdf, page: 28': None, '64. Lecture 5: CUDA Tiled Matrix Multiplication, page: 7': None, '64. ece408-lecture15-scan-part2-vk-SP24.pdf, page: 14': None, '64. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 11': None, '64. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 7': None, '64. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 23': None, '64. ece408-lecture9-convolution-analysis-vk-SP24.pdf, page: 18': None, '65': None, '65. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 14': None, '65. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 22': None, '65. A Simple Matrix Multiplication Kernel': None, '65. CUDA_C_Programming_Guide.pdf, page: 157': None, '65. ECE408/CS483/CSE408 Spring 2024': None, '65. ece408-lecture14-scan-vk-SP24.pdf, page: 19': None, '65. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 31': None, '65. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 10': None, '65. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 35': None, '65. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 9': None, '65. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 34': None, '66': None, '66. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 31': None, '66. CUDA_C_Programming_Guide.pdf, page: 172': None, '66. ece408-lecture3-CUDA-parallelism-model-vk-SP24': None, '66. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 10': None, '67': None, '67. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 6': None, '67. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 10': None, '67. CUDA_C_Best_Practices_Guide.pdf, page: 50': None, '67. CUDA_C_Best_Practices_Guide.pdf, page: 69': None, '67. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 13': None, '67. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 3': None, '67. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 24': None, '67. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 26': None, '68': None, '68. CUDA C++ Best Practices Guide': None, '68. CUDA_C_Best_Practices_Guide.pdf, page: 67': None, '68. CUDA_C_Programming_Guide.pdf, page: 159': None, '68. CUDA_C_Programming_Guide.pdf, page: 397': None, '69': None, '69. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 5': None, '69. 3rd-Edition-Chapter09-histogram-FINAL-corrected.pdf, page: 1': None, '69. 3rd-Edition-Chapter09-histogram-FINAL-corrected.pdf, page: 6': None, '69. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 3': None, '69: CUDA_C_Programming_Guide.pdf, page: 58': None, '6: CUDA_C_Programming_Guide.pdf, page: 42': None, '7': None, '7. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 10': None, '7. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 11': None, '7. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected, page: 20': None, '7. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 2': None, '7. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 27': None, '7. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 30': None, '7. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 16': None, '7. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 20': None, '7. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 27': None, '7. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 3': None, '7. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 9': None, '7. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 19': None, '7. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 2': None, '7. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 22': None, '7. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 25': None, '7. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 18': None, '7. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 20': None, '7. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 23': None, '7. 3rd-Edition-Chapter16-case-study-DNN-FINAL-corrected.pdf, page: 5': None, '7. CUDA: A General-Purpose Parallel Computing Platform and Programming Model, page: 18': None, '7. CUDA_C_Best_Practices_Guide.pdf, page: 81': None, '7. CUDA_C_Programming_Guide.pdf, page: 163': None, '7. CUDA_C_Programming_Guide.pdf, page: 164': None, '7. CUDA_C_Programming_Guide.pdf, page: 42': None, '7. CUDA_C_Programming_Guide.pdf, page: 43': None, '7. Spring 2024-ECE 408-overview.pdf, page: 9': None, '7. ece408-lecture1-introduction-vk-SP24.pdf, page: 29': None, '7. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 7': None, '7. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 2': None, '7. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 24': None, '7. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 25': None, '7. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 27': None, '7. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 34': None, '7. ece408-lecture6-dram-tiling-vk-SP24.pdf, page: 18': None, '7. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 28': None, '7. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 3': None, '7. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 32': None, '7. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 12': None, '7. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 2': None, '7. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 27': None, '7. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 40': None, '7. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 7': None, '7. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 9': None, '7. ece408-lecture9-convolution-analysis-vk-SP24.pdf, page: 2': None, '7. ece408-lecture9-convolution-analysis-vk-SP24.pdf, page: 5': None, '70': None, '70. 3rd-Edition-Chapter01-introduction-Final-corrected.pdf, page: 8': None, '70. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 17': None, '70. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 30': None, '70. CUDA C++ Programming Guide, Release 12.3': None, '70. CUDA C++ Programming Guide, Release 12.3, page: 12': None, '70. CUDA_C_Programming_Guide.pdf, page: 455': None, '70. Chapter 7: Convolution, page: 10': None, '70. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 3': None, '70. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 10': None, '70. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 1': None, '70. ece408-lecture9-convolution-analysis-vk-SP24.pdf, page: 19': None, '71': None, '71. CUDA C++ Programming Guide, page: 28': None, '71. Kernel Function - A Small Example': None, '71. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 17': None, '71. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 25': None, '71. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 28': None, '71. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 3': None, '72': None, '72. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 19': None, '72. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 2': None, '72. 3rd-Edition-Chapter09-histogram-FINAL-corrected.pdf, page: 14': None, '72. ece408-lecture15-scan-part2-vk-SP24.pdf, page: 2': None, '72. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 22': None, '72. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 9': None, '72. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 5': None, '72: CUDA_C_Programming_Guide.pdf, page: 44': None, '73': None, '73. Review – Vector Addition Kernel': None, '73. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 14': None, '73. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 26': None, '73. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 16': None, '74': None, '74. CUDA C++ Programming Guide, Release 12.3': None, '74. CUDA_C_Programming_Guide.pdf, page: 50': None, '74. ece408-lecture6-dram-tiling-vk-SP24.pdf, page: 44': None, '75': None, '75. CUDA C++ Programming Guide, page: 302': None, '75. CUDA_C_Best_Practices_Guide.pdf, page: 67': None, '75. CUDA_C_Programming_Guide.pdf, page: 44': None, '76': None, '76. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 20': None, '76. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 32': None, '76. CUDA C++ Best Practices Guide, Release 12.3': None, '76. CUDA C++ Programming Guide, page: 461': None, '76. CUDA_C_Best_Practices_Guide.pdf, page: 50': None, '76. CUDA_C_Programming_Guide.pdf, page: 163': None, '76. Problem Solving': None, '76. Spring 2024-ECE 408-overview.pdf, page: 6': None, '76. ece408-lecture15-scan-part2-vk-SP24.pdf': None, '76. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 21': None, '76. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf': None, '76. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 11': None, '76. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 31': None, '76: CUDA_C_Programming_Guide.pdf, page: 346': None, '77': None, '77. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 2': None, '77. Block Granularity Considerations': None, '77. CUDA C++ Programming Guide, page: 26': None, '77. CUDA_C_Programming_Guide.pdf, page: 155': None, '77. ece408-lecture2-CUDA-introduction-vk-SP24.pdf, page: 15': None, '77. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 26': None, '77. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 23': None, '77. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 9': None, '78': None, '78, page: 9]. When threads in a warp access consecutive locations, as shown in the matrix multiplication kernel[78': None, '78. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 10': None, '78. CUDA C++ Programming Guide, Release 12.3': None, '78. CUDA C++ Programming Guide, page: 26': None, '78. CUDA C++ Programming Guide, page: 42': None, '78. CUDA_C_Programming_Guide.pdf, page: 153': None, '78. ELL Storage Format': None, '78. ece408-lecture1-introduction-vk-SP24.pdf, page: 3': None, '79': None, '79. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 13': None, '79. 3rd-Edition-Chapter09-histogram-FINAL-corrected.pdf': None, '79. CUDA_C_Best_Practices_Guide.pdf, page: 51': None, '79. CUDA_C_Programming_Guide.pdf, page: 251': None, '8': None, '8. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 22': None, '8. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 27': None, '8. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 12': None, '8. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 23': None, '8. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 19': None, '8. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 6': None, '8. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 16': None, '8. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 24': None, '8. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 4': None, '8. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 8': None, '8. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, pages: 16-19': None, '8. 3rd-Edition-Chapter16-case-study-DNN-FINAL-corrected.pdf, page: 16': None, '8. CUDA C Programming Guide, page: 187': None, '8. CUDA C++ Programming Guide, page: 64': None, '8. CUDA_C_Best_Practices_Guide.pdf, page: 21': None, '8. CUDA_C_Programming_Guide.pdf, page: 153': None, '8. CUDA_C_Programming_Guide.pdf, page: 166': None, '8. CUDA_C_Programming_Guide.pdf, page: 172': None, '8. CUDA_C_Programming_Guide.pdf, page: 180': None, '8. CUDA_C_Programming_Guide.pdf, page: 57': None, '8. Programming Interface, page: 39': None, '8. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 2': None, '8. ece408-lecture13-reduction-tree-vk-SP24.pdf, page: 9': None, '8. ece408-lecture2-CUDA-introduction-vk-SP24.pdf, page: 21': None, '8. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 26': None, '8. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 29': None, '8. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 34': None, '8. ece408-lecture7-convolution-constant-memory-vk-SP24.pdf, page: 5': None, '8. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 3': None, '8. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 39': None, '8. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 40': None, '8. ece408-lecture9-convolution-analysis-vk-SP24.pdf, page: 2': None, '80': None, '80. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 12': None, '80. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 10': None, '80. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 3': None, '80. Host Code Example': None, '80. Lecture 4: CUDA Memory Model, page: 11': None, '80. Matrix Multiplication using Multiple Blocks by Tiling P': None, '80. ece408-lecture2-CUDA-introduction-vk-SP24.pdf, page: 22': None, '80. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 11': None, '80. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 26': None, '8: CUDA C Programming Guide, page: 43': None, '9': None, '9. 3rd-Edition-Chapter01-introduction-Final-corrected.pdf, page: 15': None, '9. 3rd-Edition-Chapter02-data-parallel-programming-FINAL-corrected.pdf, page: 24': None, '9. 3rd-Edition-Chapter03-data-parallel-execution-FINAL-corrected.pdf, page: 26': None, '9. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 20': None, '9. 3rd-Edition-Chapter04-memory-model-FINAL-corrected.pdf, page: 21': None, '9. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 1': None, '9. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 16': None, '9. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 22': None, '9. 3rd-Edition-Chapter05-performance-FINAL-corrected.pdf, page: 32': None, '9. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 14': None, '9. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 19': None, '9. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 22': None, '9. 3rd-Edition-Chapter07-convolution-FINAL-corrected.pdf, page: 8': None, '9. 3rd-Edition-Chapter08-scan-FINAL-corrected.pdf, page: 4': None, '9. 3rd-Edition-Chapter09-histogram-FINAL-corrected.pdf, page: 15': None, '9. CUDA_C_Programming_Guide': None, '9. CUDA_C_Programming_Guide.pdf, page: 102': None, '9. CUDA_C_Programming_Guide.pdf, page: 151': None, '9. CUDA_C_Programming_Guide.pdf, page: 153': None, '9. CUDA_C_Programming_Guide.pdf, page: 167': None, '9. CUDA_C_Programming_Guide.pdf, page: 402': None, '9. Mode Switches, page: 149': None, '9. Spring 2024-ECE 408-overview.pdf, page: 8': None, '9. Spring 2024-ECE 408-overview.pdf, page: 9': None, '9. ece408-lecture10-ML-DL-vk-SP24.pdf, page: 2': None, '9. ece408-lecture2-CUDA-introduction-vk-SP24.pdf, page: 22': None, '9. ece408-lecture2-CUDA-introduction-vk-SP24.pdf, page: 9': None, '9. ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 26': None, '9. ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 16': None, '9. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 12': None, '9. ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 34': None, '9. ece408-lecture6-dram-tiling-vk-SP24.pdf, page: 3': None, '9. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 13': None, '9. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 26': None, '9. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 38': None, '9. ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 6': None, '9. ece408-lecture9-convolution-analysis-vk-SP24.pdf, page: 31': None, '9: CUDA C Programming Guide, page: 43': None, 'ACCESS': None, 'Arrays of Parallel Threads': None, 'CUDA C Programming Guide': None, 'CUDA C Programming Guide, page: 157': None, 'CUDA C++ Best Practices Guide, Release 12.3': None, 'CUDA C++ Programming Guide': None, 'CUDA C++ Programming Guide, Release 12.3': None, 'CUDA C++ Programming Guide, Release 12.3, Chapter 3. A Scalable Programming Model': None, 'CUDA C++ Programming Guide, Release 12.3, page: 152': None, 'CUDA C++ Programming Guide, page: 42': None, 'CUDA C++ Programming Guide, page: 433': None, 'CUDA C++ Programming Guide, page: 71': None, 'CUDA C_Programming_Guide.pdf, page: 7': None, 'CUDA Execution Model: Thread Blocks': None, 'CUDA_C++_Programming_Guide.pdf, page: 189': None, 'CUDA_C_Best_Practices_Guide.pdf, page: 14': None, 'CUDA_C_Best_Practices_Guide.pdf, page: 21': None, 'CUDA_C_Best_Practices_Guide.pdf, page: 22': None, 'CUDA_C_Best_Practices_Guide.pdf, page: 27': None, 'CUDA_C_Best_Practices_Guide.pdf, page: 42': None, 'CUDA_C_Best_Practices_Guide.pdf, page: 43': None, 'CUDA_C_Best_Practices_Guide.pdf, page: 45': None, 'CUDA_C_Best_Practices_Guide.pdf, page: 46': None, 'CUDA_C_Best_Practices_Guide.pdf, page: 49': None, 'CUDA_C_Best_Practices_Guide.pdf, page: 50': None, 'CUDA_C_Best_Practices_Guide.pdf, page: 57': None, 'CUDA_C_Best_Practices_Guide.pdf, page: 58': None, 'CUDA_C_Best_Practices_Guide.pdf, page: 59': None, 'CUDA_C_Best_Practices_Guide.pdf, page: 63': None, 'CUDA_C_Best_Practices_Guide.pdf, page: 67': None, 'CUDA_C_Best_Practices_Guide.pdf, page: 69': None, 'CUDA_C_Best_Practices_Guide.pdf, page: 71': None, 'CUDA_C_Best_Practices_Guide.pdf, page: 72': None, 'CUDA_C_Best_Practices_Guide.pdf, page: 73': None, 'CUDA_C_Best_Practices_Guide.pdf, page: 81': None, 'CUDA_C_Best_Practices_Guide.pdf, page: 85': None, 'CUDA_C_Best_Practices_Guide.pdf, page: 89': None, 'CUDA_C_Best_Practices_Guide.pdf, page: 9': None, 'CUDA_C_Best_Practices_Guide.pdf, page: 93': None, 'CUDA_C_Programming_Guide.pdf, page: 1': None, 'CUDA_C_Programming_Guide.pdf, page: 109': None, 'CUDA_C_Programming_Guide.pdf, page: 151': None, 'CUDA_C_Programming_Guide.pdf, page: 152': None, 'CUDA_C_Programming_Guide.pdf, page: 153': None, 'CUDA_C_Programming_Guide.pdf, page: 157': None, 'CUDA_C_Programming_Guide.pdf, page: 159': None, 'CUDA_C_Programming_Guide.pdf, page: 161': None, 'CUDA_C_Programming_Guide.pdf, page: 162': None, 'CUDA_C_Programming_Guide.pdf, page: 163': None, 'CUDA_C_Programming_Guide.pdf, page: 164': None, 'CUDA_C_Programming_Guide.pdf, page: 169': None, 'CUDA_C_Programming_Guide.pdf, page: 172': None, 'CUDA_C_Programming_Guide.pdf, page: 175': None, 'CUDA_C_Programming_Guide.pdf, page: 177': None, 'CUDA_C_Programming_Guide.pdf, page: 181': None, 'CUDA_C_Programming_Guide.pdf, page: 185': None, 'CUDA_C_Programming_Guide.pdf, page: 189': None, 'CUDA_C_Programming_Guide.pdf, page: 19': None, 'CUDA_C_Programming_Guide.pdf, page: 21': None, 'CUDA_C_Programming_Guide.pdf, page: 23': None, 'CUDA_C_Programming_Guide.pdf, page: 251': None, 'CUDA_C_Programming_Guide.pdf, page: 252': None, 'CUDA_C_Programming_Guide.pdf, page: 254': None, 'CUDA_C_Programming_Guide.pdf, page: 259': None, 'CUDA_C_Programming_Guide.pdf, page: 260': None, 'CUDA_C_Programming_Guide.pdf, page: 261': None, 'CUDA_C_Programming_Guide.pdf, page: 27': None, 'CUDA_C_Programming_Guide.pdf, page: 270': None, 'CUDA_C_Programming_Guide.pdf, page: 275': None, 'CUDA_C_Programming_Guide.pdf, page: 276': None, 'CUDA_C_Programming_Guide.pdf, page: 28': None, 'CUDA_C_Programming_Guide.pdf, page: 30': None, 'CUDA_C_Programming_Guide.pdf, page: 302': None, 'CUDA_C_Programming_Guide.pdf, page: 31': None, 'CUDA_C_Programming_Guide.pdf, page: 313': None, 'CUDA_C_Programming_Guide.pdf, page: 319': None, 'CUDA_C_Programming_Guide.pdf, page: 32': None, 'CUDA_C_Programming_Guide.pdf, page: 329': None, 'CUDA_C_Programming_Guide.pdf, page: 33': None, 'CUDA_C_Programming_Guide.pdf, page: 336': None, 'CUDA_C_Programming_Guide.pdf, page: 36': None, 'CUDA_C_Programming_Guide.pdf, page: 37': None, 'CUDA_C_Programming_Guide.pdf, page: 398': None, 'CUDA_C_Programming_Guide.pdf, page: 409': None, 'CUDA_C_Programming_Guide.pdf, page: 42': None, 'CUDA_C_Programming_Guide.pdf, page: 43': None, 'CUDA_C_Programming_Guide.pdf, page: 438': None, 'CUDA_C_Programming_Guide.pdf, page: 44': None, 'CUDA_C_Programming_Guide.pdf, page: 449': None, 'CUDA_C_Programming_Guide.pdf, page: 453': None, 'CUDA_C_Programming_Guide.pdf, page: 461': None, 'CUDA_C_Programming_Guide.pdf, page: 466': None, 'CUDA_C_Programming_Guide.pdf, page: 486': None, 'CUDA_C_Programming_Guide.pdf, page: 487': None, 'CUDA_C_Programming_Guide.pdf, page: 489': None, 'CUDA_C_Programming_Guide.pdf, page: 494': None, 'CUDA_C_Programming_Guide.pdf, page: 5': None, 'CUDA_C_Programming_Guide.pdf, page: 58': None, 'CUDA_C_Programming_Guide.pdf, page: 63': None, 'CUDA_C_Programming_Guide.pdf, page: 9': None, 'CUDA_C_Programming_Guide.pdf, page: 95': None, 'Chapter 18. Understanding the Programming Environment': None, 'Compiling A CUDA Program': None, 'Generating a new SSH key and adding it to the ssh-agent': None, 'GitHub': None, 'GitHub SSH Documentation': None, 'K, C, R, S]` where `K` is the number of output feature maps, `C` is the number of input feature maps (matching the input tensor), `R` is the height of the kernel, and `S` is the width of the kernel[16': None, 'N, C, H, W]` where `N` is the number of samples in a mini-batch, `C` is the number of input feature maps, `H` is the height, and `W` is the width[61': None, 'Plane*m*n + Row * m + Col]`, where `m` and `n` are the number of columns and rows, respectively[41': None, 'Registers vs Memory': None, 'Spring 2024-ECE 408-overview.pdf, page: 1': None, 'Spring 2024-ECE 408-overview.pdf, page: 10': None, 'Spring 2024-ECE 408-overview.pdf, page: 2': None, 'Spring 2024-ECE 408-overview.pdf, page: 3': None, 'Spring 2024-ECE 408-overview.pdf, page: 4': None, 'Spring 2024-ECE 408-overview.pdf, page: 5': None, 'Spring 2024-ECE 408-overview.pdf, page: 8': None, 'Spring 2024-ECE 408-overview.pdf, page: 9': None, 'Testing your SSH connection': None, ']` array is crucial for accessing the non-zero elements of a particular row efficiently and is used in sparse matrix-vector multiplication and other operations involving sparse matrices[3': None, ']` array is of size `num_rows + 1`, where `num_rows` is the number of rows in the matrix. Each entry `row_ptr[i]` indicates the index in the `data[]` array where the non-zero elements for the `i`-th row begin. The last element `row_ptr[num_rows]` stores the total number of non-zero elements in the matrix, which is also the size of the `data[]` and `col_index[]` arrays. This last element helps in determining the end of the last row in the CSR representation[2': None, ']`, `col_index[]`, and `row_ptr[]`. The `data[]` array stores all the non-zero values of the matrix, the `col_index[]` array stores the corresponding column indices of these non-zero values, and the `row_ptr[]` array stores the indices in the `data[]` array where each row starts[1': None, \"all] Error 1`, indicates that the `make` build automation tool is trying to invoke the `nvcc` command (the NVIDIA CUDA Compiler), but it cannot find the `nvcc` executable in the system's PATH[1\": None, 'blockIdx': None, 'ece408-lecture1-introduction-vk-SP24.pdf, page: 1': None, 'ece408-lecture1-introduction-vk-SP24.pdf, page: 10': None, 'ece408-lecture1-introduction-vk-SP24.pdf, page: 11': None, 'ece408-lecture1-introduction-vk-SP24.pdf, page: 12': None, 'ece408-lecture1-introduction-vk-SP24.pdf, page: 13': None, 'ece408-lecture1-introduction-vk-SP24.pdf, page: 14': None, 'ece408-lecture1-introduction-vk-SP24.pdf, page: 15': None, 'ece408-lecture1-introduction-vk-SP24.pdf, page: 16': None, 'ece408-lecture1-introduction-vk-SP24.pdf, page: 17': None, 'ece408-lecture1-introduction-vk-SP24.pdf, page: 2': None, 'ece408-lecture1-introduction-vk-SP24.pdf, page: 22': None, 'ece408-lecture1-introduction-vk-SP24.pdf, page: 24': None, 'ece408-lecture1-introduction-vk-SP24.pdf, page: 26': None, 'ece408-lecture1-introduction-vk-SP24.pdf, page: 27': None, 'ece408-lecture1-introduction-vk-SP24.pdf, page: 29': None, 'ece408-lecture1-introduction-vk-SP24.pdf, page: 3': None, 'ece408-lecture1-introduction-vk-SP24.pdf, page: 5': None, 'ece408-lecture1-introduction-vk-SP24.pdf, page: 6': None, 'ece408-lecture1-introduction-vk-SP24.pdf, page: 8': None, 'ece408-lecture14-scan-vk-SP24.pdf, page: 4': None, 'ece408-lecture2-CUDA-introduction-vk-SP24, page: 21': None, 'ece408-lecture2-CUDA-introduction-vk-SP24.pdf, page: 12': None, 'ece408-lecture2-CUDA-introduction-vk-SP24.pdf, page: 14': None, 'ece408-lecture2-CUDA-introduction-vk-SP24.pdf, page: 2': None, 'ece408-lecture2-CUDA-introduction-vk-SP24.pdf, page: 34': None, 'ece408-lecture2-CUDA-introduction-vk-SP24.pdf, page: 7': None, 'ece408-lecture2-CUDA-introduction-vk-SP24.pdf, page: 8': None, 'ece408-lecture2-CUDA-introduction-vk-SP24.pdf, page: 9': None, 'ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 2': None, 'ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 22': None, 'ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 23': None, 'ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 24': None, 'ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 25': None, 'ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 26': None, 'ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 27': None, 'ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 29': None, 'ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 3': None, 'ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 31': None, 'ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 32': None, 'ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 6': None, 'ece408-lecture3-CUDA-parallelism-model-vk-SP24.pdf, page: 7': None, 'ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 10': None, 'ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 2': None, 'ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 28': None, 'ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 32': None, 'ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 5': None, 'ece408-lecture4-CUDA-memory-model-vk-SP24.pdf, page: 8': None, 'ece408-lecture5-CUDA-tiled-matrix-multiplication-vk-SP24.pdf, page: 14': None, 'ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 26': None, 'ece408-lecture8-tiled-convolution-vk-SP24.pdf, page: 28': None, 'here': None, \"i][j]. This thread would need to access the ith row of M and the jth column of N to compute the dot product, which becomes the value of P[i][j]. The indices for the row of M and the column of N that a thread needs to access are derived from the thread's assigned output element in P[1\": None, 'k*Width+blockIdx.x*blockDim.x+threadIdx.x]`, represents a coalesced access pattern. This is because within a given iteration of the k loop, the k*Width value is the same across all threads, and the only part that varies across a thread block is threadIdx.x[2': None, 'parallel] algorithms, and are significantly easier to implement than memory references\" [1': None, 'row * numAColumns + k] * B[k * numBColumns + col];` is designed to ensure that each thread computes one element of the output matrix P by performing the dot product of a row from matrix A and a column from matrix B[63': None, 'threadIdx': None}\n",
            "\n",
            "\n",
            "________________this is response_without_references_________________\n",
            "Tiling is a technique used in CUDA programming to improve the efficiency of memory access and thereby optimize the performance of CUDA programs. This technique involves dividing the data into smaller chunks or 'tiles' that fit into the faster shared memory on the GPU, which is much faster than global memory.\n",
            "\n",
            "The main advantage of tiling is that it reduces the number of global memory accesses. When a tile is loaded into the shared memory, all threads in a block can access the data within the tile from the shared memory, which has much higher bandwidth and lower latency than global memory  . This can significantly reduce the time taken for memory access, thereby increasing the overall performance of the program.\n",
            "\n",
            "Another advantage of tiling is that it can help to ensure coalesced access to global memory. Coalescing is a mechanism that allows multiple threads in a warp to access memory in a way that minimizes the number of memory transactions, thus maximizing memory throughput . By organizing data into tiles, it's easier to ensure that threads in a warp access adjacent memory locations, which is a requirement for coalescing  .\n",
            "\n",
            "Lastly, tiling can help to eliminate or reduce redundant loads from global memory. If multiple threads need to access the same data, loading the data into a tile in shared memory allows all the threads to access the data without having to make multiple loads from global memory .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"________________this is first query_________________\")\n",
        "print(dataset['train'][0][\"first_query\"])\n",
        "print(\"\\n\")\n",
        "print(\"________________this is first contexts_________________\")\n",
        "print(dataset['train'][0][\"first_contexts\"])\n",
        "print(\"\\n\")\n",
        "# print(\"________________this is full prompt_________________\")\n",
        "# print(dataset['train'][0]['full_prompt'])\n",
        "# print(\"\\n\")\n",
        "print(\"________________this is first response_________________\")\n",
        "print(dataset['train'][0]['first_response'])\n",
        "print(\"\\n\")\n",
        "print(\"________________this is references_________________\")\n",
        "print(dataset['train'][0]['references'])\n",
        "print(\"\\n\")\n",
        "print(\"________________this is response_without_references_________________\")\n",
        "print(dataset['train'][0]['response_without_references'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMksovizne18",
        "outputId": "1dd27259-53b8-490f-d4e6-cf7adf8fa122"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "________________this is full prompt_________________\n",
            "\n",
            "Please analyze and respond to the following question using the excerpts from the provided documents. These documents can be pdf files or web pages.\n",
            "    Integrate relevant information from these documents, ensuring each reference is linked to the document's number.\n",
            "    Your response should be semi-formal. \n",
            "    When quoting directly, cite with footnotes linked to the document number and page number, if provided. \n",
            "    Summarize or paraphrase other relevant information with inline citations, again referencing the document number and page number, if provided.\n",
            "    If the answer is not in the provided documents, state so. \n",
            "    Conclude your response with a LIST of the document titles as clickable placeholders, each linked to its respective document number and page number, if provided.\n",
            "    Always share page numbers if they were shared with you.\n",
            "    ALWAYS follow the examples below:\n",
            "    Insert an inline citation like this in your response: \n",
            "    \"[1]\" if you're referencing the first document or \n",
            "    \"[1, page: 2]\" if you're referencing page 2 of the first document.\n",
            "    At the end of your response, list the document title with a clickable link, like this: \n",
            "    \"1. [document_name](#)\" if you're referencing the first document or\n",
            "    \"1. [document_name, page: 2](#)\" if you're referencing page 2 of the first document.\n",
            "    Nothing else should prefixxed or suffixed to the citation or document name. Consecutive citations should be separated by a comma.\n",
            "    \n",
            "    Suppose a document name is shared with you along with the index and pageNumber below like \"27: www.pdf, page: 2\", \"28: www.osd\", \"29: pdf.www, page 11\n",
            "15\" where 27, 28, 29 are indices, www.pdf, www.osd, pdf.www are document_name, and 2, 11 are the pageNumbers and 15 is the content of the document, then inline citations and final list of cited documents should ALWAYS be in the following format:\n",
            "    \"\"\"\n",
            "    The sky is blue. [27, page: 2][28] The grass is green. [29, page: 11]\n",
            "    Relevant Sources:\n",
            "\n",
            "    27. [www.pdf, page: 2](#)\n",
            "    28. [www.osd](#)\n",
            "    29. [pdf.www, page: 11](#)\n",
            "    \"\"\"\n",
            "    ONLY return the documents with relevant information and cited in the response. If there are no relevant sources, don't include the \"Relevant Sources\" section in response.\n",
            "    Here are excerpts from the high-quality documents provided:\n",
            "    \n",
            "\n",
            "---\n",
            "1: CUDA_C_Best_Practices_Guide.pdf, page: 57\n",
            " CUDA C++ Best Practices Guide, Release 12.3\n",
            "Fig. 10: The performance of the sliding-window benchmark with tuned hit-ratio\n",
            "13.2.3. Shared Memory\n",
            "Because it is on-chip, shared memory has much higher bandwidth and lower latency than local and\n",
            "global memory - provided there are no bank conflicts between the threads, as detailed in the following\n",
            "section.\n",
            "13.2.3.1 Shared Memory and Memory Banks\n",
            "To achieve high memory bandwidth for concurrent accesses, shared memory is divided into equally\n",
            "sized memory modules (banks) that can be accessed simultaneously. Therefore, any memory load or\n",
            "store of n addresses that spans n distinct memory banks can be serviced simultaneously, yielding an\n",
            "effective bandwidth that is n times as high as the bandwidth of a single bank.\n",
            "However, if multiple addresses of a memory request map to the same memory bank, the accesses\n",
            "are serialized. The hardware splits a memory request that has bank conflicts into as many separate\n",
            "conflict-free requests as necessary, decreasing the effective bandwidth by a factor equal to the num-\n",
            "ber of separate memory requests. The one exception here is when multiple threads in a warp address\n",
            "the same shared memory location, resulting in a broadcast. In this case, multiple broadcasts from\n",
            "different banks are coalesced into a single multicast from the requested shared memory locations to\n",
            "the threads.\n",
            "To minimize bank conflicts, it is important to understand how memory addresses map to memory\n",
            "banks and how to optimally schedule memory requests.\n",
            "On devices of compute capability 5.x or newer, each bank has a bandwidth of 32 bits every clock cycle,\n",
            "and successive 32-bit words are assigned to successive banks. The warp size is 32 threads and the\n",
            "number of banks is also 32, so bank conflicts can occur between any threads in the warp. See Compute\n",
            "Capability 5.x in the CUDA C++ Programming Guide for further details.\n",
            "13.2. Device Memory Spaces\n",
            "51\n",
            "---\n",
            "2: CUDA_C_Programming_Guide.pdf, page: 164\n",
            " CUDA C++ Programming Guide, Release 12.3\n",
            "To achieve high bandwidth, shared memory is divided into equally-sized memory modules, called banks,\n",
            "which can be accessed simultaneously. Any memory read or write request made of n addresses that\n",
            "fall in n distinct memory banks can therefore be serviced simultaneously, yielding an overall bandwidth\n",
            "that is n times as high as the bandwidth of a single module.\n",
            "However, if two addresses of a memory request fall in the same memory bank, there is a bank conflict\n",
            "and the access has to be serialized. The hardware splits a memory request with bank conflicts into\n",
            "as many separate conflict-free requests as necessary, decreasing throughput by a factor equal to the\n",
            "number of separate memory requests. If the number of separate memory requests is n, the initial\n",
            "memory request is said to cause n-way bank conflicts.\n",
            "To get maximum performance, it is therefore important to understand how memory addresses map\n",
            "to memory banks in order to schedule the memory requests so as to minimize bank conflicts. This\n",
            "is described in Compute Capability 5.x, Compute Capability 6.x, Compute Capability 7.x, Compute Ca-\n",
            "pability 8.x, and Compute Capability 9.0 for devices of compute capability 5.x, 6.x, 7.x, 8.x, and 9.0\n",
            "respectively.\n",
            "Constant Memory\n",
            "The constant memory space resides in device memory and is cached in the constant cache.\n",
            "A request is then split into as many separate requests as there are different memory addresses in the\n",
            "initial request, decreasing throughput by a factor equal to the number of separate requests.\n",
            "The resulting requests are then serviced at the throughput of the constant cache in case of a cache\n",
            "hit, or at the throughput of device memory otherwise.\n",
            "Texture and Surface Memory\n",
            "The texture and surface memory spaces reside in device memory and are cached in texture cache, so\n",
            "a texture fetch or surface read costs one memory read from device memory only on a cache miss,\n",
            "otherwise it just costs one read from texture cache. The texture cache is optimized for 2D spatial\n",
            "locality, so threads of the same warp that read texture or surface addresses that are close together in\n",
            "2D will achieve best performance. Also, it is designed for streaming fetches with a constant latency;\n",
            "a cache hit reduces DRAM bandwidth demand but not fetch latency.\n",
            "Reading device memory through texture or surface fetching present some benefits that can make it\n",
            "an advantageous alternative to reading device memory from global or constant memory:\n",
            "▶ If the memory reads do not follow the access patterns that global or constant memory reads\n",
            "must follow to get good performance, higher bandwidth can be achieved providing that there is\n",
            "locality in the texture fetches or surface reads;\n",
            "▶ Addressing calculations are performed outside the kernel by dedicated units;\n",
            "▶ Packed data may be broadcast to separate variables in a single operation;\n",
            "▶ 8-bit and 16-bit integer input data may be optionally converted to 32 bit floating-point values in\n",
            "the range [0.0, 1.0] or [-1.0, 1.0] (see Texture Memory).\n",
            "148\n",
            "Chapter 8. Performance Guidelines\n",
            "---\n",
            "3: CUDA_C_Best_Practices_Guide.pdf, page: 43\n",
            " Chapter 13. Memory Optimizations\n",
            "Memory optimizations are the most important area for performance. The goal is to maximize the use\n",
            "of the hardware by maximizing bandwidth. Bandwidth is best served by using as much fast memory\n",
            "and as little slow-access memory as possible. This chapter discusses the various kinds of memory on\n",
            "the host and device and how best to set up data items to use the memory effectively.\n",
            "13.1. Data Transfer Between Host and Device\n",
            "The peak theoretical bandwidth between the device memory and the GPU is much higher (898 GB/s\n",
            "on the NVIDIA Tesla V100, for example) than the peak theoretical bandwidth between host memory\n",
            "and device memory (16 GB/s on the PCIe x16 Gen3). Hence, for best overall application performance,\n",
            "it is important to minimize data transfer between the host and the device, even if that means running\n",
            "kernels on the GPU that do not demonstrate any speedup compared with running them on the host\n",
            "CPU.\n",
            "Note: High Priority: Minimize data transfer between the host and the device, even if it means running\n",
            "some kernels on the device that do not show performance gains when compared with running them\n",
            "on the host CPU.\n",
            "Intermediate data structures should be created in device memory, operated on by the device, and\n",
            "destroyed without ever being mapped by the host or copied to host memory.\n",
            "Also, because of the overhead associated with each transfer, batching many small transfers into one\n",
            "larger transfer performs significantly better than making each transfer separately, even if doing so\n",
            "requires packing non-contiguous regions of memory into a contiguous buffer and then unpacking\n",
            "after the transfer.\n",
            "Finally, higher bandwidth between the host and the device is achieved when using page-locked (or\n",
            "pinned) memory, as discussed in the CUDA C++ Programming Guide and the Pinned Memory section\n",
            "of this document.\n",
            "37\n",
            "---\n",
            "4: CUDA_C_Programming_Guide.pdf, page: 163\n",
            " CUDA C++ Programming Guide, Release 12.3\n",
            "Reading non-naturally aligned 8-byte or 16-byte words produces incorrect results (off by a few words),\n",
            "so special care must be taken to maintain alignment of the starting address of any value or array of\n",
            "values of these types. A typical case where this might be easily overlooked is when using some cus-\n",
            "tom global memory allocation scheme, whereby the allocations of multiple arrays (with multiple calls\n",
            "to cudaMalloc() or cuMemAlloc()) is replaced by the allocation of a single large block of memory\n",
            "partitioned into multiple arrays, in which case the starting address of each array is offset from the\n",
            "block’s starting address.\n",
            "Two-Dimensional Arrays\n",
            "A common global memory access pattern is when each thread of index (tx,ty) uses the following\n",
            "address to access one element of a 2D array of width width, located at address BaseAddress of type\n",
            "type* (where type meets the requirement described in Maximize Utilization):\n",
            "BaseAddress + width * ty + tx\n",
            "For these accesses to be fully coalesced, both the width of the thread block and the width of the array\n",
            "must be a multiple of the warp size.\n",
            "In particular, this means that an array whose width is not a multiple of this size will be accessed much\n",
            "more efficiently if it is actually allocated with a width rounded up to the closest multiple of this size\n",
            "and its rows padded accordingly. The cudaMallocPitch() and cuMemAllocPitch() functions and\n",
            "associated memory copy functions described in the reference manual enable programmers to write\n",
            "non-hardware-dependent code to allocate arrays that conform to these constraints.\n",
            "Local Memory\n",
            "Local memory accesses only occur for some automatic variables as mentioned in Variable Memory\n",
            "Space Specifiers. Automatic variables that the compiler is likely to place in local memory are:\n",
            "▶ Arrays for which it cannot determine that they are indexed with constant quantities,\n",
            "▶ Large structures or arrays that would consume too much register space,\n",
            "▶ Any variable if the kernel uses more registers than available (this is also known as register spilling).\n",
            "Inspection of the PTX assembly code (obtained by compiling with the -ptx or-keep option) will tell if a\n",
            "variable has been placed in local memory during the first compilation phases as it will be declared using\n",
            "the .local mnemonic and accessed using the ld.local and st.local mnemonics. Even if it has not,\n",
            "subsequent compilation phases might still decide otherwise though if they find it consumes too much\n",
            "register space for the targeted architecture: Inspection of the cubin object using cuobjdump will tell if\n",
            "this is the case. Also, the compiler reports total local memory usage per kernel (lmem) when compiling\n",
            "with the --ptxas-options=-v option. Note that some mathematical functions have implementation\n",
            "paths that might access local memory.\n",
            "The local memory space resides in device memory, so local memory accesses have the same high\n",
            "latency and low bandwidth as global memory accesses and are subject to the same requirements for\n",
            "memory coalescing as described in Device Memory Accesses. Local memory is however organized\n",
            "such that consecutive 32-bit words are accessed by consecutive thread IDs. Accesses are therefore\n",
            "fully coalesced as long as all threads in a warp access the same relative address (for example, same\n",
            "index in an array variable, same member in a structure variable).\n",
            "On devices of compute capability 5.x onwards, local memory accesses are always cached in L2 in the\n",
            "same way as global memory accesses (see Compute Capability 5.x and Compute Capability 6.x).\n",
            "Shared Memory\n",
            "Because it is on-chip, shared memory has much higher bandwidth and much lower latency than local\n",
            "or global memory.\n",
            "8.3. Maximize Memory Throughput\n",
            "147\n",
            "---\n",
            "5: CUDA_C_Best_Practices_Guide.pdf, page: 63\n",
            " CUDA C++ Best Practices Guide, Release 12.3\n",
            "The examples in this section have illustrated three reasons to use shared memory:\n",
            "▶ To enable coalesced accesses to global memory, especially to avoid large strides (for general\n",
            "matrices, strides are much larger than 32)\n",
            "▶ To eliminate (or reduce) redundant loads from global memory\n",
            "▶ To avoid wasted bandwidth\n",
            "13.2.3.4 Asynchronous Copy from Global Memory to Shared Memory\n",
            "CUDA 11.0 introduces an async-copy feature that can be used within device code to explicitly manage\n",
            "the asynchronous copying of data from global memory to shared memory. This feature enables CUDA\n",
            "kernels to overlap copying data from global to shared memory with computation. It also avoids an\n",
            "intermediary register file access traditionally present between the global memory read and the shared\n",
            "memory write.\n",
            "For more details refer to the memcpy_async section in the CUDA C++ Programming Guide.\n",
            "To understand the performance difference between synchronous copy and asynchronous copy of data\n",
            "from global memory to shared memory, consider the following micro benchmark CUDA kernels for\n",
            "demonstrating the synchronous and asynchronous approaches. Asynchronous copies are hardware\n",
            "accelerated for NVIDIA A100 GPU.\n",
            "template <typename T>\n",
            "__global__ void pipeline_kernel_sync(T *global, uint64_t *clock, size_t copy_count) {\n",
            "extern __shared__ char s[];\n",
            "T *shared = reinterpret_cast<T *>(s);\n",
            "uint64_t clock_start = clock64();\n",
            "for (size_t i = 0; i < copy_count; ++i) {\n",
            "shared[blockDim.x * i + threadIdx.x] = global[blockDim.x * i + threadIdx.x];\n",
            "}\n",
            "uint64_t clock_end = clock64();\n",
            "atomicAdd(reinterpret_cast<unsigned long long *>(clock),\n",
            "clock_end - clock_start);\n",
            "}\n",
            "template <typename T>\n",
            "__global__ void pipeline_kernel_async(T *global, uint64_t *clock, size_t copy_count) {\n",
            "extern __shared__ char s[];\n",
            "T *shared = reinterpret_cast<T *>(s);\n",
            "uint64_t clock_start = clock64();\n",
            "∕∕pipeline pipe;\n",
            "for (size_t i = 0; i < copy_count; ++i) {\n",
            "__pipeline_memcpy_async(&shared[blockDim.x * i + threadIdx.x],\n",
            "&global[blockDim.x * i + threadIdx.x], sizeof(T));\n",
            "}\n",
            "__pipeline_commit();\n",
            "__pipeline_wait_prior(0);\n",
            "uint64_t clock_end = clock64();\n",
            "(continues on next page)\n",
            "13.2. Device Memory Spaces\n",
            "57\n",
            "---\n",
            "6: CUDA_C_Programming_Guide.pdf, page: 161\n",
            " CUDA C++ Programming Guide, Release 12.3\n",
            "▶ Write the results back to device memory.\n",
            "For some applications (for example, for which global memory access patterns are data-dependent),\n",
            "a traditional hardware-managed cache is more appropriate to exploit data locality. As mentioned in\n",
            "Compute Capability 7.x, Compute Capability 8.x and Compute Capability 9.0, for devices of compute\n",
            "capability 7.x, 8.x and 9.0, the same on-chip memory is used for both L1 and shared memory, and how\n",
            "much of it is dedicated to L1 versus shared memory is configurable for each kernel call.\n",
            "The throughput of memory accesses by a kernel can vary by an order of magnitude depending on ac-\n",
            "cess pattern for each type of memory. The next step in maximizing memory throughput is therefore\n",
            "to organize memory accesses as optimally as possible based on the optimal memory access patterns\n",
            "described in Device Memory Accesses. This optimization is especially important for global memory\n",
            "accesses as global memory bandwidth is low compared to available on-chip bandwidths and arith-\n",
            "metic instruction throughput, so non-optimal global memory accesses generally have a high impact\n",
            "on performance.\n",
            "8.3.1. Data Transfer between Host and Device\n",
            "Applications should strive to minimize data transfer between the host and the device. One way to\n",
            "accomplish this is to move more code from the host to the device, even if that means running kernels\n",
            "that do not expose enough parallelism to execute on the device with full efficiency. Intermediate data\n",
            "structures may be created in device memory, operated on by the device, and destroyed without ever\n",
            "being mapped by the host or copied to host memory.\n",
            "Also, because of the overhead associated with each transfer, batching many small transfers into a\n",
            "single large transfer always performs better than making each transfer separately.\n",
            "On systems with a front-side bus, higher performance for data transfers between host and device is\n",
            "achieved by using page-locked host memory as described in Page-Locked Host Memory.\n",
            "In addition, when using mapped page-locked memory (Mapped Memory), there is no need to allocate\n",
            "any device memory and explicitly copy data between device and host memory. Data transfers are\n",
            "implicitly performed each time the kernel accesses the mapped memory. For maximum performance,\n",
            "these memory accesses must be coalesced as with accesses to global memory (see Device Memory\n",
            "Accesses). Assuming that they are and that the mapped memory is read or written only once, using\n",
            "mapped page-locked memory instead of explicit copies between device and host memory can be a\n",
            "win for performance.\n",
            "On integrated systems where device memory and host memory are physically the same, any copy\n",
            "between host and device memory is superfluous and mapped page-locked memory should be used\n",
            "instead. Applications may query a device is integrated by checking that the integrated device prop-\n",
            "erty (see Device Enumeration) is equal to 1.\n",
            "8.3.2. Device Memory Accesses\n",
            "An instruction that accesses addressable memory (i.e., global, local, shared, constant, or texture mem-\n",
            "ory) might need to be re-issued multiple times depending on the distribution of the memory addresses\n",
            "across the threads within the warp. How the distribution affects the instruction throughput this way\n",
            "is specific to each type of memory and described in the following sections. For example, for global\n",
            "memory, as a general rule, the more scattered the addresses are, the more reduced the throughput is.\n",
            "Global Memory\n",
            "8.3. Maximize Memory Throughput\n",
            "145\n",
            "---\n",
            "7: CUDA_C_Programming_Guide.pdf, page: 453\n",
            " CUDA C++ Programming Guide, Release 12.3\n",
            "19.4.2. Global Memory\n",
            "Global memory accesses are always cached in L2.\n",
            "Data that is read-only for the entire lifetime of the kernel can also be cached in the unified L1/texture\n",
            "cache described in the previous section by reading it using the __ldg() function (see Read-Only Data\n",
            "Cache Load Function). When the compiler detects that the read-only condition is satisfied for some\n",
            "data, it will use __ldg() to read it. The compiler might not always be able to detect that the read-\n",
            "only condition is satisfied for some data. Marking pointers used for loading such data with both the\n",
            "const and __restrict__ qualifiers increases the likelihood that the compiler will detect the read-\n",
            "only condition.\n",
            "Data that is not read-only for the entire lifetime of the kernel cannot be cached in the unified\n",
            "L1/texture cache for devices of compute capability 5.0. For devices of compute capability 5.2, it is,\n",
            "by default, not cached in the unified L1/texture cache, but caching may be enabled using the following\n",
            "mechanisms:\n",
            "▶ Perform the read using inline assembly with the appropriate modifier as described in the PTX\n",
            "reference manual;\n",
            "▶ Compile with the -Xptxas -dlcm=ca compilation flag, in which case all reads are cached, except\n",
            "reads that are performed using inline assembly with a modifier that disables caching;\n",
            "▶ Compile with the -Xptxas -fscm=ca compilation flag, in which case all reads are cached, in-\n",
            "cluding reads that are performed using inline assembly regardless of the modifier used.\n",
            "When caching is enabled using one of the three mechanisms listed above, devices of compute capa-\n",
            "bility 5.2 will cache global memory reads in the unified L1/texture cache for all kernel launches except\n",
            "for the kernel launches for which thread blocks consume too much of the SM’s register file. These\n",
            "exceptions are reported by the profiler.\n",
            "19.4.3. Shared Memory\n",
            "Shared memory has 32 banks that are organized such that successive 32-bit words map to successive\n",
            "banks. Each bank has a bandwidth of 32 bits per clock cycle.\n",
            "A shared memory request for a warp does not generate a bank conflict between two threads that\n",
            "access any address within the same 32-bit word (even though the two addresses fall in the same bank).\n",
            "In that case, for read accesses, the word is broadcast to the requesting threads and for write accesses,\n",
            "each address is written by only one of the threads (which thread performs the write is undefined).\n",
            "Figure 22 shows some examples of strided access.\n",
            "Figure 23 shows some examples of memory read accesses that involve the broadcast mechanism.\n",
            "19.4. Compute Capability 5.x\n",
            "437\n",
            "---\n",
            "8: CUDA_C_Programming_Guide.pdf, page: 162\n",
            " CUDA C++ Programming Guide, Release 12.3\n",
            "Global memory resides in device memory and device memory is accessed via 32-, 64-, or 128-byte\n",
            "memory transactions. These memory transactions must be naturally aligned: Only the 32-, 64-, or 128-\n",
            "byte segments of device memory that are aligned to their size (i.e., whose first address is a multiple\n",
            "of their size) can be read or written by memory transactions.\n",
            "When a warp executes an instruction that accesses global memory, it coalesces the memory accesses\n",
            "of the threads within the warp into one or more of these memory transactions depending on the size\n",
            "of the word accessed by each thread and the distribution of the memory addresses across the threads.\n",
            "In general, the more transactions are necessary, the more unused words are transferred in addition to\n",
            "the words accessed by the threads, reducing the instruction throughput accordingly. For example, if a\n",
            "32-byte memory transaction is generated for each thread’s 4-byte access, throughput is divided by 8.\n",
            "How many transactions are necessary and how much throughput is ultimately affected varies with the\n",
            "compute capability of the device. Compute Capability 5.x, Compute Capability 6.x, Compute Capabil-\n",
            "ity 7.x, Compute Capability 8.x and Compute Capability 9.0 give more details on how global memory\n",
            "accesses are handled for various compute capabilities.\n",
            "To maximize global memory throughput, it is therefore important to maximize coalescing by:\n",
            "▶ Following the most optimal access patterns based on Compute Capability 5.x, Compute Capabil-\n",
            "ity 6.x, Compute Capability 7.x, Compute Capability 8.x and Compute Capability 9.0\n",
            "▶ Using data types that meet the size and alignment requirement detailed in the section Size and\n",
            "Alignment Requirement below,\n",
            "▶ Padding data in some cases, for example, when accessing a two-dimensional array as described\n",
            "in the section Two-Dimensional Arrays below.\n",
            "Size and Alignment Requirement\n",
            "Global memory instructions support reading or writing words of size equal to 1, 2, 4, 8, or 16 bytes.\n",
            "Any access (via a variable or a pointer) to data residing in global memory compiles to a single global\n",
            "memory instruction if and only if the size of the data type is 1, 2, 4, 8, or 16 bytes and the data is\n",
            "naturally aligned (i.e., its address is a multiple of that size).\n",
            "If this size and alignment requirement is not fulfilled, the access compiles to multiple instructions\n",
            "with interleaved access patterns that prevent these instructions from fully coalescing. It is therefore\n",
            "recommended to use types that meet this requirement for data that resides in global memory.\n",
            "The alignment requirement is automatically fulfilled for the Built-in Vector Types.\n",
            "For structures, the size and alignment requirements can be enforced by the compiler using the align-\n",
            "ment specifiers__align__(8) or __align__(16), such as\n",
            "struct __align__(8) {\n",
            "float x;\n",
            "float y;\n",
            "};\n",
            "or\n",
            "struct __align__(16) {\n",
            "float x;\n",
            "float y;\n",
            "float z;\n",
            "};\n",
            "Any address of a variable residing in global memory or returned by one of the memory allocation rou-\n",
            "tines from the driver or runtime API is always aligned to at least 256 bytes.\n",
            "146\n",
            "Chapter 8. Performance Guidelines\n",
            "---\n",
            "9: CUDA_C_Programming_Guide.pdf, page: 32\n",
            " CUDA C++ Programming Guide, Release 12.3\n",
            "in a cluster have the ability to read, write, and perform atomics to any address in the distributed shared\n",
            "memory. Distributed Shared Memory gives an example of performing histograms in distributed shared\n",
            "memory.\n",
            "5.3. Memory Hierarchy\n",
            "CUDA threads may access data from multiple memory spaces during their execution as illustrated by\n",
            "Figure 6. Each thread has private local memory. Each thread block has shared memory visible to all\n",
            "threads of the block and with the same lifetime as the block. Thread blocks in a thread block cluster\n",
            "can perform read, write, and atomics operations on each other’s shared memory. All threads have\n",
            "access to the same global memory.\n",
            "There are also two additional read-only memory spaces accessible by all threads: the constant and\n",
            "texture memory spaces. The global, constant, and texture memory spaces are optimized for differ-\n",
            "ent memory usages (see Device Memory Accesses). Texture memory also offers different addressing\n",
            "modes, as well as data filtering, for some specific data formats (see Texture and Surface Memory).\n",
            "The global, constant, and texture memory spaces are persistent across kernel launches by the same\n",
            "application.\n",
            "Figure 6: Memory Hierarchy\n",
            "16\n",
            "Chapter 5. Programming Model\n",
            "---\n",
            "10: CUDA_C_Best_Practices_Guide.pdf, page: 50\n",
            " CUDA C++ Best Practices Guide, Release 12.3\n",
            "13.2.1. Coalesced Access to Global Memory\n",
            "A very important performance consideration in programming for CUDA-capable GPU architectures is\n",
            "the coalescing of global memory accesses. Global memory loads and stores by threads of a warp are\n",
            "coalesced by the device into as few as possible transactions.\n",
            "Note: High Priority: Ensure global memory accesses are coalesced whenever possible.\n",
            "The access requirements for coalescing depend on the compute capability of the device and are doc-\n",
            "umented in the CUDA C++ Programming Guide.\n",
            "For devices of compute capability 6.0 or higher, the requirements can be summarized quite easily: the\n",
            "concurrent accesses of the threads of a warp will coalesce into a number of transactions equal to the\n",
            "number of 32-byte transactions necessary to service all of the threads of the warp.\n",
            "For certain devices of compute capability 5.2, L1-caching of accesses to global memory can be option-\n",
            "ally enabled. If L1-caching is enabled on these devices, the number of required transactions is equal\n",
            "to the number of required 128-byte aligned segments.\n",
            "Note:\n",
            "On devices of compute capability 6.0 or higher, L1-caching is the default, however the data\n",
            "access unit is 32-byte regardless of whether global loads are cached in L1 or not.\n",
            "On devices with GDDR memory, accessing memory in a coalesced way is even more important when\n",
            "ECC is turned on. Scattered accesses increase ECC memory transfer overhead, especially when writing\n",
            "data to global memory.\n",
            "Coalescing concepts are illustrated in the following simple examples. These examples assume com-\n",
            "pute capability 6.0 or higher and that accesses are for 4-byte words, unless otherwise noted.\n",
            "13.2.1.1 A Simple Access Pattern\n",
            "The first and simplest case of coalescing can be achieved by any CUDA-enabled device of compute\n",
            "capability 6.0 or higher: the k-th thread accesses the k-th word in a 32-byte aligned array. Not all\n",
            "threads need to participate.\n",
            "For example, if the threads of a warp access adjacent 4-byte words (e.g., adjacent float values), four\n",
            "coalesced 32-byte transactions will service that memory access. Such a pattern is shown in Figure 3.\n",
            "Fig. 3: Coalesced access\n",
            "This access pattern results in four 32-byte transactions, indicated by the red rectangles.\n",
            "If from any of the four 32-byte segments only a subset of the words are requested (e.g. if several\n",
            "threads had accessed the same word or if some threads did not participate in the access), the full\n",
            "segment is fetched anyway. Furthermore, if accesses by the threads of the warp had been permuted\n",
            "within or accross the four segments, still only four 32-byte transactions would have been performed\n",
            "by a device with compute capability 6.0 or higher.\n",
            "44\n",
            "Chapter 13. Memory Optimizations\n",
            "---\n",
            "11: CUDA_C_Best_Practices_Guide.pdf, page: 58\n",
            " CUDA C++ Best Practices Guide, Release 12.3\n",
            "13.2.3.2 Shared Memory in Matrix Multiplication (C=AB)\n",
            "Shared memory enables cooperation between threads in a block. When multiple threads in a block\n",
            "use the same data from global memory, shared memory can be used to access the data from global\n",
            "memory only once. Shared memory can also be used to avoid uncoalesced memory accesses by loading\n",
            "and storing data in a coalesced pattern from global memory and then reordering it in shared memory.\n",
            "Aside from memory bank conflicts, there is no penalty for non-sequential or unaligned accesses by a\n",
            "warp in shared memory.\n",
            "The use of shared memory is illustrated via the simple example of a matrix multiplication C = AB for\n",
            "the case with A of dimension Mxw, B of dimension wxN, and C of dimension MxN. To keep the kernels\n",
            "simple, M and N are multiples of 32, since the warp size (w) is 32 for current devices.\n",
            "A natural decomposition of the problem is to use a block and tile size of wxw threads. Therefore, in\n",
            "terms of wxw tiles, A is a column matrix, B is a row matrix, and C is their outer product; see Figure 11. A\n",
            "grid of N/w by M/w blocks is launched, where each thread block calculates the elements of a different\n",
            "tile in C from a single tile of A and a single tile of B.\n",
            "Fig. 11: Block-column matrix multiplied by block-row matrix. Block-column matrix (A) multiplied by\n",
            "block-row matrix (B) with resulting product matrix (C).\n",
            "To do this, the simpleMultiply kernel (Unoptimized matrix multiplication) calculates the output el-\n",
            "ements of a tile of matrix C.\n",
            "Unoptimized matrix multiplication\n",
            "__global__ void simpleMultiply(float *a, float* b, float *c,\n",
            "int N)\n",
            "{\n",
            "int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
            "int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
            "float sum = 0.0f;\n",
            "for (int i = 0; i < TILE_DIM; i++) {\n",
            "sum += a[row*TILE_DIM+i] * b[i*N+col];\n",
            "}\n",
            "c[row*N+col] = sum;\n",
            "}\n",
            "In Unoptimized matrix multiplication, a, b, and c are pointers to global memory for the matrices A, B,\n",
            "and C, respectively; blockDim.x, blockDim.y, and TILE_DIM are all equal to w. Each thread in the\n",
            "52\n",
            "Chapter 13. Memory Optimizations\n",
            "---\n",
            "12: CUDA_C_Programming_Guide.pdf, page: 20\n",
            " CUDA C++ Programming Guide, Release 12.3\n",
            "of relying on large data caches and complex flow control to avoid long memory access latencies, both\n",
            "of which are expensive in terms of transistors.\n",
            "In general, an application has a mix of parallel parts and sequential parts, so systems are designed with\n",
            "a mix of GPUs and CPUs in order to maximize overall performance. Applications with a high degree of\n",
            "parallelism can exploit this massively parallel nature of the GPU to achieve higher performance than\n",
            "on the CPU.\n",
            "4\n",
            "Chapter 1. The Benefits of Using GPUs\n",
            "---\n",
            "13: CUDA_C_Best_Practices_Guide.pdf, page: 65\n",
            " CUDA C++ Best Practices Guide, Release 12.3\n",
            "▶ Best performance with synchronous copy is achieved when the copy_count parameter is a mul-\n",
            "tiple of 4 for all three element sizes. The compiler can optimize groups of 4 load and store in-\n",
            "structions. This is evident from the saw tooth curves.\n",
            "▶ Asynchronous copy achieves better performance in nearly all cases.\n",
            "▶ The async-copy does not require the copy_count parameter to be a multiple of 4, to maximize\n",
            "performance through compiler optimizations.\n",
            "▶ Overall, best performance is achieved when using asynchronous copies with an element of size\n",
            "8 or 16 bytes.\n",
            "13.2.4. Local Memory\n",
            "Local memory is so named because its scope is local to the thread, not because of its physical location.\n",
            "In fact, local memory is off-chip. Hence, access to local memory is as expensive as access to global\n",
            "memory. In other words, the term local in the name does not imply faster access.\n",
            "Local memory is used only to hold automatic variables. This is done by the nvcc compiler when it\n",
            "determines that there is insufficient register space to hold the variable. Automatic variables that are\n",
            "likely to be placed in local memory are large structures or arrays that would consume too much register\n",
            "space and arrays that the compiler determines may be indexed dynamically.\n",
            "Inspection of the PTX assembly code (obtained by compiling with -ptx or -keep command-line op-\n",
            "tions to nvcc) reveals whether a variable has been placed in local memory during the first compilation\n",
            "phases. If it has, it will be declared using the .local mnemonic and accessed using the ld.local\n",
            "and st.local mnemonics. If it has not, subsequent compilation phases might still decide otherwise,\n",
            "if they find the variable consumes too much register space for the targeted architecture. There is no\n",
            "way to check this for a specific variable, but the compiler reports total local memory usage per kernel\n",
            "(lmem) when run with the--ptxas-options=-v option.\n",
            "13.2.5. Texture Memory\n",
            "The read-only texture memory space is cached. Therefore, a texture fetch costs one device memory\n",
            "read only on a cache miss; otherwise, it just costs one read from the texture cache. The texture cache\n",
            "is optimized for 2D spatial locality, so threads of the same warp that read texture addresses that are\n",
            "close together will achieve best performance. Texture memory is also designed for streaming fetches\n",
            "with a constant latency; that is, a cache hit reduces DRAM bandwidth demand, but not fetch latency.\n",
            "In certain addressing situations, reading device memory through texture fetching can be an advanta-\n",
            "geous alternative to reading device memory from global or constant memory.\n",
            "13.2.5.1 Additional Texture Capabilities\n",
            "If textures are fetched using tex1D(),tex2D(), or tex3D() rather than tex1Dfetch(), the hardware\n",
            "provides other capabilities that might be useful for some applications such as image processing, as\n",
            "shown in Table 4.\n",
            "13.2. Device Memory Spaces\n",
            "59\n",
            "---\n",
            "14: CUDA_C_Best_Practices_Guide.pdf, page: 23\n",
            " CUDA C++ Best Practices Guide, Release 12.3\n",
            "▶ For best performance, there should be some coherence in memory access by adjacent threads\n",
            "running on the device. Certain memory access patterns enable the hardware to coalesce groups\n",
            "of reads or writes of multiple data items into one operation. Data that cannot be laid out so as to\n",
            "enable coalescing, or that doesn’t have enough locality to use the L1 or texture caches effectively,\n",
            "will tend to see lesser speedups when used in computations on GPUs. A noteworthy exception\n",
            "to this are completely random memory access patterns. In general, they should be avoided, be-\n",
            "cause compared to peak capabilities any architecture processes these memory access patterns\n",
            "at a low efficiency. However, compared to cache based architectures, like CPUs, latency hiding\n",
            "architectures, like GPUs, tend to cope better with completely random memory access patterns.\n",
            "6.2. What Runs on a CUDA-Enabled Device?\n",
            "17\n",
            "---\n",
            "15: CUDA_C_Best_Practices_Guide.pdf, page: 42\n",
            " CUDA C++ Best Practices Guide, Release 12.3\n",
            "12.2.3. Throughput Reported by Visual Profiler\n",
            "For devices with compute capability of 2.0 or greater, the Visual Profiler can be used to collect several\n",
            "different memory throughput measures. The following throughput metrics can be displayed in the\n",
            "Details or Detail Graphs view:\n",
            "▶ Requested Global Load Throughput\n",
            "▶ Requested Global Store Throughput\n",
            "▶ Global Load Throughput\n",
            "▶ Global Store Throughput\n",
            "▶ DRAM Read Throughput\n",
            "▶ DRAM Write Throughput\n",
            "The Requested Global Load Throughput and Requested Global Store Throughput values indicate the\n",
            "global memory throughput requested by the kernel and therefore correspond to the effective band-\n",
            "width obtained by the calculation shown under Effective Bandwidth Calculation.\n",
            "Because the minimum memory transaction size is larger than most word sizes, the actual memory\n",
            "throughput required for a kernel can include the transfer of data not used by the kernel. For global\n",
            "memory accesses, this actual throughput is reported by the Global Load Throughput and Global Store\n",
            "Throughput values.\n",
            "It’s important to note that both numbers are useful. The actual memory throughput shows how close\n",
            "the code is to the hardware limit, and a comparison of the effective or requested bandwidth to the\n",
            "actual bandwidth presents a good estimate of how much bandwidth is wasted by suboptimal coalesc-\n",
            "ing of memory accesses (see Coalesced Access to Global Memory). For global memory accesses, this\n",
            "comparison of requested memory bandwidth to actual memory bandwidth is reported by the Global\n",
            "Memory Load Efficiency and Global Memory Store Efficiency metrics.\n",
            "36\n",
            "Chapter 12. Performance Metrics\n",
            "\n",
            "\n",
            "Now please respond to my query: Explain how tiling helps with global memory bandwidth.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"________________this is full prompt_________________\")\n",
        "print(dataset['train'][0]['full_prompt'])\n",
        "print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khJ0vb_Bn-SJ",
        "outputId": "43db4685-90f9-4191-b79b-103421a67910"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1072\n"
          ]
        }
      ],
      "source": [
        "print(len(dataset[\"train\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhK71CUxj29q",
        "outputId": "26282ee6-9332-424e-a79e-fd6f5020c019"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                         first_query  \\\n",
            "0  Explain how tiling helps with global memory ba...   \n",
            "1      Show an example of [CUDA API] function usage.   \n",
            "2  what is my chance to get into ece master at ui...   \n",
            "3  Explain how tiling helps with global memory ba...   \n",
            "4                    When is the first midterm exam?   \n",
            "\n",
            "                                      first_response  \\\n",
            "0  Tiling is a technique used in CUDA programming...   \n",
            "1  Sure, here are a few examples of CUDA API func...   \n",
            "2  I'm sorry, but the documents provided do not c...   \n",
            "3  Tiling is a technique that helps optimize the ...   \n",
            "4  The first midterm exam is scheduled after appr...   \n",
            "\n",
            "                                         full_prompt  \n",
            "0  \\nPlease analyze and respond to the following ...  \n",
            "1  \\nPlease analyze and respond to the following ...  \n",
            "2  \\nPlease analyze and respond to the following ...  \n",
            "3  \\nPlease analyze and respond to the following ...  \n",
            "4  \\nPlease analyze and respond to the following ...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "first_query = dataset['train'][\"first_query\"]\n",
        "first_response = dataset['train'][\"first_response\"]\n",
        "full_prompt = dataset['train']['full_prompt']\n",
        "\n",
        "\n",
        "# Create a dictionary of lists\n",
        "data = {'first_query': first_query,\n",
        "     'first_response': first_response,\n",
        "     \"full_prompt\": full_prompt}\n",
        "\n",
        "\n",
        "# Create a DataFrame from the dictionary\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdPanYKrcXOK",
        "outputId": "177a9607-8a18-4e72-8850-f503b674da56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "975\n",
            "1072\n"
          ]
        }
      ],
      "source": [
        "print(len(df[\"first_query\"].unique()))\n",
        "print(len(df[\"first_query\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3QIjmRvdcuz",
        "outputId": "78b0ee21-f0ba-4489-de49-ffd62bf5f296"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                             first_query  \\\n",
            "780  explain tiles matrix multiplication   \n",
            "\n",
            "                                        first_response  \\\n",
            "780  Tiled matrix multiplication is an optimization...   \n",
            "\n",
            "                                           full_prompt  \\\n",
            "780  \\nPlease analyze and respond to the following ...   \n",
            "\n",
            "                                              critique  \n",
            "780  af3570b60c2c121304683a6ea099da58b26a109f8beaec...  \n"
          ]
        }
      ],
      "source": [
        "test = df[df[\"first_query\"] == \"explain tiles matrix multiplication\"]\n",
        "print(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BM1wl8lAkNwi",
        "outputId": "fc25946a-993b-4977-92b6-88cff1ad4a89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated the 0 sample\n",
            "generated the 1 sample\n",
            "generated the 2 sample\n",
            "generated the 3 sample\n",
            "generated the 4 sample\n",
            "generated the 5 sample\n",
            "generated the 6 sample\n",
            "generated the 7 sample\n",
            "generated the 8 sample\n",
            "generated the 9 sample\n",
            "generated the 10 sample\n",
            "generated the 11 sample\n",
            "generated the 12 sample\n",
            "generated the 13 sample\n",
            "generated the 14 sample\n",
            "generated the 15 sample\n",
            "generated the 16 sample\n",
            "generated the 17 sample\n",
            "generated the 18 sample\n",
            "generated the 19 sample\n",
            "generated the 20 sample\n",
            "generated the 21 sample\n",
            "generated the 22 sample\n",
            "generated the 23 sample\n",
            "generated the 24 sample\n",
            "generated the 25 sample\n",
            "generated the 26 sample\n",
            "generated the 27 sample\n",
            "generated the 28 sample\n",
            "generated the 29 sample\n",
            "generated the 30 sample\n",
            "generated the 31 sample\n",
            "generated the 32 sample\n",
            "generated the 33 sample\n",
            "generated the 34 sample\n",
            "generated the 35 sample\n",
            "generated the 36 sample\n",
            "generated the 37 sample\n",
            "generated the 38 sample\n",
            "generated the 39 sample\n",
            "generated the 40 sample\n",
            "generated the 41 sample\n",
            "generated the 42 sample\n",
            "generated the 43 sample\n",
            "generated the 44 sample\n",
            "generated the 45 sample\n",
            "generated the 46 sample\n",
            "generated the 47 sample\n",
            "generated the 48 sample\n",
            "generated the 49 sample\n",
            "generated the 50 sample\n",
            "generated the 51 sample\n",
            "generated the 52 sample\n",
            "generated the 53 sample\n",
            "generated the 54 sample\n",
            "generated the 55 sample\n",
            "generated the 56 sample\n",
            "generated the 57 sample\n",
            "generated the 58 sample\n",
            "generated the 59 sample\n",
            "generated the 60 sample\n",
            "generated the 61 sample\n",
            "generated the 62 sample\n",
            "generated the 63 sample\n",
            "generated the 64 sample\n",
            "generated the 65 sample\n",
            "generated the 66 sample\n",
            "generated the 67 sample\n",
            "generated the 68 sample\n",
            "generated the 69 sample\n",
            "generated the 70 sample\n",
            "generated the 71 sample\n",
            "generated the 72 sample\n",
            "generated the 73 sample\n",
            "generated the 74 sample\n",
            "generated the 75 sample\n",
            "generated the 76 sample\n",
            "generated the 77 sample\n",
            "generated the 78 sample\n",
            "generated the 79 sample\n",
            "generated the 80 sample\n",
            "generated the 81 sample\n",
            "generated the 82 sample\n",
            "generated the 83 sample\n",
            "generated the 84 sample\n",
            "generated the 85 sample\n",
            "generated the 86 sample\n",
            "generated the 87 sample\n",
            "generated the 88 sample\n",
            "generated the 89 sample\n",
            "generated the 90 sample\n",
            "generated the 91 sample\n",
            "generated the 92 sample\n",
            "generated the 93 sample\n",
            "generated the 94 sample\n",
            "generated the 95 sample\n",
            "generated the 96 sample\n",
            "generated the 97 sample\n",
            "generated the 98 sample\n",
            "generated the 99 sample\n",
            "generated the 100 sample\n",
            "generated the 101 sample\n",
            "generated the 102 sample\n",
            "generated the 103 sample\n",
            "generated the 104 sample\n",
            "generated the 105 sample\n",
            "generated the 106 sample\n",
            "generated the 107 sample\n",
            "generated the 108 sample\n",
            "generated the 109 sample\n",
            "generated the 110 sample\n",
            "generated the 111 sample\n",
            "generated the 112 sample\n",
            "generated the 113 sample\n",
            "generated the 114 sample\n",
            "generated the 115 sample\n",
            "generated the 116 sample\n",
            "generated the 117 sample\n",
            "generated the 118 sample\n",
            "generated the 119 sample\n",
            "generated the 120 sample\n",
            "generated the 121 sample\n",
            "generated the 122 sample\n",
            "generated the 123 sample\n",
            "generated the 124 sample\n",
            "generated the 125 sample\n",
            "generated the 126 sample\n",
            "generated the 127 sample\n",
            "generated the 128 sample\n",
            "generated the 129 sample\n",
            "generated the 130 sample\n",
            "generated the 131 sample\n",
            "generated the 132 sample\n",
            "generated the 133 sample\n",
            "generated the 134 sample\n",
            "generated the 135 sample\n",
            "generated the 136 sample\n",
            "generated the 137 sample\n",
            "generated the 138 sample\n",
            "generated the 139 sample\n",
            "generated the 140 sample\n",
            "generated the 141 sample\n",
            "generated the 142 sample\n",
            "generated the 143 sample\n",
            "generated the 144 sample\n",
            "generated the 145 sample\n",
            "generated the 146 sample\n",
            "generated the 147 sample\n",
            "generated the 148 sample\n",
            "generated the 149 sample\n",
            "generated the 150 sample\n",
            "generated the 151 sample\n",
            "generated the 152 sample\n",
            "generated the 153 sample\n",
            "generated the 154 sample\n",
            "generated the 155 sample\n",
            "generated the 156 sample\n",
            "generated the 157 sample\n",
            "generated the 158 sample\n",
            "generated the 159 sample\n",
            "generated the 160 sample\n",
            "generated the 161 sample\n",
            "generated the 162 sample\n",
            "generated the 163 sample\n",
            "generated the 164 sample\n",
            "generated the 165 sample\n",
            "generated the 166 sample\n",
            "generated the 167 sample\n",
            "generated the 168 sample\n",
            "generated the 169 sample\n",
            "generated the 170 sample\n",
            "generated the 171 sample\n",
            "generated the 172 sample\n",
            "generated the 173 sample\n",
            "generated the 174 sample\n",
            "generated the 175 sample\n",
            "generated the 176 sample\n",
            "generated the 177 sample\n",
            "generated the 178 sample\n",
            "generated the 179 sample\n",
            "generated the 180 sample\n",
            "generated the 181 sample\n",
            "generated the 182 sample\n",
            "generated the 183 sample\n",
            "generated the 184 sample\n",
            "generated the 185 sample\n",
            "generated the 186 sample\n",
            "generated the 187 sample\n",
            "generated the 188 sample\n",
            "generated the 189 sample\n",
            "generated the 190 sample\n",
            "generated the 191 sample\n",
            "generated the 192 sample\n",
            "generated the 193 sample\n",
            "generated the 194 sample\n",
            "generated the 195 sample\n",
            "generated the 196 sample\n",
            "generated the 197 sample\n",
            "generated the 198 sample\n",
            "generated the 199 sample\n",
            "generated the 200 sample\n",
            "generated the 201 sample\n",
            "generated the 202 sample\n",
            "generated the 203 sample\n",
            "generated the 204 sample\n",
            "generated the 205 sample\n",
            "generated the 206 sample\n",
            "generated the 207 sample\n",
            "generated the 208 sample\n",
            "generated the 209 sample\n",
            "generated the 210 sample\n",
            "generated the 211 sample\n",
            "generated the 212 sample\n",
            "generated the 213 sample\n",
            "generated the 214 sample\n",
            "generated the 215 sample\n",
            "generated the 216 sample\n",
            "generated the 217 sample\n",
            "generated the 218 sample\n",
            "generated the 219 sample\n",
            "generated the 220 sample\n",
            "generated the 221 sample\n",
            "generated the 222 sample\n",
            "generated the 223 sample\n",
            "generated the 224 sample\n",
            "generated the 225 sample\n",
            "generated the 226 sample\n",
            "generated the 227 sample\n",
            "generated the 228 sample\n",
            "generated the 229 sample\n",
            "generated the 230 sample\n",
            "generated the 231 sample\n",
            "generated the 232 sample\n",
            "generated the 233 sample\n",
            "generated the 234 sample\n",
            "generated the 235 sample\n",
            "generated the 236 sample\n",
            "generated the 237 sample\n",
            "generated the 238 sample\n",
            "generated the 239 sample\n",
            "generated the 240 sample\n",
            "generated the 241 sample\n",
            "generated the 242 sample\n",
            "generated the 243 sample\n",
            "generated the 244 sample\n",
            "generated the 245 sample\n",
            "generated the 246 sample\n",
            "generated the 247 sample\n",
            "generated the 248 sample\n",
            "generated the 249 sample\n",
            "generated the 250 sample\n",
            "generated the 251 sample\n",
            "generated the 252 sample\n",
            "generated the 253 sample\n",
            "generated the 254 sample\n",
            "generated the 255 sample\n",
            "generated the 256 sample\n",
            "generated the 257 sample\n",
            "generated the 258 sample\n",
            "generated the 259 sample\n",
            "generated the 260 sample\n",
            "generated the 261 sample\n",
            "generated the 262 sample\n",
            "generated the 263 sample\n",
            "generated the 264 sample\n",
            "generated the 265 sample\n",
            "generated the 266 sample\n",
            "generated the 267 sample\n",
            "generated the 268 sample\n",
            "generated the 269 sample\n",
            "generated the 270 sample\n",
            "generated the 271 sample\n",
            "generated the 272 sample\n",
            "generated the 273 sample\n",
            "generated the 274 sample\n",
            "generated the 275 sample\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "\n",
        "llm_client = InferenceClient(\n",
        "    model=model_id,\n",
        "    timeout=120,\n",
        ")\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    first_query = row[\"first_query\"]\n",
        "    first_response = row[\"first_response\"]\n",
        "\n",
        "    system_prompt = f\"\"\"You are a brilliant AI assistant.\n",
        "    You will receive an input question and the model-generated answer.\n",
        "    You need to generate the specific and actionable critiques for the model-generated answer,\n",
        "    which contain the critical comments and helpful suggestions.\"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"input question: {first_query}\n",
        "                    model-generated answer: {first_response}.\n",
        "                    critique:\"\"\"\n",
        "\n",
        "    full_prompt = f\"[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>{user_prompt} [/INST]\\n\\n\"\n",
        "    output = llm_client.text_generation(prompt=full_prompt, max_new_tokens=1000)\n",
        "\n",
        "    df.at[i, 'critique'] = output\n",
        "    print(f\"generated the {i} sample\")\n",
        "    df.to_json('critic_output.json', orient='records', lines=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsuxBOJRB0Zu",
        "outputId": "fbcb5cf3-8408-4970-82a7-86aa93305873"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the JSON file into a DataFrame\n",
        "df = pd.read_json('critic_output.json', orient='records', lines=True)\n",
        "\n",
        "print(df.loc[277]['critique'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_ZTovZtdp--",
        "outputId": "b2f3c47a-2da5-4532-c393-200336e7adee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1072, 4)\n"
          ]
        }
      ],
      "source": [
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The input maybe too long so that we need to set a upper bound for that. The following code shows how to do that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5usVjkrC4GB",
        "outputId": "9b16aea8-3df5-4756-cba3-912f08359376"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated the 781 sample\n",
            "generated the 782 sample\n",
            "generated the 783 sample\n",
            "generated the 784 sample\n",
            "generated the 785 sample\n",
            "generated the 786 sample\n",
            "generated the 787 sample\n",
            "generated the 788 sample\n",
            "generated the 789 sample\n",
            "generated the 790 sample\n",
            "generated the 791 sample\n",
            "generated the 792 sample\n",
            "generated the 793 sample\n",
            "generated the 794 sample\n",
            "generated the 795 sample\n",
            "generated the 796 sample\n",
            "generated the 797 sample\n",
            "generated the 798 sample\n",
            "generated the 799 sample\n",
            "generated the 800 sample\n",
            "generated the 801 sample\n",
            "generated the 802 sample\n",
            "generated the 803 sample\n",
            "generated the 804 sample\n",
            "generated the 805 sample\n",
            "generated the 806 sample\n",
            "generated the 807 sample\n",
            "generated the 808 sample\n",
            "generated the 809 sample\n",
            "generated the 810 sample\n",
            "generated the 811 sample\n",
            "generated the 812 sample\n",
            "generated the 813 sample\n",
            "generated the 814 sample\n",
            "generated the 815 sample\n",
            "generated the 816 sample\n",
            "generated the 817 sample\n",
            "generated the 818 sample\n",
            "generated the 819 sample\n",
            "generated the 820 sample\n",
            "generated the 821 sample\n",
            "generated the 822 sample\n",
            "generated the 823 sample\n",
            "generated the 824 sample\n",
            "generated the 825 sample\n",
            "generated the 826 sample\n",
            "generated the 827 sample\n",
            "generated the 828 sample\n",
            "generated the 829 sample\n",
            "generated the 830 sample\n",
            "generated the 831 sample\n",
            "generated the 832 sample\n",
            "generated the 833 sample\n",
            "generated the 834 sample\n",
            "generated the 835 sample\n",
            "generated the 836 sample\n",
            "generated the 837 sample\n",
            "generated the 838 sample\n",
            "generated the 839 sample\n",
            "generated the 840 sample\n",
            "generated the 841 sample\n",
            "generated the 842 sample\n",
            "generated the 843 sample\n",
            "generated the 844 sample\n",
            "generated the 845 sample\n",
            "generated the 846 sample\n",
            "generated the 847 sample\n",
            "generated the 848 sample\n",
            "generated the 849 sample\n",
            "generated the 850 sample\n",
            "generated the 851 sample\n",
            "generated the 852 sample\n",
            "generated the 853 sample\n",
            "generated the 854 sample\n",
            "generated the 855 sample\n",
            "generated the 856 sample\n",
            "generated the 857 sample\n",
            "generated the 858 sample\n",
            "generated the 859 sample\n",
            "generated the 860 sample\n",
            "generated the 861 sample\n",
            "generated the 862 sample\n",
            "generated the 863 sample\n",
            "generated the 864 sample\n",
            "generated the 865 sample\n",
            "generated the 866 sample\n",
            "generated the 867 sample\n",
            "generated the 868 sample\n",
            "generated the 869 sample\n",
            "generated the 870 sample\n",
            "generated the 871 sample\n",
            "generated the 872 sample\n",
            "generated the 873 sample\n",
            "generated the 874 sample\n",
            "generated the 875 sample\n",
            "generated the 876 sample\n",
            "generated the 877 sample\n",
            "generated the 878 sample\n",
            "generated the 879 sample\n",
            "generated the 880 sample\n",
            "generated the 881 sample\n",
            "generated the 882 sample\n",
            "generated the 883 sample\n",
            "generated the 884 sample\n",
            "generated the 885 sample\n",
            "generated the 886 sample\n",
            "generated the 887 sample\n",
            "generated the 888 sample\n",
            "generated the 889 sample\n",
            "generated the 890 sample\n",
            "generated the 891 sample\n",
            "generated the 892 sample\n",
            "generated the 893 sample\n",
            "generated the 894 sample\n",
            "generated the 895 sample\n",
            "generated the 896 sample\n",
            "generated the 897 sample\n",
            "generated the 898 sample\n",
            "generated the 899 sample\n",
            "generated the 900 sample\n",
            "generated the 901 sample\n",
            "generated the 902 sample\n",
            "generated the 903 sample\n",
            "generated the 904 sample\n",
            "generated the 905 sample\n",
            "generated the 906 sample\n",
            "generated the 907 sample\n",
            "generated the 908 sample\n",
            "generated the 909 sample\n",
            "generated the 910 sample\n",
            "generated the 911 sample\n",
            "generated the 912 sample\n",
            "generated the 913 sample\n",
            "generated the 914 sample\n",
            "generated the 915 sample\n",
            "generated the 916 sample\n",
            "generated the 917 sample\n",
            "generated the 918 sample\n",
            "generated the 919 sample\n",
            "generated the 920 sample\n",
            "generated the 921 sample\n",
            "generated the 922 sample\n",
            "generated the 923 sample\n",
            "generated the 924 sample\n",
            "generated the 925 sample\n",
            "generated the 926 sample\n",
            "generated the 927 sample\n",
            "generated the 928 sample\n",
            "generated the 929 sample\n",
            "generated the 930 sample\n",
            "generated the 931 sample\n",
            "generated the 932 sample\n",
            "generated the 933 sample\n",
            "generated the 934 sample\n",
            "generated the 935 sample\n",
            "generated the 936 sample\n",
            "generated the 937 sample\n",
            "generated the 938 sample\n",
            "generated the 939 sample\n",
            "generated the 940 sample\n",
            "generated the 941 sample\n",
            "generated the 942 sample\n",
            "generated the 943 sample\n",
            "generated the 944 sample\n",
            "generated the 945 sample\n",
            "generated the 946 sample\n",
            "generated the 947 sample\n",
            "generated the 948 sample\n",
            "generated the 949 sample\n",
            "generated the 950 sample\n",
            "generated the 951 sample\n",
            "generated the 952 sample\n",
            "generated the 953 sample\n",
            "generated the 954 sample\n",
            "generated the 955 sample\n",
            "generated the 956 sample\n",
            "generated the 957 sample\n",
            "generated the 958 sample\n",
            "generated the 959 sample\n",
            "generated the 960 sample\n",
            "generated the 961 sample\n",
            "generated the 962 sample\n",
            "generated the 963 sample\n",
            "generated the 964 sample\n",
            "generated the 965 sample\n",
            "generated the 966 sample\n",
            "generated the 967 sample\n",
            "generated the 968 sample\n",
            "generated the 969 sample\n",
            "generated the 970 sample\n",
            "generated the 971 sample\n",
            "generated the 972 sample\n",
            "generated the 973 sample\n",
            "generated the 974 sample\n",
            "generated the 975 sample\n",
            "generated the 976 sample\n",
            "generated the 977 sample\n",
            "generated the 978 sample\n",
            "generated the 979 sample\n",
            "generated the 980 sample\n",
            "generated the 981 sample\n",
            "generated the 982 sample\n",
            "generated the 983 sample\n",
            "generated the 984 sample\n",
            "generated the 985 sample\n",
            "generated the 986 sample\n",
            "generated the 987 sample\n",
            "generated the 988 sample\n",
            "generated the 989 sample\n",
            "generated the 990 sample\n",
            "generated the 991 sample\n",
            "generated the 992 sample\n",
            "generated the 993 sample\n",
            "generated the 994 sample\n",
            "generated the 995 sample\n",
            "generated the 996 sample\n",
            "generated the 997 sample\n",
            "generated the 998 sample\n",
            "generated the 999 sample\n",
            "generated the 1000 sample\n",
            "generated the 1001 sample\n",
            "generated the 1002 sample\n",
            "generated the 1003 sample\n",
            "generated the 1004 sample\n",
            "generated the 1005 sample\n",
            "generated the 1006 sample\n",
            "generated the 1007 sample\n",
            "generated the 1008 sample\n",
            "generated the 1009 sample\n",
            "generated the 1010 sample\n",
            "generated the 1011 sample\n",
            "generated the 1012 sample\n",
            "generated the 1013 sample\n",
            "generated the 1014 sample\n",
            "generated the 1015 sample\n",
            "generated the 1016 sample\n",
            "generated the 1017 sample\n",
            "generated the 1018 sample\n",
            "generated the 1019 sample\n",
            "generated the 1020 sample\n",
            "generated the 1021 sample\n",
            "generated the 1022 sample\n",
            "generated the 1023 sample\n",
            "generated the 1024 sample\n",
            "generated the 1025 sample\n",
            "generated the 1026 sample\n",
            "generated the 1027 sample\n",
            "generated the 1028 sample\n",
            "generated the 1029 sample\n",
            "generated the 1030 sample\n",
            "generated the 1031 sample\n",
            "generated the 1032 sample\n",
            "generated the 1033 sample\n",
            "generated the 1034 sample\n",
            "generated the 1035 sample\n",
            "generated the 1036 sample\n",
            "generated the 1037 sample\n",
            "generated the 1038 sample\n",
            "generated the 1039 sample\n",
            "generated the 1040 sample\n",
            "generated the 1041 sample\n",
            "generated the 1042 sample\n",
            "generated the 1043 sample\n",
            "generated the 1044 sample\n",
            "generated the 1045 sample\n",
            "generated the 1046 sample\n",
            "generated the 1047 sample\n",
            "generated the 1048 sample\n",
            "generated the 1049 sample\n",
            "generated the 1050 sample\n",
            "generated the 1051 sample\n",
            "generated the 1052 sample\n",
            "generated the 1053 sample\n",
            "generated the 1054 sample\n",
            "generated the 1055 sample\n",
            "generated the 1056 sample\n",
            "generated the 1057 sample\n",
            "generated the 1058 sample\n",
            "generated the 1059 sample\n",
            "generated the 1060 sample\n",
            "generated the 1061 sample\n",
            "generated the 1062 sample\n",
            "generated the 1063 sample\n",
            "generated the 1064 sample\n",
            "generated the 1065 sample\n",
            "generated the 1066 sample\n",
            "generated the 1067 sample\n",
            "generated the 1068 sample\n",
            "generated the 1069 sample\n",
            "generated the 1070 sample\n",
            "generated the 1071 sample\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "from transformers import AutoTokenizer\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize the tokenizer\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "# tokenizer.model_max_length = 32768\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
        "\n",
        "# Define a maximum token limit for the input\n",
        "max_input_tokens = 32768-1000-10  # Reserve some tokens for the output and the extra tokens\n",
        "\n",
        "model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "\n",
        "llm_client = InferenceClient(\n",
        "    model=model_id,\n",
        "    timeout=120,\n",
        ")\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "  if i >= 781:\n",
        "    first_query = row[\"first_query\"]\n",
        "    first_response = row[\"first_response\"]\n",
        "\n",
        "    system_prompt = f\"\"\"You are a brilliant AI assistant.\n",
        "    You will receive an input question and the model-generated answer.\n",
        "    You need to generate the specific and actionable critiques for the model-generated answer,\n",
        "    which contain the critical comments and helpful suggestions.\"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"input question: {first_query}\n",
        "                    model-generated answer: {first_response}.\n",
        "                    critique:\"\"\"\n",
        "\n",
        "    full_prompt = f\"[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>{user_prompt} [/INST]\\n\\n\"\n",
        "\n",
        "     # Tokenize the input text\n",
        "    input_tokens = tokenizer.encode(full_prompt)\n",
        "\n",
        "    # If the input text is too long, truncate it\n",
        "    if len(input_tokens) > max_input_tokens:\n",
        "        input_tokens = input_tokens[:max_input_tokens]\n",
        "\n",
        "    # Convert the tokens back to text\n",
        "    truncated_prompt = tokenizer.decode(input_tokens)\n",
        "\n",
        "    # Generate the output\n",
        "    output = llm_client.text_generation(prompt=truncated_prompt, max_new_tokens=1000)\n",
        "\n",
        "\n",
        "    df.at[i, 'critique'] = output\n",
        "    print(f\"generated the {i} sample\")\n",
        "     # Convert the row to a dictionary\n",
        "    row_dict = df.loc[i].to_dict()\n",
        "\n",
        "    # Append the row to the JSON file\n",
        "    with open('/content/drive/MyDrive/ECE_data/critic_output_part4.json', 'a') as f:\n",
        "        f.write(json.dumps(row_dict) + '\\n')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0eb1b19e25c44782b839d11617affdfc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16cdce3d28d54a588c094b88a0848561": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "18ad3c635e944fb8b588050837808dc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a92acd3e322c4a168b73f0dcea2cf578",
              "IPY_MODEL_4e660636610d4cf3a166e55eb61f7a33",
              "IPY_MODEL_b179cd1a6a6d41c098a18366c615eedb"
            ],
            "layout": "IPY_MODEL_e6bbdb2988f34fbe81398a0f14026d93"
          }
        },
        "27962ff14c2c40e592ea4894ea605d44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0eb1b19e25c44782b839d11617affdfc",
            "max": 1072,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_16cdce3d28d54a588c094b88a0848561",
            "value": 1072
          }
        },
        "3720e92b4e58458d9c7513ad019a553a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9145b882f306494991f51f082c2a40df",
            "placeholder": "​",
            "style": "IPY_MODEL_bff1bfa9aaaa4374833d39c076c48a89",
            "value": "Downloading readme: 100%"
          }
        },
        "3b152e2573874204ab02d341c088857d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e660636610d4cf3a166e55eb61f7a33": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a71f1b911a0346db95216680cf99b2ac",
            "max": 215889948,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a77535dabc8b400ba6edca10beaa0ef2",
            "value": 215889948
          }
        },
        "57369b947c2b48fa8b6041a00eb84375": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "576616bf75224bbbb6b2e7297a6f2e0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61fcf7a204df43819c1b96431b81bdca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6c5a2b3de6e4b0dba31b812a4af10c5",
            "placeholder": "​",
            "style": "IPY_MODEL_576616bf75224bbbb6b2e7297a6f2e0c",
            "value": " 1072/1072 [00:03&lt;00:00, 284.89 examples/s]"
          }
        },
        "635698185a364db0a50e6fad692abe34": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6bf5b18bdc6948fca69f728bba98c361": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6fdec7ba61b443faab785c31847c8e2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3720e92b4e58458d9c7513ad019a553a",
              "IPY_MODEL_8543aef8a11e4e2b8462293ac4c94050",
              "IPY_MODEL_779c0dddbec541a797ce21340d8ee73c"
            ],
            "layout": "IPY_MODEL_fecd041b849642cea28999820c5de64a"
          }
        },
        "779c0dddbec541a797ce21340d8ee73c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bc250482fe44ccd8ba6b6811d1cbbd0",
            "placeholder": "​",
            "style": "IPY_MODEL_635698185a364db0a50e6fad692abe34",
            "value": " 180k/180k [00:00&lt;00:00, 2.13MB/s]"
          }
        },
        "7e1ddbec3e5743b9917cc07f76d85585": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8543aef8a11e4e2b8462293ac4c94050": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b152e2573874204ab02d341c088857d",
            "max": 179780,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_faf50398f0c74656ae8b32b2be6c3303",
            "value": 179780
          }
        },
        "8e05e15e0a6540d8949a5d9bbe7c624c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f27bd3f50328414c8485452911666183",
            "placeholder": "​",
            "style": "IPY_MODEL_7e1ddbec3e5743b9917cc07f76d85585",
            "value": "Generating train split: 100%"
          }
        },
        "9145b882f306494991f51f082c2a40df": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bc250482fe44ccd8ba6b6811d1cbbd0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9db9bd7e2a944221bfcc41c2c91489ce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a71f1b911a0346db95216680cf99b2ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a77535dabc8b400ba6edca10beaa0ef2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a92acd3e322c4a168b73f0dcea2cf578": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57369b947c2b48fa8b6041a00eb84375",
            "placeholder": "​",
            "style": "IPY_MODEL_6bf5b18bdc6948fca69f728bba98c361",
            "value": "Downloading data: 100%"
          }
        },
        "b179cd1a6a6d41c098a18366c615eedb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9db9bd7e2a944221bfcc41c2c91489ce",
            "placeholder": "​",
            "style": "IPY_MODEL_e38c2b96b1b547bf81d8acb07c3db858",
            "value": " 216M/216M [00:10&lt;00:00, 19.6MB/s]"
          }
        },
        "b35c153b6c4d458bbbbc7ac79a408177": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e05e15e0a6540d8949a5d9bbe7c624c",
              "IPY_MODEL_27962ff14c2c40e592ea4894ea605d44",
              "IPY_MODEL_61fcf7a204df43819c1b96431b81bdca"
            ],
            "layout": "IPY_MODEL_fdf08ebec2414fe2845611c8d19416dd"
          }
        },
        "bff1bfa9aaaa4374833d39c076c48a89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e38c2b96b1b547bf81d8acb07c3db858": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6bbdb2988f34fbe81398a0f14026d93": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f27bd3f50328414c8485452911666183": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6c5a2b3de6e4b0dba31b812a4af10c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "faf50398f0c74656ae8b32b2be6c3303": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fdf08ebec2414fe2845611c8d19416dd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fecd041b849642cea28999820c5de64a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
