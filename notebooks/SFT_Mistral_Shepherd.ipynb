{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Tesla V100-PCIE-32GB\n",
      "(33337507840, 34079899648)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import torch \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"SFT_mistral\"\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name())\n",
    "\n",
    "print(torch.cuda.mem_get_info())\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trevahok/utils/miniconda3/envs/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-02-08 19:44:54.651414: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-08 19:44:54.651466: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-08 19:44:54.653152: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-08 19:44:54.664640: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-08 19:44:55.824472: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging, TextStreamer\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use WeightsAndBiases \n",
    "to monitor system metrics and training process on the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find SFT_mistral.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtrevahok\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/trevahok/codebase/shepherd_test/wandb/run-20240201_162243-l2f1msga</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/trevahok/Fine%20tuning%20mistral%207B/runs/l2f1msga' target=\"_blank\">confused-salad-6</a></strong> to <a href='https://wandb.ai/trevahok/Fine%20tuning%20mistral%207B' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/trevahok/Fine%20tuning%20mistral%207B' target=\"_blank\">https://wandb.ai/trevahok/Fine%20tuning%20mistral%207B</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/trevahok/Fine%20tuning%20mistral%207B/runs/l2f1msga' target=\"_blank\">https://wandb.ai/trevahok/Fine%20tuning%20mistral%207B/runs/l2f1msga</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login()\n",
    "\n",
    "wandb_project = \"mistral-finetune\"\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project\n",
    "\n",
    "run = wandb.init(project='Fine tuning mistral 7B', job_type=\"training\", anonymous=\"allow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load dataset and model \n",
    "n22/Mistral-7B-Instruct-v0.1-sharded if you want to train on Colab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"mistralai/Mistral-7B-v0.1\" #\n",
    "dataset_name, new_model = \"philschmid/meta-shepherd-human-data\", \"trevahok/mistral_SFT_quantized\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_name, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['### Question: Jolene and Phil have four children, each with the same birthday.  They gave birth to their first child exactly 15 years ago.  They gave birth to their second child exactly one year after the birth of their first child.  They gave birth to their third child on the fourth birthday of their second child. Two years after the birth of their third child, they gave birth to their fourth child.  How old, in years, is their fourth child?\\n          \\n### Answer: The first child was born 15 years ago. The second child was born 1 year after the first child. The third child was born on the second child\\'s fourth birthday. The fourth child was born 2 years after the third child. So the fourth child is 15 + 1 + 4 + 2 = 22 years old. The answer is 22.\\n\\n\\n\\n### Feedback: Firstly, The math is off in the previous to last sentence, since the ages of the children should be subtracted, not summed. 15 - 1 - 4 -2 = 8 is the right answer. Secondly, \"So the fourth child is 15 + 1 + 4 + 2 = 22 years old. The answer is 22.\" The fourth born child cannot be older than the first born child.  ',\n",
       " \"### Question: Ryan has 40 balls. There are twice as many red balls as blue while the rest are green. If there are 11 blue balls, how many green balls are there?\\n          \\n### Answer: Ryan has 40 balls. There are twice as many red balls as blue. So there are 20 red balls. The rest are green. 40 - 20 - 11 = 19. The answer is 19.\\n\\n\\n\\n### Feedback: Saying that there are twice as many red balls as blue doesn't tell us what the total of red balls is, so the deduction that there are 20 is wrongly based. From this wrong premise, the rest of the answer is wrong\",\n",
       " '### Question: A family has three adults and children, both girls and boys. They went out for a family trip and prepared 3 dozen boiled eggs for the trip. Every adult got 3 eggs, and the boys each received 1 more egg than each girl since the girls had some snacks. How many boys went on the trip if the number of girls was 7 and each girl received an egg?\\n          \\n### Answer: There were 3 adults and children, both girls and boys. Each adult got 3 eggs. So 3 adults got 3 * 3 = 9 eggs. Each boy got 1 more egg than each girl. So 7 girls got 7 eggs. 7 + 7 = 14 eggs. So the boys got 9 + 14 = 23 eggs. 9 + 14 = 23. The answer is 23.\\n\\n\\n\\n### Feedback: \"So the boys got 9 + 14 = 23 eggs.\" Where is the 9 coming from? the 14 is girls multiplied by 2 but the 9 has no context. ']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['text'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type= \"nf4\", \n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant= False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.98s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map= \"auto\",\n",
    "    max_memory = { 0: '30.1GiB', 1: '30.1GiB', }\n",
    ")\n",
    "model.config.use_cache = False # silence the warnings. Please re-enable for inference!\n",
    "model.config.pretraining_tp = 1\n",
    "model.gradient_checkpointing_enable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.add_bos_token = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add PEFT with LoRa for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "peft_config = LoraConfig(\n",
    "        r=32,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n",
    "    )\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Arguments\n",
    "Hyperparameters should beadjusted based on the hardware you using. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trevahok/utils/miniconda3/envs/venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:223: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1317/1317 [00:00<00:00, 2704.74 examples/s]\n",
      "/home/trevahok/utils/miniconda3/envs/venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:290: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir= \"./results\",\n",
    "    num_train_epochs= 2,\n",
    "    per_device_train_batch_size= 16, \n",
    "    auto_find_batch_size=True,\n",
    "    gradient_accumulation_steps= 2,\n",
    "    optim = \"paged_adamw_8bit\",\n",
    "    save_steps= 200,\n",
    "    logging_steps= 30,\n",
    "    learning_rate= 2e-4,\n",
    "    weight_decay= 0.001,\n",
    "    fp16= False,\n",
    "    bf16= False,\n",
    "    max_grad_norm= 0.3,\n",
    "    max_steps= -1,\n",
    "    warmup_ratio= 0.3,\n",
    "    group_by_length= True,\n",
    "    lr_scheduler_type= \"constant\",\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "# Setting sft parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length= None,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing= False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find SFT_mistral.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtrevahok\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/trevahok/codebase/shepherd_test/wandb/run-20240208_201735-6sa4w5mi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/trevahok/huggingface/runs/6sa4w5mi' target=\"_blank\">breezy-aardvark-1</a></strong> to <a href='https://wandb.ai/trevahok/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/trevahok/huggingface' target=\"_blank\">https://wandb.ai/trevahok/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/trevahok/huggingface/runs/6sa4w5mi' target=\"_blank\">https://wandb.ai/trevahok/huggingface/runs/6sa4w5mi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/trevahok/utils/miniconda3/envs/venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='82' max='82' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [82/82 45:29, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.474000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.223700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=82, training_loss=1.3039202573822766, metrics={'train_runtime': 2838.9806, 'train_samples_per_second': 0.928, 'train_steps_per_second': 0.029, 'total_flos': 3.297225324896256e+16, 'train_loss': 1.3039202573822766, 'epoch': 1.98})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MistralDecoderLayer(\n",
       "            (self_attn): MistralAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): MistralRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm()\n",
       "            (post_attention_layernorm): MistralRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer.model.save_pretrained(\"SFT_Mistral_with_EOS\")\n",
    "model.config.use_cache = True\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_feedback( question, answer):\n",
    "    runtimeFlag = \"cuda:0\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.unk_token\n",
    "    tokenizer.add_bos_token = True\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    prompt =f\"### Question: {question} \\n### Answer: { answer} \\n### Feedback: \" \n",
    "\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(runtimeFlag)\n",
    "\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=False )\n",
    "\n",
    "    output = trainer.model.generate(**inputs, streamer=streamer, max_new_tokens=200)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ### Question: What is 2+2 in an imaginary world where 2+2=99? \n",
      "### Answer: 99 \n",
      "### Feedback: 2+2=4 in the real world. The answer is incorrect because it says 2+2=99.</s>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   774, 22478, 28747,  1824,   349, 28705, 28750, 28806, 28750,\n",
       "           297,   396, 28374,  1526,   970, 28705, 28750, 28806, 28750, 28746,\n",
       "         28774, 28774, 28804, 28705,    13, 27332, 26307, 28747, 28705, 28774,\n",
       "         28774, 28705,    13, 27332,  4615,   286,  1435, 28747, 28705, 28750,\n",
       "         28806, 28750, 28746, 28781,   297,   272,  1353,  1526, 28723,   415,\n",
       "          4372,   349, 16390,  1096,   378,  2627, 28705, 28750, 28806, 28750,\n",
       "         28746, 28774, 28774, 28723,     2]], device='cuda:0')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_feedback(\"What is 2+2 in an imaginary world where 2+2=99?\", \"99\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Compare against base model ( Mistral 7B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.56s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vanilla_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    # quantization_config=bnb_config,\n",
    "    # device_map= {\"\" : 1} ,\n",
    "    device_map= \"auto\",\n",
    "    max_memory = { 0: '30.1GiB', 1: '30.1GiB', }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feedback_vanilla( question, answer):\n",
    "    vanilla_model.eval()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.add_bos_token = True\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    runtimeFlag = \"cuda:0\"\n",
    "\n",
    "    prompt = f\"### Question: {question} \\n### Answer: { answer} \\n### Feedback: \"\n",
    "\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(runtimeFlag)\n",
    "\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    output = vanilla_model.generate(**inputs, streamer=streamer, max_new_tokens=200)\n",
    "\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "### \n",
      "###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   774, 22478, 28747,  1824, 28705, 28750, 28806, 28750,   297,\n",
       "           396, 28374,  1526,   970, 28705, 28750, 28806, 28750,   327, 28705,\n",
       "         28774, 28774, 28804, 28705,    13, 27332, 26307, 28747, 28705, 28774,\n",
       "         28774, 28705,    13, 27332,  4615,   286,  1435, 28747, 28705,    13,\n",
       "         27332, 28705,    13, 27332, 28705,    13, 27332, 28705,    13, 27332,\n",
       "         28705,    13, 27332, 28705,    13, 27332, 28705,    13, 27332, 28705,\n",
       "            13, 27332, 28705,    13, 27332, 28705,    13, 27332, 28705,    13,\n",
       "         27332, 28705,    13, 27332, 28705,    13, 27332, 28705,    13, 27332,\n",
       "         28705,    13, 27332, 28705,    13, 27332, 28705,    13, 27332, 28705,\n",
       "            13, 27332, 28705,    13, 27332, 28705,    13, 27332, 28705,    13,\n",
       "         27332, 28705,    13, 27332, 28705,    13, 27332, 28705,    13, 27332,\n",
       "         28705,    13, 27332, 28705,    13, 27332, 28705,    13, 27332, 28705,\n",
       "            13, 27332, 28705,    13, 27332, 28705,    13, 27332, 28705,    13,\n",
       "         27332, 28705,    13, 27332, 28705,    13, 27332, 28705,    13, 27332,\n",
       "         28705,    13, 27332, 28705,    13, 27332, 28705,    13, 27332, 28705,\n",
       "            13, 27332, 28705,    13, 27332, 28705,    13, 27332, 28705,    13,\n",
       "         27332, 28705,    13, 27332, 28705,    13, 27332, 28705,    13, 27332,\n",
       "         28705,    13, 27332, 28705,    13, 27332, 28705,    13, 27332, 28705,\n",
       "            13, 27332, 28705,    13, 27332, 28705,    13, 27332, 28705,    13,\n",
       "         27332, 28705,    13, 27332, 28705,    13, 27332, 28705,    13, 27332,\n",
       "         28705,    13, 27332, 28705,    13, 27332, 28705,    13, 27332, 28705,\n",
       "            13, 27332, 28705,    13, 27332, 28705,    13, 27332, 28705,    13,\n",
       "         27332, 28705,    13, 27332, 28705,    13, 27332, 28705,    13, 27332,\n",
       "         28705,    13, 27332, 28705,    13, 27332, 28705,    13, 27332]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_feedback_vanilla(\"What 2+2 in an imaginary world where 2+2 = 99?\", \"99\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_feedback_vanilla(\"What color is the sun?\", \"Blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_feedback(\"What color is the sun?\", \"Blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up RAM and GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import gc \n",
    "\n",
    "# del model, tokenizer \n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
